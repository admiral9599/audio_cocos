<!DOCTYPE html>
<!-- saved from url=(0034)https://zenodo.org/records/4282267 -->
<html lang="en" dir="ltr"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
    
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="google-site-verification" content="5fPGCLllnWrvFxH9QWI0l1TadV7byeEvfPcyK2VkS_s">
    <meta name="google-site-verification" content="Rp5zp04IKW-s1IbpTOGB7Z6XY60oloZD5C3kTM-AiY4">

    
    
    <meta name="generator" content="InvenioRDM 13.0">

    
    
    
  
    <meta name="description" content="SpeechCoco Introduction Our corpus is an extension of the MS COCO image recognition and captioning dataset. MS COCO comprises images paired with a set of five captions. Yet, it does not include any speech. Therefore, we used Voxygen&#39;s text-to-speech system to synthesise the available captions. The addition of speech as a new modality enables MSCOCO to be used for researches in the field of language acquisition, unsupervised term discovery, keyword spotting, or semantic embedding using speech and vision. Our corpus is licensed under a Creative Commons Attribution 4.0 License. Data Set This corpus contains 616,767 spoken captions from MSCOCO&#39;s val2014 and train2014 subsets (respectively 414,113 for train2014 and 202,654 for val2014). We used 8 different voices. 4 of them have a British accent (Paul, Bronwen, Judith, and Elizabeth) and the 4 others have an American accent (Phil, Bruce, Amanda, Jenny). In order to make the captions sound more natural, we used SOX tempo command, enabling us to change the speed without changing the pitch. 1/3 of the captions are 10% slower than the original pace, 1/3 are 10% faster. The last third of the captions was kept untouched. We also modified approximately 30% of the original captions and added disfluencies such as &quot;um&quot;, &quot;uh&quot;, &quot;er&quot; so that the captions would sound more natural. Each WAV file is paired with a JSON file containing various information: timecode of each word in the caption, name of the speaker, name of the WAV file, etc. The JSON files have the following data structure: { &quot;duration&quot;: float, &quot;speaker&quot;: string, &quot;synthesisedCaption&quot;: string, &quot;timecode&quot;: list, &quot;speed&quot;: float, &quot;wavFilename&quot;: string, &quot;captionID&quot;: int, &quot;imgID&quot;: int, &quot;disfluency&quot;: list } On average, each caption comprises 10.79 tokens, disfluencies included. The WAV files are on average 3.52 seconds long. Repository The repository is organized as follows: CORPUS-MSCOCO (~75GB once decompressed) train2014/ : folder contains 413,915 captions json/ wav/ translations/ train_en_ja.txt train_translate.sqlite3 train_2014.sqlite3 val2014/ : folder contains 202,520 captions json/ wav/ translations/ train_en_ja.txt train_translate.sqlite3 val_2014.sqlite3 speechcoco_API/ speechcoco/ __init__.py speechcoco.py setup.py Filenames .wav files contain the spoken version of a caption .json files contain all the metadata of a given WAV file .sqlite3 files are SQLite databases containing all the information contained in the JSON files We adopted the following naming convention for both the WAV and JSON files: imageID_captionID_Speaker_DisfluencyPosition_Speed[.wav/.json] Script We created a script called speechcoco.py in order to handle the metadata and allow the user to easily find captions according to specific filters. The script uses the *.db files. Features: Aggregate all the information in the JSON files into a single SQLite database Find captions according to specific filters (name, gender and nationality of the speaker, disfluency position, speed, duration, and words in the caption). The script automatically builds the SQLite query. The user can also provide his own SQLite query. The following Python code returns all the captions spoken by a male with an American accent for which the speed was slowed down by 10% and that contain &quot;keys&quot; at any position # create SpeechCoco object db = SpeechCoco(train_2014.sqlite3, train_translate.sqlite3, verbose=True) # filter captions (returns Caption Objects) captions = db.filterCaptions(gender=&quot;Male&quot;, nationality=&quot;US&quot;, speed=0.9, text=&#39;%keys%&#39;) for caption in captions: print(&#39;\n{}\t{}\t{}\t{}\t{}\t{}\t\t{}&#39;.format(caption.imageID, caption.captionID, caption.speaker.name, caption.speaker.nationality, caption.speed, caption.filename, caption.text)) ... 298817 26763 Phil 0.9 298817_26763_Phil_None_0-9.wav A group of turkeys with bushes in the background. 108505 147972 Phil 0.9 108505_147972_Phil_Middle_0-9.wav Person using a, um, slider cell phone with blue backlit keys. 258289 154380 Bruce 0.9 258289_154380_Bruce_None_0-9.wav Some donkeys and sheep are in their green pens . 545312 201303 Phil 0.9 545312_201303_Phil_None_0-9.wav A man walking next to a couple of donkeys. ... Find all the captions belonging to a specific image captions = db.getImgCaptions(298817) for caption in captions: print(&#39;\n{}&#39;.format(caption.text)) Birds wondering through grassy ground next to bushes. A flock of turkeys are making their way up a hill. Um, ah. Two wild turkeys in a field walking around. Four wild turkeys and some bushes trees and weeds. A group of turkeys with bushes in the background. Parse the timecodes and have them structured input: ... [1926.3068, &quot;SYL&quot;, &quot;&quot;], [1926.3068, &quot;SEPR&quot;, &quot; &quot;], [1926.3068, &quot;WORD&quot;, &quot;white&quot;], [1926.3068, &quot;PHO&quot;, &quot;w&quot;], [2050.7955, &quot;PHO&quot;, &quot;ai&quot;], [2144.6591, &quot;PHO&quot;, &quot;t&quot;], [2179.3182, &quot;SYL&quot;, &quot;&quot;], [2179.3182, &quot;SEPR&quot;, &quot; &quot;] ... output: print(caption.timecode.parse()) ... { &#39;begin&#39;: 1926.3068, &#39;end&#39;: 2179.3182, &#39;syllable&#39;: [{&#39;begin&#39;: 1926.3068, &#39;end&#39;: 2179.3182, &#39;phoneme&#39;: [{&#39;begin&#39;: 1926.3068, &#39;end&#39;: 2050.7955, &#39;value&#39;: &#39;w&#39;}, {&#39;begin&#39;: 2050.7955, &#39;end&#39;: 2144.6591, &#39;value&#39;: &#39;ai&#39;}, {&#39;begin&#39;: 2144.6591, &#39;end&#39;: 2179.3182, &#39;value&#39;: &#39;t&#39;}], &#39;value&#39;: &#39;wait&#39;}], &#39;value&#39;: &#39;white&#39; }, ... Convert the timecodes to Praat TextGrid files caption.timecode.toTextgrid(outputDir, level=3) Get the words, syllables and phonemes between n seconds/milliseconds The following Python code returns all the words between 0.2 and 0.6 seconds for which at least 50% of the word&#39;s total length is within the specified interval pprint(caption.getWords(0.20, 0.60, seconds=True, level=1, olapthr=50)) ... 404537 827239 Bruce US 0.9 404537_827239_Bruce_None_0-9.wav Eyeglasses, a cellphone, some keys and other pocket items are all laid out on the cloth. . [ { &#39;begin&#39;: 0.0, &#39;end&#39;: 0.7202778, &#39;overlapPercentage&#39;: 55.53412863758955, &#39;word&#39;: &#39;eyeglasses&#39; } ] ... Get the translations of the selected captions As for now, only japanese translations are available. We also used Kytea to tokenize and tag the captions translated with Google Translate captions = db.getImgCaptions(298817) for caption in captions: print(&#39;\n{}&#39;.format(caption.text)) # Get translations and POS print(&#39;\tja_google: {}&#39;.format(db.getTranslation(caption.captionID, &quot;ja_google&quot;))) print(&#39;\t\tja_google_tokens: {}&#39;.format(db.getTokens(caption.captionID, &quot;ja_google&quot;))) print(&#39;\t\tja_google_pos: {}&#39;.format(db.getPOS(caption.captionID, &quot;ja_google&quot;))) print(&#39;\tja_excite: {}&#39;.format(db.getTranslation(caption.captionID, &quot;ja_excite&quot;))) Birds wondering through grassy ground next to bushes. ja_google: 鳥は茂みの下に茂った地面を抱えています。 ja_google_tokens: 鳥 は 茂み の 下 に 茂 っ た 地面 を 抱え て い ま す 。 ja_google_pos: 鳥/名詞/とり は/助詞/は 茂み/名詞/しげみ の/助詞/の 下/名詞/した に/助詞/に 茂/動詞/しげ っ/語尾/っ た/助動詞/た 地面/名詞/じめん を/助詞/を 抱え/動詞/かかえ て/助詞/て い/動詞/い ま/助動詞/ま す/語尾/す 。/補助記号/。 ja_excite: 低木と隣接した草深いグラウンドを通って疑う鳥。 A flock of turkeys are making their way up a hill. ja_google: 七面鳥の群れが丘を上っています。 ja_google_tokens: 七 面 鳥 の 群れ が 丘 を 上 っ て い ま す 。 ja_google_pos: 七/名詞/なな 面/名詞/めん 鳥/名詞/とり の/助詞/の 群れ/名詞/むれ が/助詞/が 丘/名詞/おか を/助詞/を 上/動詞/のぼ っ/語尾/っ て/助詞/て い/動詞/い ま/助動詞/ま す/語尾/す 。/補助記号/。 ja_excite: 七面鳥の群れは丘の上で進んでいる。 Um, ah. Two wild turkeys in a field walking around. ja_google: 野生のシチメンチョウ、野生の七面鳥 ja_google_tokens: 野生 の シチメンチョウ 、 野生 の 七 面 鳥 ja_google_pos: 野生/名詞/やせい の/助詞/の シチメンチョウ/名詞/しちめんちょう 、/補助記号/、 野生/名詞/やせい の/助詞/の 七/名詞/なな 面/名詞/めん 鳥/名詞/ちょう ja_excite: まわりで移動しているフィールドの2羽の野生の七面鳥 Four wild turkeys and some bushes trees and weeds. ja_google: 4本の野生のシチメンチョウといくつかの茂みの木と雑草 ja_google_tokens: 4 本 の 野生 の シチメンチョウ と いく つ か の 茂み の 木 と 雑草 ja_google_pos: 4/名詞/4 本/接尾辞/ほん の/助詞/の 野生/名詞/やせい の/助詞/の シチメンチョウ/名詞/しちめんちょう と/助詞/と いく/名詞/いく つ/接尾辞/つ か/助詞/か の/助詞/の 茂み/名詞/しげみ の/助詞/の 木/名詞/き と/助詞/と 雑草/名詞/ざっそう ja_excite: 4羽の野生の七面鳥およびいくつかの低木木と雑草 A group of turkeys with bushes in the background. ja_google: 背景に茂みを持つ七面鳥の群 ja_google_tokens: 背景 に 茂み を 持 つ 七 面 鳥 の 群 ja_google_pos: 背景/名詞/はいけい に/助詞/に 茂み/名詞/しげみ を/助詞/を 持/動詞/も つ/語尾/つ 七/名詞/なな 面/名詞/めん 鳥/名詞/ちょう の/助詞/の 群/名詞/むれ ja_excite: 背景の低木を持つ七面鳥のグループ  ">
    <meta name="citation_title" content="SPEECH-COCO">
    <meta name="citation_author" content="William N. Havard">
    <meta name="citation_author" content="Laurent Besacier">
    <meta name="citation_doi" content="10.5281/zenodo.4282267">
    <meta name="citation_keywords" content="MSCOCO">
    <meta name="citation_keywords" content="VGS">
    <meta name="citation_keywords" content="Speech">
    <meta name="citation_keywords" content="Visually Grounded Speech">
    <meta name="citation_keywords" content="audio">
    <meta name="citation_keywords" content="captions">
    <meta name="citation_abstract_html_url" content="https://zenodo.org/records/4282267">
    <meta property="og:title" content="SPEECH-COCO">
    <meta property="og:description" content="SpeechCoco Introduction Our corpus is an extension of the MS COCO image recognition and captioning dataset. MS COCO comprises images paired with a set of five captions. Yet, it does not include any speech. Therefore, we used Voxygen&#39;s text-to-speech system to synthesise the available captions. The addition of speech as a new modality enables MSCOCO to be used for researches in the field of language acquisition, unsupervised term discovery, keyword spotting, or semantic embedding using speech and vision. Our corpus is licensed under a Creative Commons Attribution 4.0 License. Data Set This corpus contains 616,767 spoken captions from MSCOCO&#39;s val2014 and train2014 subsets (respectively 414,113 for train2014 and 202,654 for val2014). We used 8 different voices. 4 of them have a British accent (Paul, Bronwen, Judith, and Elizabeth) and the 4 others have an American accent (Phil, Bruce, Amanda, Jenny). In order to make the captions sound more natural, we used SOX tempo command, enabling us to change the speed without changing the pitch. 1/3 of the captions are 10% slower than the original pace, 1/3 are 10% faster. The last third of the captions was kept untouched. We also modified approximately 30% of the original captions and added disfluencies such as &quot;um&quot;, &quot;uh&quot;, &quot;er&quot; so that the captions would sound more natural. Each WAV file is paired with a JSON file containing various information: timecode of each word in the caption, name of the speaker, name of the WAV file, etc. The JSON files have the following data structure: { &quot;duration&quot;: float, &quot;speaker&quot;: string, &quot;synthesisedCaption&quot;: string, &quot;timecode&quot;: list, &quot;speed&quot;: float, &quot;wavFilename&quot;: string, &quot;captionID&quot;: int, &quot;imgID&quot;: int, &quot;disfluency&quot;: list } On average, each caption comprises 10.79 tokens, disfluencies included. The WAV files are on average 3.52 seconds long. Repository The repository is organized as follows: CORPUS-MSCOCO (~75GB once decompressed) train2014/ : folder contains 413,915 captions json/ wav/ translations/ train_en_ja.txt train_translate.sqlite3 train_2014.sqlite3 val2014/ : folder contains 202,520 captions json/ wav/ translations/ train_en_ja.txt train_translate.sqlite3 val_2014.sqlite3 speechcoco_API/ speechcoco/ __init__.py speechcoco.py setup.py Filenames .wav files contain the spoken version of a caption .json files contain all the metadata of a given WAV file .sqlite3 files are SQLite databases containing all the information contained in the JSON files We adopted the following naming convention for both the WAV and JSON files: imageID_captionID_Speaker_DisfluencyPosition_Speed[.wav/.json] Script We created a script called speechcoco.py in order to handle the metadata and allow the user to easily find captions according to specific filters. The script uses the *.db files. Features: Aggregate all the information in the JSON files into a single SQLite database Find captions according to specific filters (name, gender and nationality of the speaker, disfluency position, speed, duration, and words in the caption). The script automatically builds the SQLite query. The user can also provide his own SQLite query. The following Python code returns all the captions spoken by a male with an American accent for which the speed was slowed down by 10% and that contain &quot;keys&quot; at any position # create SpeechCoco object db = SpeechCoco(train_2014.sqlite3, train_translate.sqlite3, verbose=True) # filter captions (returns Caption Objects) captions = db.filterCaptions(gender=&quot;Male&quot;, nationality=&quot;US&quot;, speed=0.9, text=&#39;%keys%&#39;) for caption in captions: print(&#39;\n{}\t{}\t{}\t{}\t{}\t{}\t\t{}&#39;.format(caption.imageID, caption.captionID, caption.speaker.name, caption.speaker.nationality, caption.speed, caption.filename, caption.text)) ... 298817 26763 Phil 0.9 298817_26763_Phil_None_0-9.wav A group of turkeys with bushes in the background. 108505 147972 Phil 0.9 108505_147972_Phil_Middle_0-9.wav Person using a, um, slider cell phone with blue backlit keys. 258289 154380 Bruce 0.9 258289_154380_Bruce_None_0-9.wav Some donkeys and sheep are in their green pens . 545312 201303 Phil 0.9 545312_201303_Phil_None_0-9.wav A man walking next to a couple of donkeys. ... Find all the captions belonging to a specific image captions = db.getImgCaptions(298817) for caption in captions: print(&#39;\n{}&#39;.format(caption.text)) Birds wondering through grassy ground next to bushes. A flock of turkeys are making their way up a hill. Um, ah. Two wild turkeys in a field walking around. Four wild turkeys and some bushes trees and weeds. A group of turkeys with bushes in the background. Parse the timecodes and have them structured input: ... [1926.3068, &quot;SYL&quot;, &quot;&quot;], [1926.3068, &quot;SEPR&quot;, &quot; &quot;], [1926.3068, &quot;WORD&quot;, &quot;white&quot;], [1926.3068, &quot;PHO&quot;, &quot;w&quot;], [2050.7955, &quot;PHO&quot;, &quot;ai&quot;], [2144.6591, &quot;PHO&quot;, &quot;t&quot;], [2179.3182, &quot;SYL&quot;, &quot;&quot;], [2179.3182, &quot;SEPR&quot;, &quot; &quot;] ... output: print(caption.timecode.parse()) ... { &#39;begin&#39;: 1926.3068, &#39;end&#39;: 2179.3182, &#39;syllable&#39;: [{&#39;begin&#39;: 1926.3068, &#39;end&#39;: 2179.3182, &#39;phoneme&#39;: [{&#39;begin&#39;: 1926.3068, &#39;end&#39;: 2050.7955, &#39;value&#39;: &#39;w&#39;}, {&#39;begin&#39;: 2050.7955, &#39;end&#39;: 2144.6591, &#39;value&#39;: &#39;ai&#39;}, {&#39;begin&#39;: 2144.6591, &#39;end&#39;: 2179.3182, &#39;value&#39;: &#39;t&#39;}], &#39;value&#39;: &#39;wait&#39;}], &#39;value&#39;: &#39;white&#39; }, ... Convert the timecodes to Praat TextGrid files caption.timecode.toTextgrid(outputDir, level=3) Get the words, syllables and phonemes between n seconds/milliseconds The following Python code returns all the words between 0.2 and 0.6 seconds for which at least 50% of the word&#39;s total length is within the specified interval pprint(caption.getWords(0.20, 0.60, seconds=True, level=1, olapthr=50)) ... 404537 827239 Bruce US 0.9 404537_827239_Bruce_None_0-9.wav Eyeglasses, a cellphone, some keys and other pocket items are all laid out on the cloth. . [ { &#39;begin&#39;: 0.0, &#39;end&#39;: 0.7202778, &#39;overlapPercentage&#39;: 55.53412863758955, &#39;word&#39;: &#39;eyeglasses&#39; } ] ... Get the translations of the selected captions As for now, only japanese translations are available. We also used Kytea to tokenize and tag the captions translated with Google Translate captions = db.getImgCaptions(298817) for caption in captions: print(&#39;\n{}&#39;.format(caption.text)) # Get translations and POS print(&#39;\tja_google: {}&#39;.format(db.getTranslation(caption.captionID, &quot;ja_google&quot;))) print(&#39;\t\tja_google_tokens: {}&#39;.format(db.getTokens(caption.captionID, &quot;ja_google&quot;))) print(&#39;\t\tja_google_pos: {}&#39;.format(db.getPOS(caption.captionID, &quot;ja_google&quot;))) print(&#39;\tja_excite: {}&#39;.format(db.getTranslation(caption.captionID, &quot;ja_excite&quot;))) Birds wondering through grassy ground next to bushes. ja_google: 鳥は茂みの下に茂った地面を抱えています。 ja_google_tokens: 鳥 は 茂み の 下 に 茂 っ た 地面 を 抱え て い ま す 。 ja_google_pos: 鳥/名詞/とり は/助詞/は 茂み/名詞/しげみ の/助詞/の 下/名詞/した に/助詞/に 茂/動詞/しげ っ/語尾/っ た/助動詞/た 地面/名詞/じめん を/助詞/を 抱え/動詞/かかえ て/助詞/て い/動詞/い ま/助動詞/ま す/語尾/す 。/補助記号/。 ja_excite: 低木と隣接した草深いグラウンドを通って疑う鳥。 A flock of turkeys are making their way up a hill. ja_google: 七面鳥の群れが丘を上っています。 ja_google_tokens: 七 面 鳥 の 群れ が 丘 を 上 っ て い ま す 。 ja_google_pos: 七/名詞/なな 面/名詞/めん 鳥/名詞/とり の/助詞/の 群れ/名詞/むれ が/助詞/が 丘/名詞/おか を/助詞/を 上/動詞/のぼ っ/語尾/っ て/助詞/て い/動詞/い ま/助動詞/ま す/語尾/す 。/補助記号/。 ja_excite: 七面鳥の群れは丘の上で進んでいる。 Um, ah. Two wild turkeys in a field walking around. ja_google: 野生のシチメンチョウ、野生の七面鳥 ja_google_tokens: 野生 の シチメンチョウ 、 野生 の 七 面 鳥 ja_google_pos: 野生/名詞/やせい の/助詞/の シチメンチョウ/名詞/しちめんちょう 、/補助記号/、 野生/名詞/やせい の/助詞/の 七/名詞/なな 面/名詞/めん 鳥/名詞/ちょう ja_excite: まわりで移動しているフィールドの2羽の野生の七面鳥 Four wild turkeys and some bushes trees and weeds. ja_google: 4本の野生のシチメンチョウといくつかの茂みの木と雑草 ja_google_tokens: 4 本 の 野生 の シチメンチョウ と いく つ か の 茂み の 木 と 雑草 ja_google_pos: 4/名詞/4 本/接尾辞/ほん の/助詞/の 野生/名詞/やせい の/助詞/の シチメンチョウ/名詞/しちめんちょう と/助詞/と いく/名詞/いく つ/接尾辞/つ か/助詞/か の/助詞/の 茂み/名詞/しげみ の/助詞/の 木/名詞/き と/助詞/と 雑草/名詞/ざっそう ja_excite: 4羽の野生の七面鳥およびいくつかの低木木と雑草 A group of turkeys with bushes in the background. ja_google: 背景に茂みを持つ七面鳥の群 ja_google_tokens: 背景 に 茂み を 持 つ 七 面 鳥 の 群 ja_google_pos: 背景/名詞/はいけい に/助詞/に 茂み/名詞/しげみ を/助詞/を 持/動詞/も つ/語尾/つ 七/名詞/なな 面/名詞/めん 鳥/名詞/ちょう の/助詞/の 群/名詞/むれ ja_excite: 背景の低木を持つ七面鳥のグループ  ">
    <meta property="og:url" content="https://zenodo.org/records/4282267">
    <meta property="og:site_name" content="Zenodo">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@zenodo_org">
    <meta name="twitter:title" content="SPEECH-COCO">
    <meta name="twitter:description" content="SpeechCoco Introduction Our corpus is an extension of the MS COCO image recognition and captioning dataset. MS COCO comprises images paired with a set of five captions. Yet, it does not include any speech. Therefore, we used Voxygen&#39;s text-to-speech system to synthesise the available captions. The addition of speech as a new modality enables MSCOCO to be used for researches in the field of language acquisition, unsupervised term discovery, keyword spotting, or semantic embedding using speech and vision. Our corpus is licensed under a Creative Commons Attribution 4.0 License. Data Set This corpus contains 616,767 spoken captions from MSCOCO&#39;s val2014 and train2014 subsets (respectively 414,113 for train2014 and 202,654 for val2014). We used 8 different voices. 4 of them have a British accent (Paul, Bronwen, Judith, and Elizabeth) and the 4 others have an American accent (Phil, Bruce, Amanda, Jenny). In order to make the captions sound more natural, we used SOX tempo command, enabling us to change the speed without changing the pitch. 1/3 of the captions are 10% slower than the original pace, 1/3 are 10% faster. The last third of the captions was kept untouched. We also modified approximately 30% of the original captions and added disfluencies such as &quot;um&quot;, &quot;uh&quot;, &quot;er&quot; so that the captions would sound more natural. Each WAV file is paired with a JSON file containing various information: timecode of each word in the caption, name of the speaker, name of the WAV file, etc. The JSON files have the following data structure: { &quot;duration&quot;: float, &quot;speaker&quot;: string, &quot;synthesisedCaption&quot;: string, &quot;timecode&quot;: list, &quot;speed&quot;: float, &quot;wavFilename&quot;: string, &quot;captionID&quot;: int, &quot;imgID&quot;: int, &quot;disfluency&quot;: list } On average, each caption comprises 10.79 tokens, disfluencies included. The WAV files are on average 3.52 seconds long. Repository The repository is organized as follows: CORPUS-MSCOCO (~75GB once decompressed) train2014/ : folder contains 413,915 captions json/ wav/ translations/ train_en_ja.txt train_translate.sqlite3 train_2014.sqlite3 val2014/ : folder contains 202,520 captions json/ wav/ translations/ train_en_ja.txt train_translate.sqlite3 val_2014.sqlite3 speechcoco_API/ speechcoco/ __init__.py speechcoco.py setup.py Filenames .wav files contain the spoken version of a caption .json files contain all the metadata of a given WAV file .sqlite3 files are SQLite databases containing all the information contained in the JSON files We adopted the following naming convention for both the WAV and JSON files: imageID_captionID_Speaker_DisfluencyPosition_Speed[.wav/.json] Script We created a script called speechcoco.py in order to handle the metadata and allow the user to easily find captions according to specific filters. The script uses the *.db files. Features: Aggregate all the information in the JSON files into a single SQLite database Find captions according to specific filters (name, gender and nationality of the speaker, disfluency position, speed, duration, and words in the caption). The script automatically builds the SQLite query. The user can also provide his own SQLite query. The following Python code returns all the captions spoken by a male with an American accent for which the speed was slowed down by 10% and that contain &quot;keys&quot; at any position # create SpeechCoco object db = SpeechCoco(train_2014.sqlite3, train_translate.sqlite3, verbose=True) # filter captions (returns Caption Objects) captions = db.filterCaptions(gender=&quot;Male&quot;, nationality=&quot;US&quot;, speed=0.9, text=&#39;%keys%&#39;) for caption in captions: print(&#39;\n{}\t{}\t{}\t{}\t{}\t{}\t\t{}&#39;.format(caption.imageID, caption.captionID, caption.speaker.name, caption.speaker.nationality, caption.speed, caption.filename, caption.text)) ... 298817 26763 Phil 0.9 298817_26763_Phil_None_0-9.wav A group of turkeys with bushes in the background. 108505 147972 Phil 0.9 108505_147972_Phil_Middle_0-9.wav Person using a, um, slider cell phone with blue backlit keys. 258289 154380 Bruce 0.9 258289_154380_Bruce_None_0-9.wav Some donkeys and sheep are in their green pens . 545312 201303 Phil 0.9 545312_201303_Phil_None_0-9.wav A man walking next to a couple of donkeys. ... Find all the captions belonging to a specific image captions = db.getImgCaptions(298817) for caption in captions: print(&#39;\n{}&#39;.format(caption.text)) Birds wondering through grassy ground next to bushes. A flock of turkeys are making their way up a hill. Um, ah. Two wild turkeys in a field walking around. Four wild turkeys and some bushes trees and weeds. A group of turkeys with bushes in the background. Parse the timecodes and have them structured input: ... [1926.3068, &quot;SYL&quot;, &quot;&quot;], [1926.3068, &quot;SEPR&quot;, &quot; &quot;], [1926.3068, &quot;WORD&quot;, &quot;white&quot;], [1926.3068, &quot;PHO&quot;, &quot;w&quot;], [2050.7955, &quot;PHO&quot;, &quot;ai&quot;], [2144.6591, &quot;PHO&quot;, &quot;t&quot;], [2179.3182, &quot;SYL&quot;, &quot;&quot;], [2179.3182, &quot;SEPR&quot;, &quot; &quot;] ... output: print(caption.timecode.parse()) ... { &#39;begin&#39;: 1926.3068, &#39;end&#39;: 2179.3182, &#39;syllable&#39;: [{&#39;begin&#39;: 1926.3068, &#39;end&#39;: 2179.3182, &#39;phoneme&#39;: [{&#39;begin&#39;: 1926.3068, &#39;end&#39;: 2050.7955, &#39;value&#39;: &#39;w&#39;}, {&#39;begin&#39;: 2050.7955, &#39;end&#39;: 2144.6591, &#39;value&#39;: &#39;ai&#39;}, {&#39;begin&#39;: 2144.6591, &#39;end&#39;: 2179.3182, &#39;value&#39;: &#39;t&#39;}], &#39;value&#39;: &#39;wait&#39;}], &#39;value&#39;: &#39;white&#39; }, ... Convert the timecodes to Praat TextGrid files caption.timecode.toTextgrid(outputDir, level=3) Get the words, syllables and phonemes between n seconds/milliseconds The following Python code returns all the words between 0.2 and 0.6 seconds for which at least 50% of the word&#39;s total length is within the specified interval pprint(caption.getWords(0.20, 0.60, seconds=True, level=1, olapthr=50)) ... 404537 827239 Bruce US 0.9 404537_827239_Bruce_None_0-9.wav Eyeglasses, a cellphone, some keys and other pocket items are all laid out on the cloth. . [ { &#39;begin&#39;: 0.0, &#39;end&#39;: 0.7202778, &#39;overlapPercentage&#39;: 55.53412863758955, &#39;word&#39;: &#39;eyeglasses&#39; } ] ... Get the translations of the selected captions As for now, only japanese translations are available. We also used Kytea to tokenize and tag the captions translated with Google Translate captions = db.getImgCaptions(298817) for caption in captions: print(&#39;\n{}&#39;.format(caption.text)) # Get translations and POS print(&#39;\tja_google: {}&#39;.format(db.getTranslation(caption.captionID, &quot;ja_google&quot;))) print(&#39;\t\tja_google_tokens: {}&#39;.format(db.getTokens(caption.captionID, &quot;ja_google&quot;))) print(&#39;\t\tja_google_pos: {}&#39;.format(db.getPOS(caption.captionID, &quot;ja_google&quot;))) print(&#39;\tja_excite: {}&#39;.format(db.getTranslation(caption.captionID, &quot;ja_excite&quot;))) Birds wondering through grassy ground next to bushes. ja_google: 鳥は茂みの下に茂った地面を抱えています。 ja_google_tokens: 鳥 は 茂み の 下 に 茂 っ た 地面 を 抱え て い ま す 。 ja_google_pos: 鳥/名詞/とり は/助詞/は 茂み/名詞/しげみ の/助詞/の 下/名詞/した に/助詞/に 茂/動詞/しげ っ/語尾/っ た/助動詞/た 地面/名詞/じめん を/助詞/を 抱え/動詞/かかえ て/助詞/て い/動詞/い ま/助動詞/ま す/語尾/す 。/補助記号/。 ja_excite: 低木と隣接した草深いグラウンドを通って疑う鳥。 A flock of turkeys are making their way up a hill. ja_google: 七面鳥の群れが丘を上っています。 ja_google_tokens: 七 面 鳥 の 群れ が 丘 を 上 っ て い ま す 。 ja_google_pos: 七/名詞/なな 面/名詞/めん 鳥/名詞/とり の/助詞/の 群れ/名詞/むれ が/助詞/が 丘/名詞/おか を/助詞/を 上/動詞/のぼ っ/語尾/っ て/助詞/て い/動詞/い ま/助動詞/ま す/語尾/す 。/補助記号/。 ja_excite: 七面鳥の群れは丘の上で進んでいる。 Um, ah. Two wild turkeys in a field walking around. ja_google: 野生のシチメンチョウ、野生の七面鳥 ja_google_tokens: 野生 の シチメンチョウ 、 野生 の 七 面 鳥 ja_google_pos: 野生/名詞/やせい の/助詞/の シチメンチョウ/名詞/しちめんちょう 、/補助記号/、 野生/名詞/やせい の/助詞/の 七/名詞/なな 面/名詞/めん 鳥/名詞/ちょう ja_excite: まわりで移動しているフィールドの2羽の野生の七面鳥 Four wild turkeys and some bushes trees and weeds. ja_google: 4本の野生のシチメンチョウといくつかの茂みの木と雑草 ja_google_tokens: 4 本 の 野生 の シチメンチョウ と いく つ か の 茂み の 木 と 雑草 ja_google_pos: 4/名詞/4 本/接尾辞/ほん の/助詞/の 野生/名詞/やせい の/助詞/の シチメンチョウ/名詞/しちめんちょう と/助詞/と いく/名詞/いく つ/接尾辞/つ か/助詞/か の/助詞/の 茂み/名詞/しげみ の/助詞/の 木/名詞/き と/助詞/と 雑草/名詞/ざっそう ja_excite: 4羽の野生の七面鳥およびいくつかの低木木と雑草 A group of turkeys with bushes in the background. ja_google: 背景に茂みを持つ七面鳥の群 ja_google_tokens: 背景 に 茂み を 持 つ 七 面 鳥 の 群 ja_google_pos: 背景/名詞/はいけい に/助詞/に 茂み/名詞/しげみ を/助詞/を 持/動詞/も つ/語尾/つ 七/名詞/なな 面/名詞/めん 鳥/名詞/ちょう の/助詞/の 群/名詞/むれ ja_excite: 背景の低木を持つ七面鳥のグループ  ">
    
    <link rel="alternate" type="application/x-xz" href="https://zenodo.org/records/4282267/files/train2014.tar.xz">
    <link rel="item" type="application/x-xz" href="https://zenodo.org/records/4282267/files/train2014.tar.xz">
    
    <link rel="alternate" type="application/zip" href="https://zenodo.org/records/4282267/files/speechcoco_API.zip">
    <link rel="item" type="application/zip" href="https://zenodo.org/records/4282267/files/speechcoco_API.zip">
    
    <link rel="alternate" type="application/x-xz" href="https://zenodo.org/records/4282267/files/val2014.tar.xz">
    <link rel="item" type="application/x-xz" href="https://zenodo.org/records/4282267/files/val2014.tar.xz">


<link rel="canonical" href="https://zenodo.org/records/4282267">
      <title>SPEECH-COCO</title>
      <link rel="shortcut icon" type="image/x-icon" href="https://zenodo.org/static/favicon.ico">
          <link rel="apple-touch-icon" sizes="120x120" href="https://zenodo.org/static/apple-touch-icon-120.png">
          <link rel="apple-touch-icon" sizes="152x152" href="https://zenodo.org/static/apple-touch-icon-152.png">
          <link rel="apple-touch-icon" sizes="167x167" href="https://zenodo.org/static/apple-touch-icon-167.png">
          <link rel="apple-touch-icon" sizes="180x180" href="https://zenodo.org/static/apple-touch-icon-180.png">
    
      <link rel="stylesheet" href="./SPEECH-COCO_files/3526.6cdabbfcb822ead09125.css">
      <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
      <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
      <![endif]-->
    
  <style type="text/css">.CtxtMenu_InfoClose {  top:.2em; right:.2em;}
.CtxtMenu_InfoContent {  overflow:auto; text-align:left; font-size:80%;  padding:.4em .6em; border:1px inset; margin:1em 0px;  max-height:20em; max-width:30em; background-color:#EEEEEE;  white-space:normal;}
.CtxtMenu_Info.CtxtMenu_MousePost {outline:none;}
.CtxtMenu_Info {  position:fixed; left:50%; width:auto; text-align:center;  border:3px outset; padding:1em 2em; background-color:#DDDDDD;  color:black;  cursor:default; font-family:message-box; font-size:120%;  font-style:normal; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 15px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius:15px;               /* Safari and Chrome */  -moz-border-radius:15px;                  /* Firefox */  -khtml-border-radius:15px;                /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */  filter:progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color="gray", Positive="true"); /* IE */}
</style><style type="text/css">.CtxtMenu_MenuClose {  position:absolute;  cursor:pointer;  display:inline-block;  border:2px solid #AAA;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  font-family: "Courier New", Courier;  font-size:24px;  color:#F0F0F0}
.CtxtMenu_MenuClose span {  display:block; background-color:#AAA; border:1.5px solid;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  line-height:0;  padding:8px 0 6px     /* may need to be browser-specific */}
.CtxtMenu_MenuClose:hover {  color:white!important;  border:2px solid #CCC!important}
.CtxtMenu_MenuClose:hover span {  background-color:#CCC!important}
.CtxtMenu_MenuClose:hover:focus {  outline:none}
</style><style type="text/css">.CtxtMenu_Menu {  position:absolute;  background-color:white;  color:black;  width:auto; padding:5px 0px;  border:1px solid #CCCCCC; margin:0; cursor:default;  font: menu; text-align:left; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 5px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius: 5px;             /* Safari and Chrome */  -moz-border-radius: 5px;                /* Firefox */  -khtml-border-radius: 5px;              /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */}
.CtxtMenu_MenuItem {  padding: 1px 2em;  background:transparent;}
.CtxtMenu_MenuArrow {  position:absolute; right:.5em; padding-top:.25em; color:#666666;  font-family: null; font-size: .75em}
.CtxtMenu_MenuActive .CtxtMenu_MenuArrow {color:white}
.CtxtMenu_MenuArrow.CtxtMenu_RTL {left:.5em; right:auto}
.CtxtMenu_MenuCheck {  position:absolute; left:.7em;  font-family: null}
.CtxtMenu_MenuCheck.CtxtMenu_RTL { right:.7em; left:auto }
.CtxtMenu_MenuRadioCheck {  position:absolute; left: .7em;}
.CtxtMenu_MenuRadioCheck.CtxtMenu_RTL {  right: .7em; left:auto}
.CtxtMenu_MenuInputBox {  padding-left: 1em; right:.5em; color:#666666;  font-family: null;}
.CtxtMenu_MenuInputBox.CtxtMenu_RTL {  left: .1em;}
.CtxtMenu_MenuComboBox {  left:.1em; padding-bottom:.5em;}
.CtxtMenu_MenuSlider {  left: .1em;}
.CtxtMenu_SliderValue {  position:absolute; right:.1em; padding-top:.25em; color:#333333;  font-size: .75em}
.CtxtMenu_SliderBar {  outline: none; background: #d3d3d3}
.CtxtMenu_MenuLabel {  padding: 1px 2em 3px 1.33em;  font-style:italic}
.CtxtMenu_MenuRule {  border-top: 1px solid #DDDDDD;  margin: 4px 3px;}
.CtxtMenu_MenuDisabled {  color:GrayText}
.CtxtMenu_MenuActive {  background-color: #606872;  color: white;}
.CtxtMenu_MenuDisabled:focus {  background-color: #E8E8E8}
.CtxtMenu_MenuLabel:focus {  background-color: #E8E8E8}
.CtxtMenu_ContextMenu:focus {  outline:none}
.CtxtMenu_ContextMenu .CtxtMenu_MenuItem:focus {  outline:none}
.CtxtMenu_SelectionMenu {  position:relative; float:left;  border-bottom: none; -webkit-box-shadow:none; -webkit-border-radius:0px; }
.CtxtMenu_SelectionItem {  padding-right: 1em;}
.CtxtMenu_Selection {  right: 40%; width:50%; }
.CtxtMenu_SelectionBox {  padding: 0em; max-height:20em; max-width: none;  background-color:#FFFFFF;}
.CtxtMenu_SelectionDivider {  clear: both; border-top: 2px solid #000000;}
.CtxtMenu_Menu .CtxtMenu_MenuClose {  top:-10px; left:-10px}
</style><style type="text/css">.AutoCompleteText{position:relative;z-index:100}.AutoCompleteText .input button,.AutoCompleteText .input input{border:none;border-radius:0}.AutoCompleteText ul{background-color:#fff;border:1px solid #e0e1e2;border-radius:0 0 .3rem .3rem;color:#000;list-style-type:none;margin:0;padding:0;position:absolute;text-align:left;width:100%}.AutoCompleteText ul:before{content:""}.AutoCompleteText li{cursor:pointer;padding:.3em 1em}.AutoCompleteText li:hover{background-color:#e0e1e2;border-radius:.3rem;text-decoration:underline}
/*# sourceMappingURL=data:application/json;base64,eyJ2ZXJzaW9uIjozLCJzb3VyY2VzIjpbImZpbGU6Ly8vaG9tZS9ydW5uZXIvd29yay9yZWFjdC1zZWFyY2hraXQvcmVhY3Qtc2VhcmNoa2l0L3NyYy9saWIvY29tcG9uZW50cy9BdXRvY29tcGxldGVTZWFyY2hCYXIvQXV0b2NvbXBsZXRlU2VhcmNoQmFyLnNjc3MiLCJBdXRvY29tcGxldGVTZWFyY2hCYXIuc2NzcyJdLCJuYW1lcyI6W10sIm1hcHBpbmdzIjoiQUFBQSxrQkFDRSxpQkFBQSxDQUNBLFdDQ0YsQ0RPQSwrREFDRSxXQUFBLENBQ0EsZUNDRixDREVBLHFCQU1FLHFCQUFBLENBR0Esd0JBQUEsQ0FDQSw2QkFBQSxDQUxBLFVBQUEsQ0FGQSxvQkFBQSxDQUlBLFFBQUEsQ0FDQSxTQUFBLENBTkEsaUJBQUEsQ0FFQSxlQUFBLENBSEEsVUNVRixDREVBLDRCQUNFLFVDQ0YsQ0RFQSxxQkFFRSxjQUFBLENBREEsZ0JDRUYsQ0RFQSwyQkFFRSx3QkFBQSxDQUNBLG1CQUFBLENBRkEseUJDR0YiLCJmaWxlIjoiQXV0b2NvbXBsZXRlU2VhcmNoQmFyLnNjc3MifQ== */</style><style id="MJX-CHTML-styles">
mjx-container[jax="CHTML"] {
  line-height: 0;
}

mjx-container [space="1"] {
  margin-left: .111em;
}

mjx-container [space="2"] {
  margin-left: .167em;
}

mjx-container [space="3"] {
  margin-left: .222em;
}

mjx-container [space="4"] {
  margin-left: .278em;
}

mjx-container [space="5"] {
  margin-left: .333em;
}

mjx-container [rspace="1"] {
  margin-right: .111em;
}

mjx-container [rspace="2"] {
  margin-right: .167em;
}

mjx-container [rspace="3"] {
  margin-right: .222em;
}

mjx-container [rspace="4"] {
  margin-right: .278em;
}

mjx-container [rspace="5"] {
  margin-right: .333em;
}

mjx-container [size="s"] {
  font-size: 70.7%;
}

mjx-container [size="ss"] {
  font-size: 50%;
}

mjx-container [size="Tn"] {
  font-size: 60%;
}

mjx-container [size="sm"] {
  font-size: 85%;
}

mjx-container [size="lg"] {
  font-size: 120%;
}

mjx-container [size="Lg"] {
  font-size: 144%;
}

mjx-container [size="LG"] {
  font-size: 173%;
}

mjx-container [size="hg"] {
  font-size: 207%;
}

mjx-container [size="HG"] {
  font-size: 249%;
}

mjx-container [width="full"] {
  width: 100%;
}

mjx-box {
  display: inline-block;
}

mjx-block {
  display: block;
}

mjx-itable {
  display: inline-table;
}

mjx-row {
  display: table-row;
}

mjx-row > * {
  display: table-cell;
}

mjx-mtext {
  display: inline-block;
}

mjx-mstyle {
  display: inline-block;
}

mjx-merror {
  display: inline-block;
  color: red;
  background-color: yellow;
}

mjx-mphantom {
  visibility: hidden;
}

_::-webkit-full-page-media, _:future, :root mjx-container {
  will-change: opacity;
}

mjx-assistive-mml {
  position: absolute !important;
  top: 0px;
  left: 0px;
  clip: rect(1px, 1px, 1px, 1px);
  padding: 1px 0px 0px 0px !important;
  border: 0px !important;
  display: block !important;
  width: auto !important;
  overflow: hidden !important;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

mjx-assistive-mml[display="block"] {
  width: 100% !important;
}

mjx-c::before {
  display: block;
  width: 0;
}

.MJX-TEX {
  font-family: MJXZERO, MJXTEX;
}

.TEX-B {
  font-family: MJXZERO, MJXTEX-B;
}

.TEX-I {
  font-family: MJXZERO, MJXTEX-I;
}

.TEX-MI {
  font-family: MJXZERO, MJXTEX-MI;
}

.TEX-BI {
  font-family: MJXZERO, MJXTEX-BI;
}

.TEX-S1 {
  font-family: MJXZERO, MJXTEX-S1;
}

.TEX-S2 {
  font-family: MJXZERO, MJXTEX-S2;
}

.TEX-S3 {
  font-family: MJXZERO, MJXTEX-S3;
}

.TEX-S4 {
  font-family: MJXZERO, MJXTEX-S4;
}

.TEX-A {
  font-family: MJXZERO, MJXTEX-A;
}

.TEX-C {
  font-family: MJXZERO, MJXTEX-C;
}

.TEX-CB {
  font-family: MJXZERO, MJXTEX-CB;
}

.TEX-FR {
  font-family: MJXZERO, MJXTEX-FR;
}

.TEX-FRB {
  font-family: MJXZERO, MJXTEX-FRB;
}

.TEX-SS {
  font-family: MJXZERO, MJXTEX-SS;
}

.TEX-SSB {
  font-family: MJXZERO, MJXTEX-SSB;
}

.TEX-SSI {
  font-family: MJXZERO, MJXTEX-SSI;
}

.TEX-SC {
  font-family: MJXZERO, MJXTEX-SC;
}

.TEX-T {
  font-family: MJXZERO, MJXTEX-T;
}

.TEX-V {
  font-family: MJXZERO, MJXTEX-V;
}

.TEX-VB {
  font-family: MJXZERO, MJXTEX-VB;
}

mjx-stretchy-v mjx-c, mjx-stretchy-h mjx-c {
  font-family: MJXZERO, MJXTEX-S1, MJXTEX-S4, MJXTEX, MJXTEX-A ! important;
}

@font-face /* 0 */ {
  font-family: MJXZERO;
  src: url("https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_Zero.woff") format("woff");
}

@font-face /* 1 */ {
  font-family: MJXTEX;
  src: url("https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff") format("woff");
}

@font-face /* 2 */ {
  font-family: MJXTEX-B;
  src: url("https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_Main-Bold.woff") format("woff");
}

@font-face /* 3 */ {
  font-family: MJXTEX-I;
  src: url("https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff") format("woff");
}

@font-face /* 4 */ {
  font-family: MJXTEX-MI;
  src: url("https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_Main-Italic.woff") format("woff");
}

@font-face /* 5 */ {
  font-family: MJXTEX-BI;
  src: url("https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_Math-BoldItalic.woff") format("woff");
}

@font-face /* 6 */ {
  font-family: MJXTEX-S1;
  src: url("https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff") format("woff");
}

@font-face /* 7 */ {
  font-family: MJXTEX-S2;
  src: url("https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_Size2-Regular.woff") format("woff");
}

@font-face /* 8 */ {
  font-family: MJXTEX-S3;
  src: url("https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_Size3-Regular.woff") format("woff");
}

@font-face /* 9 */ {
  font-family: MJXTEX-S4;
  src: url("https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_Size4-Regular.woff") format("woff");
}

@font-face /* 10 */ {
  font-family: MJXTEX-A;
  src: url("https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_AMS-Regular.woff") format("woff");
}

@font-face /* 11 */ {
  font-family: MJXTEX-C;
  src: url("https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Regular.woff") format("woff");
}

@font-face /* 12 */ {
  font-family: MJXTEX-CB;
  src: url("https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Bold.woff") format("woff");
}

@font-face /* 13 */ {
  font-family: MJXTEX-FR;
  src: url("https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Regular.woff") format("woff");
}

@font-face /* 14 */ {
  font-family: MJXTEX-FRB;
  src: url("https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Bold.woff") format("woff");
}

@font-face /* 15 */ {
  font-family: MJXTEX-SS;
  src: url("https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Regular.woff") format("woff");
}

@font-face /* 16 */ {
  font-family: MJXTEX-SSB;
  src: url("https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Bold.woff") format("woff");
}

@font-face /* 17 */ {
  font-family: MJXTEX-SSI;
  src: url("https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Italic.woff") format("woff");
}

@font-face /* 18 */ {
  font-family: MJXTEX-SC;
  src: url("https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_Script-Regular.woff") format("woff");
}

@font-face /* 19 */ {
  font-family: MJXTEX-T;
  src: url("https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_Typewriter-Regular.woff") format("woff");
}

@font-face /* 20 */ {
  font-family: MJXTEX-V;
  src: url("https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Regular.woff") format("woff");
}

@font-face /* 21 */ {
  font-family: MJXTEX-VB;
  src: url("https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Bold.woff") format("woff");
}
</style></head>

  <body data-invenio-config="{&quot;isMathJaxEnabled&quot;: &quot;//cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js?config=TeX-AMS-MML_HTMLorMML&quot;}" itemscope="" itemtype="http://schema.org/WebPage" data-spy="scroll" data-target=".scrollspy-target">
      <a id="skip-to-main" class="ui button primary ml-5 mt-5 skip-link" href="https://zenodo.org/records/4282267#main">Skip to main</a>
      <!--[if lt IE 8]>
        <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</p>
      <![endif]-->
    

<div>
  <header class="theme header">

    <div class="outer-navbar">
      <div class="ui container invenio-header-container">
        <nav id="invenio-nav" class="ui inverted menu borderless p-0">
          <div class="item logo p-0">
                    <a class="logo-link" href="https://zenodo.org/">
                      <img class="ui image rdm-logo" src="./SPEECH-COCO_files/invenio-rdm.svg" alt="Zenodo home">
                    </a>
          </div>

          <div id="rdm-burger-toggle">
            <button id="rdm-burger-menu-icon" class="ui button transparent" aria-label="Menu" aria-haspopup="menu" aria-expanded="false" aria-controls="invenio-menu">
              <span class="navicon" aria-hidden="true"></span>
            </button>
          </div>

          <nav id="invenio-menu" aria-labelledby="rdm-burger-menu-icon" class="ui fluid menu borderless mobile-hidden">
            <button id="rdm-close-burger-menu-icon" class="ui button transparent" aria-label="Close menu">
              <span class="navicon" aria-hidden="true"></span>
            </button>
    
    
    
      <div class="item p-0 search-bar">
        <div id="header-search-bar" data-options="[{&quot;key&quot;: &quot;records&quot;, &quot;text&quot;: &quot;All Zenodo&quot;, &quot;value&quot;: &quot;/search&quot;}]"><div class="ui fluid search right-angle-search-content"><div class="ui icon input"><input aria-label="Search records..." autocomplete="off" placeholder="Search records..." type="text" tabindex="0" class="prompt" value=""><button aria-label="Search" class="ui icon button right-floated search"><i aria-hidden="true" class="search icon"></i></button></div><div class="results transition"><div text="All Zenodo" value="/search" class="active result"><div class="flex"><div class="truncated pt-5">...</div><div class="ui label right-floated">All Zenodo</div></div></div></div></div></div>
      </div>

                  <div class="item">
                    <a href="https://zenodo.org/communities">Communities</a>
                  </div>

              
                <div class="item">
                  <a href="https://zenodo.org/me/uploads">My dashboard</a>
                </div>
              
              <div class="right menu item">
    <form>
      <a href="https://zenodo.org/login/?next=/records/4282267" class="ui button auth-button" aria-busy="false" aria-live="polite" aria-label="Log in">
        <i class="sign-in icon auth-icon" aria-hidden="true"></i>
        Log in
      </a>
      
        <a href="https://zenodo.org/signup/" class="ui button signup">
          <i class="edit outline icon"></i>
          Sign up
        </a>
      
    </form>
              </div>
          </nav>
        </nav>
      </div>
    </div>
      
      
    
      
    
    
  </header>
</div>

  <main id="main">
    <div class="invenio-page-body">
  <section id="banners" class="banners" aria-label="Information banner">

    <!-- COMMUNITY HEADER: hide it when displaying the submission request -->
    
      
    
    <!-- /COMMUNITY HEADER -->

    <!-- PREVIEW HEADER -->
    
    <!-- /PREVIEW HEADER -->

    
  </section>


  <div class="ui container">
    <div class="ui relaxed grid mt-5">
      <div class="two column row top-padded">

        
        <article class="sixteen wide tablet eleven wide computer column main-record-content">
          

              

              

              <section id="record-info" aria-label="Publication date and version number">
                <div class="ui grid middle aligned">
                  <div class="two column row">
                    <div class="left floated left aligned column">
                      <span class="ui" title="Publication date">
                        Published June 1, 2017
                      </span>
                      <span class="label text-muted"> | Version v1</span>
                    </div>
                    <div class="right floated right aligned column">

                      
                        <span role="note" class="ui label horizontal small neutral mb-5" aria-label="Resource type">
                          Dataset
                        </span>
                      

                      <span role="note" class="ui label horizontal small access-status open mb-5" data-tooltip="The record and files are publicly accessible." data-inverted="" aria-label="Access status">
                        
                          <i class="icon unlock" aria-hidden="true"></i>
                        
                        <span aria-label="The record and files are publicly accessible.">
                          Open
                        </span>
                      </span>
                    </div>
                  </div>
                </div>
              </section>
              <div class="ui divider hidden"></div><section id="record-title-section" aria-label="Record title and creators">
                <h1 id="record-title" class="wrap-overflowing-text">SPEECH-COCO</h1>

                
                  <section id="creatibutors" aria-label="Creators and contributors">

<div class="ui grid">
  
    <div class="row ui accordion affiliations">
      <div class="sixteen wide mobile twelve wide tablet thirteen wide computer column">
          <h3 class="sr-only">Creators</h3>
          <ul class="creatibutors">
            
  
  <li class="creatibutor-wrap separated">
    <a class="ui creatibutor-link" data-tooltip="Université Grenoble Alpes" href="https://zenodo.org/search?q=metadata.creators.person_or_org.name:%22William+N.+Havard%22">

      <span class="creatibutor-name">William N. Havard</span><sup class="font-tiny">1</sup></a>
  

  
    
      
        
        <a href="https://orcid.org/0000-0002-1226-4156" aria-label="William N. Havard&#39;s ORCID profile" title="William N. Havard&#39;s ORCID profile">
          <img class="ml-5 inline-id-icon" src="./SPEECH-COCO_files/orcid.svg" alt="ORCID icon">
        </a>
      
    
      
    
      
    
      
    
  

  
  
</li>
  
  <li class="creatibutor-wrap separated">
    <a class="ui creatibutor-link" data-tooltip="Université Grenoble Alpes" href="https://zenodo.org/search?q=metadata.creators.person_or_org.name:%22Laurent+Besacier%22">

      <span class="creatibutor-name">Laurent Besacier</span><sup class="font-tiny">1</sup></a>
  

  
    
      
        
        <a href="https://orcid.org/0000-0001-7411-9125" aria-label="Laurent Besacier&#39;s ORCID profile" title="Laurent Besacier&#39;s ORCID profile">
          <img class="ml-5 inline-id-icon" src="./SPEECH-COCO_files/orcid.svg" alt="ORCID icon">
        </a>
      
    
      
    
      
    
      
    
  

  
  
</li>
  
          </ul>
      </div>

        
        
          
<div class="ui sixteen wide tablet three wide computer column title right aligned bottom aligned">
  <button class="ui affiliations-button trigger button mini mr-0" aria-controls="creators-affiliations" data-open-text="Show affiliations" data-close-text="Hide affiliations" aria-expanded="false">
    Show affiliations
  </button>
</div>

<section class="ui sixteen wide column content" id="creators-affiliations" aria-label="Affiliations for creators">
  <ul>
    
    <li>
      1.

      

      Université Grenoble Alpes
    </li>
  

    </ul>
</section>

        

    </div>
  

  
</div>
                  </section>
                
              </section>



  <section id="description" class="rel-mt-2 rich-input-content" aria-label="Record description">
    <h2 id="description-heading" class="sr-only">Description</h2>
    
    <div style="word-wrap: break-word;">
      <p><strong>SpeechCoco</strong></p>

<p><em>Introduction</em></p>

<p>Our corpus is an extension of the MS COCO image recognition and captioning dataset. MS COCO comprises images paired with a set of five captions. Yet, it does not include any speech. Therefore, we used&nbsp;<a href="https://www.voxygen.fr/">Voxygen's text-to-speech system</a>&nbsp;to synthesise&nbsp;the available captions.</p>

<p>The addition of speech as a new modality enables MSCOCO to be used for researches in the field of language acquisition, unsupervised term discovery, keyword spotting, or semantic embedding using speech and vision.</p>

<p>Our corpus is licensed under a&nbsp;<a href="https://creativecommons.org/licenses/by/4.0/legalcode">Creative Commons Attribution 4.0 License</a>.</p>

<p><em>Data Set</em></p>

<ul>
	<li>
	<p>This corpus contains&nbsp;<strong>616,767</strong>&nbsp;spoken captions from MSCOCO's val2014 and train2014 subsets (respectively 414,113 for train2014 and 202,654 for val2014).</p>
	</li>
	<li>
	<p>We used 8 different voices. 4 of them have a British accent (Paul, Bronwen, Judith, and Elizabeth) and the 4 others have an American accent (Phil, Bruce, Amanda, Jenny).</p>
	</li>
	<li>
	<p>In order to make the captions sound more natural, we used SOX&nbsp;<em>tempo</em>&nbsp;command, enabling us to change the speed without changing the pitch. 1/3 of the captions are 10% slower than the original pace, 1/3 are 10% faster. The last third of the captions was kept untouched.</p>
	</li>
	<li>
	<p>We also modified approximately 30% of the original captions and added&nbsp;<strong>disfluencies</strong>&nbsp;such as "um", "uh", "er" so that the captions would sound more natural.</p>
	</li>
	<li>
	<p>Each WAV file is paired with a JSON file containing various information: timecode of each word in the caption, name of the speaker, name of the WAV file, etc. The JSON files have the following data structure:</p>
	</li>
</ul>

<pre><code class="language-json">{
    "duration": float,
    "speaker": string,
    "synthesisedCaption": string,
    "timecode": list,
    "speed": float,
    "wavFilename": string,
    "captionID": int,
    "imgID": int,
    "disfluency": list
}</code></pre>

<ul>
	<li>
	<p>On average, each caption comprises 10.79 tokens, disfluencies included. The WAV files are on average 3.52 seconds long.</p>
	</li>
</ul>

<p><em>Repository</em></p>

<p>The repository is organized as follows:</p>

<ul>
	<li>
	<p>CORPUS-MSCOCO (~75GB once decompressed)</p>

	<blockquote>
	<ul>
		<li>
		<p><strong>train2014/</strong>&nbsp;:&nbsp;folder contains 413,915 captions</p>

		<ul>
			<li>
			<p>json/</p>
			</li>
			<li>
			<p>wav/</p>
			</li>
			<li>
			<p>translations/</p>

			<ul>
				<li>
				<p>train_en_ja.txt</p>
				</li>
				<li>
				<p>train_translate.sqlite3</p>
				</li>
			</ul>
			</li>
			<li>
			<p>train_2014.sqlite3</p>
			</li>
		</ul>
		</li>
		<li>
		<p><strong>val2014/</strong>&nbsp;:&nbsp;folder contains 202,520 captions</p>

		<ul>
			<li>
			<p>json/</p>
			</li>
			<li>
			<p>wav/</p>
			</li>
			<li>
			<p>translations/</p>

			<ul>
				<li>
				<p>train_en_ja.txt</p>
				</li>
				<li>
				<p>train_translate.sqlite3</p>
				</li>
			</ul>
			</li>
			<li>
			<p>val_2014.sqlite3</p>
			</li>
		</ul>
		</li>
		<li>
		<p><strong>speechcoco_API/</strong></p>

		<ul>
			<li>
			<p>speechcoco/</p>

			<ul>
				<li>
				<p>__init__.py</p>
				</li>
				<li>
				<p>speechcoco.py</p>
				</li>
			</ul>
			</li>
			<li>
			<p>setup.py</p>
			</li>
		</ul>
		</li>
	</ul>
	</blockquote>
	</li>
</ul>

<p><em>Filenames</em></p>

<p><strong>.wav</strong>&nbsp;files contain the spoken version of a caption</p>

<p><strong>.json</strong>&nbsp;files contain all the metadata of a given WAV file</p>

<p><strong>.sqlite3</strong>&nbsp;files are SQLite databases containing all the information contained in the JSON files</p>

<p>We adopted the following naming convention for both the WAV and JSON files:</p>

<p><em>imageID_captionID_Speaker_DisfluencyPosition_Speed[.wav/.json]</em></p>

<p><em>Script</em></p>

<p>We created a script called&nbsp;<strong>speechcoco.py</strong>&nbsp;in order to handle the metadata and allow the user to easily find captions according to specific filters. The script uses the *.db files.</p>

<p>Features:</p>

<ul>
	<li>
	<p><strong>Aggregate all the information in the JSON files into a single SQLite database</strong></p>
	</li>
	<li>
	<p><strong>Find captions according to specific filters (name, gender and nationality of the speaker, disfluency position, speed, duration, and words in the caption).</strong>&nbsp;<em>The script automatically builds the SQLite query. The user can also provide his own SQLite query.</em></p>
	</li>
</ul>

<p><em>The following Python code returns all the captions spoken by a male with an American accent for which the speed was slowed&nbsp;down by 10% and that contain "keys" at any position</em></p>

<pre><code class="language-python"># create SpeechCoco object
db = SpeechCoco(train_2014.sqlite3, train_translate.sqlite3, verbose=True)

# filter captions (returns Caption Objects)
captions = db.filterCaptions(gender="Male", nationality="US", speed=0.9, text='%keys%')
for caption in captions:
    print('\n{}\t{}\t{}\t{}\t{}\t{}\t\t{}'.format(caption.imageID,
                                                  caption.captionID,
                                                  caption.speaker.name,
                                                  caption.speaker.nationality,
                                                  caption.speed,
                                                  caption.filename,
                                                  caption.text))</code></pre>

<pre><code>...
298817      26763   Phil    0.9     298817_26763_Phil_None_0-9.wav          A group of turkeys with bushes in the background.
108505      147972  Phil    0.9     108505_147972_Phil_Middle_0-9.wav               Person using a, um, slider cell phone with blue backlit keys.
258289      154380  Bruce   0.9     258289_154380_Bruce_None_0-9.wav                Some donkeys and sheep are in their green pens .
545312      201303  Phil    0.9     545312_201303_Phil_None_0-9.wav         A man walking next to a couple of donkeys.
...</code></pre>

<ul>
	<li>
	<p><strong>Find all the captions belonging to a specific image</strong></p>
	</li>
</ul>

<pre><code class="language-python">captions = db.getImgCaptions(298817)
for caption in captions:
    print('\n{}'.format(caption.text))</code></pre>

<pre><code>Birds wondering through grassy ground next to bushes.
A flock of turkeys are making their way up a hill.
Um, ah. Two wild turkeys in a field walking around.
Four wild turkeys and some bushes trees and weeds.
A group of turkeys with bushes in the background.</code></pre>

<ul>
	<li>
	<p><strong>Parse the timecodes and have them structured</strong></p>
	</li>
</ul>

<p><strong>input</strong>:</p>

<pre><code>...
[1926.3068, "SYL", ""],
[1926.3068, "SEPR", " "],
[1926.3068, "WORD", "white"],
[1926.3068, "PHO", "w"],
[2050.7955, "PHO", "ai"],
[2144.6591, "PHO", "t"],
[2179.3182, "SYL", ""],
[2179.3182, "SEPR", " "]
...</code></pre>

<p><strong>output</strong>:</p>

<pre><code class="language-python">print(caption.timecode.parse())</code></pre>

<pre><code>...
{
'begin': 1926.3068,
'end': 2179.3182,
'syllable': [{'begin': 1926.3068,
              'end': 2179.3182,
              'phoneme': [{'begin': 1926.3068,
                           'end': 2050.7955,
                           'value': 'w'},
                          {'begin': 2050.7955,
                           'end': 2144.6591,
                           'value': 'ai'},
                          {'begin': 2144.6591,
                           'end': 2179.3182,
                           'value': 't'}],
              'value': 'wait'}],
'value': 'white'
},
...</code></pre>

<ul>
	<li>
	<p><strong>Convert the timecodes to Praat TextGrid files</strong></p>
	</li>
</ul>

<pre><code class="language-python">caption.timecode.toTextgrid(outputDir, level=3)</code></pre>

<ul>
	<li>
	<p><strong>Get the words, syllables and phonemes between</strong>&nbsp;<em>n</em>&nbsp;<strong>seconds/milliseconds</strong></p>
	</li>
</ul>

<p><em>The following Python code returns all the words between 0.2 and 0.6 seconds for which at least 50% of the word's total length is within the specified interval</em></p>

<pre><code class="language-python">pprint(caption.getWords(0.20, 0.60, seconds=True, level=1, olapthr=50))</code></pre>

<pre><code>...
404537      827239  Bruce   US      0.9     404537_827239_Bruce_None_0-9.wav                Eyeglasses, a cellphone, some keys and other pocket items are all laid out on the cloth. .
[
    {
        'begin': 0.0,
        'end': 0.7202778,
        'overlapPercentage': 55.53412863758955,
        'word': 'eyeglasses'
    }
]
 ...</code></pre>

<ul>
	<li>
	<p><strong>Get the translations of the selected captions</strong></p>
	</li>
</ul>

<p><em>As for now, only japanese translations are available. We also used</em>&nbsp;<a href="http://www.phontron.com/kytea/">Kytea</a>&nbsp;<em>to tokenize and tag the captions translated with Google Translate</em></p>

<pre><code class="language-python">captions = db.getImgCaptions(298817)
for caption in captions:
    print('\n{}'.format(caption.text))

    # Get translations and POS
    print('\tja_google: {}'.format(db.getTranslation(caption.captionID, "ja_google")))
    print('\t\tja_google_tokens: {}'.format(db.getTokens(caption.captionID, "ja_google")))
    print('\t\tja_google_pos: {}'.format(db.getPOS(caption.captionID, "ja_google")))
    print('\tja_excite: {}'.format(db.getTranslation(caption.captionID, "ja_excite")))</code></pre>

<pre><code>   Birds wondering through grassy ground next to bushes.
    ja_google: 鳥は茂みの下に茂った地面を抱えています。
        ja_google_tokens: 鳥 は 茂み の 下 に 茂 っ た 地面 を 抱え て い ま す 。
        ja_google_pos: 鳥/名詞/とり は/助詞/は 茂み/名詞/しげみ の/助詞/の 下/名詞/した に/助詞/に 茂/動詞/しげ っ/語尾/っ た/助動詞/た 地面/名詞/じめん を/助詞/を 抱え/動詞/かかえ て/助詞/て い/動詞/い ま/助動詞/ま す/語尾/す 。/補助記号/。
    ja_excite: 低木と隣接した草深いグラウンドを通って疑う鳥。

A flock of turkeys are making their way up a hill.
    ja_google: 七面鳥の群れが丘を上っています。
        ja_google_tokens: 七 面 鳥 の 群れ が 丘 を 上 っ て い ま す 。
        ja_google_pos: 七/名詞/なな 面/名詞/めん 鳥/名詞/とり の/助詞/の 群れ/名詞/むれ が/助詞/が 丘/名詞/おか を/助詞/を 上/動詞/のぼ っ/語尾/っ て/助詞/て い/動詞/い ま/助動詞/ま す/語尾/す 。/補助記号/。
    ja_excite: 七面鳥の群れは丘の上で進んでいる。

Um, ah. Two wild turkeys in a field walking around.
    ja_google: 野生のシチメンチョウ、野生の七面鳥
        ja_google_tokens: 野生 の シチメンチョウ 、 野生 の 七 面 鳥
        ja_google_pos: 野生/名詞/やせい の/助詞/の シチメンチョウ/名詞/しちめんちょう 、/補助記号/、 野生/名詞/やせい の/助詞/の 七/名詞/なな 面/名詞/めん 鳥/名詞/ちょう
    ja_excite: まわりで移動しているフィールドの2羽の野生の七面鳥

Four wild turkeys and some bushes trees and weeds.
    ja_google: 4本の野生のシチメンチョウといくつかの茂みの木と雑草
        ja_google_tokens: 4 本 の 野生 の シチメンチョウ と いく つ か の 茂み の 木 と 雑草
        ja_google_pos: 4/名詞/4 本/接尾辞/ほん の/助詞/の 野生/名詞/やせい の/助詞/の シチメンチョウ/名詞/しちめんちょう と/助詞/と いく/名詞/いく つ/接尾辞/つ か/助詞/か の/助詞/の 茂み/名詞/しげみ の/助詞/の 木/名詞/き と/助詞/と 雑草/名詞/ざっそう
    ja_excite: 4羽の野生の七面鳥およびいくつかの低木木と雑草

A group of turkeys with bushes in the background.
    ja_google: 背景に茂みを持つ七面鳥の群
        ja_google_tokens: 背景 に 茂み を 持 つ 七 面 鳥 の 群
        ja_google_pos: 背景/名詞/はいけい に/助詞/に 茂み/名詞/しげみ を/助詞/を 持/動詞/も つ/語尾/つ 七/名詞/なな 面/名詞/めん 鳥/名詞/ちょう の/助詞/の 群/名詞/むれ
    ja_excite: 背景の低木を持つ七面鳥のグループ</code></pre>

<p>&nbsp;</p>
    </div>
  </section>


<section id="record-files" class="rel-mt-2 rel-mb-3" aria-label="Files"><h2 id="files-heading">Files</h2>
            
  <div class="ui accordion panel mb-10 open" href="#files-preview-accordion-panel">
    <h3 class="active title panel-heading open m-0">
      <div role="button" id="files-preview-accordion-trigger" aria-controls="files-preview-accordion-panel" aria-expanded="true" tabindex="0" class="trigger" aria-label="File preview">
        <span id="preview-file-title">speechcoco_API.zip</span>
        <i class="angle right icon" aria-hidden="true"></i>
      </div>
    </h3>
    <div role="region" id="files-preview-accordion-panel" aria-labelledby="files-preview-accordion-trigger" class="active content preview-container pt-0 open">
      <div>
        
  
  <iframe title="Preview" class="preview-iframe" id="preview-iframe" name="preview-iframe" src="./SPEECH-COCO_files/speechcoco_API.html">
  </iframe>
      </div>
    </div>
  </div>
  <div class="ui accordion panel mb-10 open" href="#files-list-accordion-panel">
    <h3 class="active title panel-heading open m-0">
      <div role="button" id="files-list-accordion-trigger" aria-controls="files-list-accordion-panel" aria-expanded="true" tabindex="0" class="trigger">
        Files
        <small class="text-muted"> (46.9 GB)</small>
        <i class="angle right icon" aria-hidden="true"></i>
      </div>
    </h3>

    <div role="region" id="files-list-accordion-panel" aria-labelledby="files-list-accordion-trigger" class="active content pt-0">
      
      <div>
        
  <table class="ui striped table files fluid open">
    <thead>
      <tr>
        <th>Name</th>
        <th>Size</th>
        <th class="">
            
            <a role="button" class="ui compact mini button right floated archive-link" href="https://zenodo.org/api/records/4282267/files-archive">
              <i class="file archive icon button" aria-hidden="true"></i> Download all
            </a>
        </th>
      </tr>
    </thead>
    <tbody>
    
      
        
        
        <tr>
          <td class="ten wide">
            <div>
              <a href="https://zenodo.org/records/4282267/files/speechcoco_API.zip?download=1">speechcoco_API.zip</a>
            </div>
            <small class="ui text-muted font-tiny">md5:8cb717d90ee91183bd737b87690600b1
            <div class="ui icon inline-block" data-tooltip="This is the file fingerprint (checksum), which can be used to verify the file integrity.">
              <i class="question circle checksum icon"></i>
            </div>
            </small>
          </td>
          <td>7.2 kB</td>
          <td class="right aligned">
            <span>
              
                <a role="button" class="ui compact mini button preview-link" href="https://zenodo.org/records/4282267/preview/speechcoco_API.zip?include_deleted=0" target="preview-iframe" data-file-key="speechcoco_API.zip">
                  <i class="eye icon" aria-hidden="true"></i>Preview
                </a>
              
              <a role="button" class="ui compact mini button" href="https://zenodo.org/records/4282267/files/speechcoco_API.zip?download=1">
                <i class="download icon" aria-hidden="true"></i>Download
              </a>
            </span>
          </td>
        </tr>
      
    
      
        
        
        <tr>
          <td class="ten wide">
            <div>
              <a href="https://zenodo.org/records/4282267/files/train2014.tar.xz?download=1">train2014.tar.xz</a>
            </div>
            <small class="ui text-muted font-tiny">md5:418c096d84169d562ea32594c00f218b
            <div class="ui icon inline-block" data-tooltip="This is the file fingerprint (checksum), which can be used to verify the file integrity.">
              <i class="question circle checksum icon"></i>
            </div>
            </small>
          </td>
          <td>31.5 GB</td>
          <td class="right aligned">
            <span>
              
              <a role="button" class="ui compact mini button" href="https://zenodo.org/records/4282267/files/train2014.tar.xz?download=1">
                <i class="download icon" aria-hidden="true"></i>Download
              </a>
            </span>
          </td>
        </tr>
      
    
      
        
        
        <tr>
          <td class="ten wide">
            <div>
              <a href="https://zenodo.org/records/4282267/files/val2014.tar.xz?download=1">val2014.tar.xz</a>
            </div>
            <small class="ui text-muted font-tiny">md5:16dc7bdbf375e400c47918f0fc25db49
            <div class="ui icon inline-block" data-tooltip="This is the file fingerprint (checksum), which can be used to verify the file integrity.">
              <i class="question circle checksum icon"></i>
            </div>
            </small>
          </td>
          <td>15.4 GB</td>
          <td class="right aligned">
            <span>
              
              <a role="button" class="ui compact mini button" href="https://zenodo.org/records/4282267/files/val2014.tar.xz?download=1">
                <i class="download icon" aria-hidden="true"></i>Download
              </a>
            </span>
          </td>
        </tr>
      
    
    </tbody>
  </table>
      </div>
    </div>
  </div>

    </section>
              

  <section id="additional-details" class="rel-mt-2" aria-label="Additional record details">











  <h2 id="record-details-heading">Additional details</h2>

  <div class="ui divider"></div>

  

  

  
    
      <div class="ui grid">
        <div class="sixteen wide mobile four wide tablet three wide computer column">
          <h3 class="ui header">Related works</h3>
        </div>
        <div class="sixteen wide mobile twelve wide tablet thirteen wide computer column">
          
  <dl class="details-list">
      <dt class="ui tiny header">Is documented by</dt>
      
  
    <dd>
      
        Conference paper:
      

      
      
      
        <a href="https://doi.org/10.21437/GLU.2017-9" target="_blank" title="Opens in new tab">
          10.21437/GLU.2017-9
        </a>
      

      
         (DOI)
      
    </dd>
  

      <dt class="ui tiny header">Is new version of</dt>
      
  
    <dd>
      
        Dataset:
      

      
      
      
        <a href="https://doi.org/10.18709/perscido.2017.06.ds80" target="_blank" title="Opens in new tab">
          10.18709/perscido.2017.06.ds80
        </a>
      

      
         (DOI)
      
    </dd>
  

  </dl>

        </div>
      </div>
    
    <div class="ui divider"></div>
  

  

  

  
      
      
      
      
        
      
        
      
        
      

      
      
      
      
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      

      
      
      
      
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      

      
  

  
    
      <div class="ui grid">
        <div class="sixteen wide mobile four wide tablet three wide computer column">
          <h3 class="ui header">References</h3>
        </div>
        <div class="sixteen wide mobile twelve wide tablet thirteen wide computer column">
          
  <ul class="ui bulleted list details-list">
    
      
      
      <li class="item">SPEECH-COCO: 600k Visually Grounded Spoken Captions Aligned to MSCOCO Data Set</li>
    
  </ul>

        </div>
      </div>
    
    <div class="ui divider"></div>
  



  

  </section>
    
    <section id="citations-search" data-record-pids="{&quot;doi&quot;: {&quot;client&quot;: &quot;datacite&quot;, &quot;identifier&quot;: &quot;10.5281/zenodo.4282267&quot;, &quot;provider&quot;: &quot;datacite&quot;}, &quot;oai&quot;: {&quot;identifier&quot;: &quot;oai:zenodo.org:4282267&quot;, &quot;provider&quot;: &quot;oai&quot;}}" data-record-parent-pids="{&quot;doi&quot;: {&quot;client&quot;: &quot;datacite&quot;, &quot;identifier&quot;: &quot;10.5281/zenodo.4282266&quot;, &quot;provider&quot;: &quot;datacite&quot;}}" data-citations-endpoint="https://zenodo-broker.web.cern.ch/api/relationships" aria-label="Record citations" class="rel-mb-1"><div class="accordion ui panel"><div class="active title panel-heading">Citations<a href="https://support.zenodo.org/help/en-gb/25-citations" target="_blank" class="ml-5 mr-5" rel="noreferrer" aria-label="Citations help page"><i aria-hidden="true" class="question circle icon button"></i></a><i aria-hidden="true" class="angle right icon"></i></div><div class="content active"><div class="ui padded grid"><div class="row"><div class="ten wide column"><div class="ui grid"><div class="three wide computer sixteen wide mobile four wide tablet column"><div class="ui tiny header"><b>Show only:</b></div></div><div class="thirteen wide computer sixteen wide mobile twelve wide tablet column"><div role="list" class="ui horizontal list filter-list"><div role="listitem" class="item"><div class="ui disabled checkbox"><input class="hidden" disabled="" id="literature-facet-checkbox" readonly="" tabindex="-1" type="checkbox" value=""><label for="literature-facet-checkbox">literature (0)</label></div></div><div role="listitem" class="item"><div class="ui disabled checkbox"><input class="hidden" disabled="" id="dataset-facet-checkbox" readonly="" tabindex="-1" type="checkbox" value=""><label for="dataset-facet-checkbox">dataset (0)</label></div></div><div role="listitem" class="item"><div class="ui disabled checkbox"><input class="hidden" disabled="" id="software-facet-checkbox" readonly="" tabindex="-1" type="checkbox" value=""><label for="software-facet-checkbox">software (0)</label></div></div><div role="listitem" class="item"><div class="ui disabled checkbox"><input class="hidden" disabled="" id="unknown-facet-checkbox" readonly="" tabindex="-1" type="checkbox" value=""><label for="unknown-facet-checkbox">unknown (0)</label></div></div><div role="listitem" class="item"><div class="ui checkbox"><input class="hidden" id="citations-to-version" readonly="" tabindex="0" type="checkbox" value=""><label for="citations-to-version">Citations to this version</label></div></div></div></div></div></div><div class="six wide column"><label class="ui label hidden" for="citations-search-bar">Search citations</label><div class="ui fluid action input"><input name="citations-search" id="citations-search-bar" placeholder="Search for citation ..." type="text" value=""><button class="ui button">Search</button></div></div></div></div><div class="ui grid mt-0"><div class="row pt-0"><div class="column"><table class="ui striped table citations"><tbody class=""></tbody></table><p class="rel-pl-1 rel-pr-1"><i>No citations found</i></p></div></div></div><div class="ui padded grid"><div class="row"><div class="center aligned thirteen wide computer sixteen wide mobile ten wide tablet column pr-0"></div><div class="right aligned three wide computer sixteen wide mobile six wide tablet column pl-0"></div></div></div></div></div></section>
  
        </article>

        
        <aside class="sixteen wide tablet five wide computer column sidebar" aria-label="Record details">
          




<section id="metrics" aria-label="Metrics" class="ui segment rdm-sidebar sidebar-container">


<div class="ui tiny two statistics rel-mt-1">
  
  

  <div class="ui statistic">
    <div class="value">2K</div>
    <div class="label">
      <i aria-hidden="true" class="eye icon"></i>
      Views
    </div>
  </div>

  <div class="ui statistic">
    <div class="value">665</div>
    <div class="label">
      <i aria-hidden="true" class="download icon"></i>
      Downloads
    </div>
  </div>
</div>

<div class="ui accordion rel-mt-1 centered">
  <div class="title">
    <i class="caret right icon" aria-hidden="true"></i>
    <span tabindex="0" class="trigger" data-open-text="Show more details" data-close-text="Show less details">
      Show more details
    </span>
  </div>

  <div class="content">
    <table id="record-statistics" class="ui definition table fluid">
      <thead>
        <tr>
          <th></th>
          <th class="right aligned">All versions</th>
          <th class="right aligned">This version</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>
            Views
            <i tabindex="0" role="button" style="position:relative" class="popup-trigger question circle small icon" aria-expanded="false" aria-label="More info" data-variation="mini inverted">
            </i>
            <p role="tooltip" class="popup-content ui flowing popup transition hidden">
              Total views
            </p>
          </td>
          <td data-label="All versions" class="right aligned">
  
    1,849
  
</td>
          <td data-label="This version" class="right aligned">
  
    1,841
  
</td>
        </tr>
        <tr>
          <td>
            Downloads
            <i tabindex="0" role="button" style="position:relative" class="popup-trigger question circle small icon" aria-expanded="false" aria-label="More info" data-variation="mini inverted">
            </i>
            <p role="tooltip" class="popup-content ui flowing popup transition hidden">
              Total downloads
            </p>
          </td>
          <td data-label="All versions" class="right aligned">
  
    665
  
</td>
          <td data-label="This version" class="right aligned">
  
    653
  
</td>
        </tr>
        <tr>
          <td>
            Data volume
            <i tabindex="0" role="button" style="position:relative" class="popup-trigger question circle small icon" aria-expanded="false" aria-label="More info" data-variation="mini inverted">
            </i>
            <p role="tooltip" class="popup-content ui flowing popup transition hidden">
              Total data volume
            </p>
          </td>

          <td data-label="All versions" class="right aligned">33.3 TB</td>
          <td data-label="This version" class="right aligned">33.0 TB</td>
        </tr>
      </tbody>
    </table>
    <p class="text-align-center rel-mt-1">
      <small>
        <a href="https://zenodo.org/help/statistics">More info on how stats are collected....</a>
      </small>
    </p>
  </div>

</div>

</section>

<div class="sidebar-container">
  <h2 class="ui medium top attached header mt-0">Versions</h2>
  <div id="record-versions" class="ui segment rdm-sidebar bottom attached pl-0 pr-0 pt-0">
    <div class="versions">
      <div id="recordVersions" data-record="{&quot;access&quot;: {&quot;embargo&quot;: {&quot;active&quot;: false, &quot;reason&quot;: null}, &quot;files&quot;: &quot;public&quot;, &quot;record&quot;: &quot;public&quot;, &quot;status&quot;: &quot;open&quot;}, &quot;created&quot;: &quot;2020-11-23T13:46:31.442227+00:00&quot;, &quot;custom_fields&quot;: {&quot;meeting:meeting&quot;: {&quot;acronym&quot;: &quot;GLU2017&quot;, &quot;dates&quot;: &quot;2017&quot;, &quot;place&quot;: &quot;Stockholm, Sweden&quot;, &quot;title&quot;: &quot;International Workshop on Grounding Language Understanding&quot;}}, &quot;deletion_status&quot;: {&quot;is_deleted&quot;: false, &quot;status&quot;: &quot;P&quot;}, &quot;expanded&quot;: {&quot;parent&quot;: {&quot;access&quot;: {&quot;owned_by&quot;: {&quot;active&quot;: null, &quot;blocked_at&quot;: null, &quot;confirmed_at&quot;: null, &quot;email&quot;: &quot;&quot;, &quot;id&quot;: &quot;55623&quot;, &quot;is_current_user&quot;: false, &quot;links&quot;: {&quot;avatar&quot;: &quot;https://zenodo.org/api/users/55623/avatar.svg&quot;, &quot;records_html&quot;: &quot;https://zenodo.org/search/records?q=parent.access.owned_by.user:55623&quot;, &quot;self&quot;: &quot;https://zenodo.org/api/users/55623&quot;}, &quot;profile&quot;: {&quot;affiliations&quot;: &quot;&quot;, &quot;full_name&quot;: &quot;&quot;}, &quot;username&quot;: &quot;WilliamNH&quot;, &quot;verified_at&quot;: null}}}}, &quot;files&quot;: {&quot;count&quot;: 3, &quot;enabled&quot;: true, &quot;entries&quot;: {&quot;speechcoco_API.zip&quot;: {&quot;access&quot;: {&quot;hidden&quot;: false}, &quot;checksum&quot;: &quot;md5:8cb717d90ee91183bd737b87690600b1&quot;, &quot;ext&quot;: &quot;zip&quot;, &quot;id&quot;: &quot;c85b78fe-1790-4a61-b03c-1bfd19a9c9ca&quot;, &quot;key&quot;: &quot;speechcoco_API.zip&quot;, &quot;links&quot;: {&quot;content&quot;: &quot;https://zenodo.org/api/records/4282267/files/speechcoco_API.zip/content&quot;, &quot;self&quot;: &quot;https://zenodo.org/api/records/4282267/files/speechcoco_API.zip&quot;}, &quot;metadata&quot;: null, &quot;mimetype&quot;: &quot;application/zip&quot;, &quot;size&quot;: 7171, &quot;storage_class&quot;: &quot;L&quot;}, &quot;train2014.tar.xz&quot;: {&quot;access&quot;: {&quot;hidden&quot;: false}, &quot;checksum&quot;: &quot;md5:418c096d84169d562ea32594c00f218b&quot;, &quot;ext&quot;: &quot;xz&quot;, &quot;id&quot;: &quot;9105f916-ede9-4938-b76a-ecbb4e1b9eea&quot;, &quot;key&quot;: &quot;train2014.tar.xz&quot;, &quot;links&quot;: {&quot;content&quot;: &quot;https://zenodo.org/api/records/4282267/files/train2014.tar.xz/content&quot;, &quot;self&quot;: &quot;https://zenodo.org/api/records/4282267/files/train2014.tar.xz&quot;}, &quot;metadata&quot;: null, &quot;mimetype&quot;: &quot;application/x-xz&quot;, &quot;size&quot;: 31525496168, &quot;storage_class&quot;: &quot;L&quot;}, &quot;val2014.tar.xz&quot;: {&quot;access&quot;: {&quot;hidden&quot;: false}, &quot;checksum&quot;: &quot;md5:16dc7bdbf375e400c47918f0fc25db49&quot;, &quot;ext&quot;: &quot;xz&quot;, &quot;id&quot;: &quot;d5fe8b3f-ed94-4308-9b14-70158f631985&quot;, &quot;key&quot;: &quot;val2014.tar.xz&quot;, &quot;links&quot;: {&quot;content&quot;: &quot;https://zenodo.org/api/records/4282267/files/val2014.tar.xz/content&quot;, &quot;self&quot;: &quot;https://zenodo.org/api/records/4282267/files/val2014.tar.xz&quot;}, &quot;metadata&quot;: null, &quot;mimetype&quot;: &quot;application/x-xz&quot;, &quot;size&quot;: 15391031688, &quot;storage_class&quot;: &quot;L&quot;}}, &quot;order&quot;: [], &quot;total_bytes&quot;: 46916535027}, &quot;id&quot;: &quot;4282267&quot;, &quot;is_draft&quot;: false, &quot;is_published&quot;: true, &quot;links&quot;: {&quot;access&quot;: &quot;https://zenodo.org/api/records/4282267/access&quot;, &quot;access_grants&quot;: &quot;https://zenodo.org/api/records/4282267/access/grants&quot;, &quot;access_links&quot;: &quot;https://zenodo.org/api/records/4282267/access/links&quot;, &quot;access_request&quot;: &quot;https://zenodo.org/api/records/4282267/access/request&quot;, &quot;access_users&quot;: &quot;https://zenodo.org/api/records/4282267/access/users&quot;, &quot;archive&quot;: &quot;https://zenodo.org/api/records/4282267/files-archive&quot;, &quot;archive_media&quot;: &quot;https://zenodo.org/api/records/4282267/media-files-archive&quot;, &quot;communities&quot;: &quot;https://zenodo.org/api/records/4282267/communities&quot;, &quot;communities-suggestions&quot;: &quot;https://zenodo.org/api/records/4282267/communities-suggestions&quot;, &quot;doi&quot;: &quot;https://doi.org/10.5281/zenodo.4282267&quot;, &quot;draft&quot;: &quot;https://zenodo.org/api/records/4282267/draft&quot;, &quot;files&quot;: &quot;https://zenodo.org/api/records/4282267/files&quot;, &quot;latest&quot;: &quot;https://zenodo.org/api/records/4282267/versions/latest&quot;, &quot;latest_html&quot;: &quot;https://zenodo.org/records/4282267/latest&quot;, &quot;media_files&quot;: &quot;https://zenodo.org/api/records/4282267/media-files&quot;, &quot;parent&quot;: &quot;https://zenodo.org/api/records/4282266&quot;, &quot;parent_doi&quot;: &quot;https://doi.org/10.5281/zenodo.4282266&quot;, &quot;parent_doi_html&quot;: &quot;https://zenodo.org/doi/10.5281/zenodo.4282266&quot;, &quot;parent_html&quot;: &quot;https://zenodo.org/records/4282266&quot;, &quot;preview_html&quot;: &quot;https://zenodo.org/records/4282267?preview=1&quot;, &quot;requests&quot;: &quot;https://zenodo.org/api/records/4282267/requests&quot;, &quot;reserve_doi&quot;: &quot;https://zenodo.org/api/records/4282267/draft/pids/doi&quot;, &quot;self&quot;: &quot;https://zenodo.org/api/records/4282267&quot;, &quot;self_doi&quot;: &quot;https://doi.org/10.5281/zenodo.4282267&quot;, &quot;self_doi_html&quot;: &quot;https://zenodo.org/doi/10.5281/zenodo.4282267&quot;, &quot;self_html&quot;: &quot;https://zenodo.org/records/4282267&quot;, &quot;self_iiif_manifest&quot;: &quot;https://zenodo.org/api/iiif/record:4282267/manifest&quot;, &quot;self_iiif_sequence&quot;: &quot;https://zenodo.org/api/iiif/record:4282267/sequence/default&quot;, &quot;versions&quot;: &quot;https://zenodo.org/api/records/4282267/versions&quot;}, &quot;media_files&quot;: {&quot;count&quot;: 0, &quot;enabled&quot;: false, &quot;entries&quot;: {}, &quot;order&quot;: [], &quot;total_bytes&quot;: 0}, &quot;metadata&quot;: {&quot;creators&quot;: [{&quot;affiliations&quot;: [{&quot;name&quot;: &quot;Universit\u00e9 Grenoble Alpes&quot;}], &quot;person_or_org&quot;: {&quot;family_name&quot;: &quot;William N. Havard&quot;, &quot;identifiers&quot;: [{&quot;identifier&quot;: &quot;0000-0002-1226-4156&quot;, &quot;scheme&quot;: &quot;orcid&quot;}], &quot;name&quot;: &quot;William N. Havard&quot;, &quot;type&quot;: &quot;personal&quot;}}, {&quot;affiliations&quot;: [{&quot;name&quot;: &quot;Universit\u00e9 Grenoble Alpes&quot;}], &quot;person_or_org&quot;: {&quot;family_name&quot;: &quot;Laurent Besacier&quot;, &quot;identifiers&quot;: [{&quot;identifier&quot;: &quot;0000-0001-7411-9125&quot;, &quot;scheme&quot;: &quot;orcid&quot;}], &quot;name&quot;: &quot;Laurent Besacier&quot;, &quot;type&quot;: &quot;personal&quot;}}], &quot;description&quot;: &quot;\u003cp\u003e\u003cstrong\u003eSpeechCoco\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eIntroduction\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eOur corpus is an extension of the MS COCO image recognition and captioning dataset. MS COCO comprises images paired with a set of five captions. Yet, it does not include any speech. Therefore, we used\u0026nbsp;\u003ca href=\&quot;https://www.voxygen.fr/\&quot;\u003eVoxygen\u0026#39;s text-to-speech system\u003c/a\u003e\u0026nbsp;to synthesise\u0026nbsp;the available captions.\u003c/p\u003e\n\n\u003cp\u003eThe addition of speech as a new modality enables MSCOCO to be used for researches in the field of language acquisition, unsupervised term discovery, keyword spotting, or semantic embedding using speech and vision.\u003c/p\u003e\n\n\u003cp\u003eOur corpus is licensed under a\u0026nbsp;\u003ca href=\&quot;https://creativecommons.org/licenses/by/4.0/legalcode\&quot;\u003eCreative Commons Attribution 4.0 License\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eData Set\u003c/em\u003e\u003c/p\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003eThis corpus contains\u0026nbsp;\u003cstrong\u003e616,767\u003c/strong\u003e\u0026nbsp;spoken captions from MSCOCO\u0026#39;s val2014 and train2014 subsets (respectively 414,113 for train2014 and 202,654 for val2014).\u003c/p\u003e\n\t\u003c/li\u003e\n\t\u003cli\u003e\n\t\u003cp\u003eWe used 8 different voices. 4 of them have a British accent (Paul, Bronwen, Judith, and Elizabeth) and the 4 others have an American accent (Phil, Bruce, Amanda, Jenny).\u003c/p\u003e\n\t\u003c/li\u003e\n\t\u003cli\u003e\n\t\u003cp\u003eIn order to make the captions sound more natural, we used SOX\u0026nbsp;\u003cem\u003etempo\u003c/em\u003e\u0026nbsp;command, enabling us to change the speed without changing the pitch. 1/3 of the captions are 10% slower than the original pace, 1/3 are 10% faster. The last third of the captions was kept untouched.\u003c/p\u003e\n\t\u003c/li\u003e\n\t\u003cli\u003e\n\t\u003cp\u003eWe also modified approximately 30% of the original captions and added\u0026nbsp;\u003cstrong\u003edisfluencies\u003c/strong\u003e\u0026nbsp;such as \u0026quot;um\u0026quot;, \u0026quot;uh\u0026quot;, \u0026quot;er\u0026quot; so that the captions would sound more natural.\u003c/p\u003e\n\t\u003c/li\u003e\n\t\u003cli\u003e\n\t\u003cp\u003eEach WAV file is paired with a JSON file containing various information: timecode of each word in the caption, name of the speaker, name of the WAV file, etc. The JSON files have the following data structure:\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cpre\u003e\u003ccode class=\&quot;language-json\&quot;\u003e{\n    \&quot;duration\&quot;: float,\n    \&quot;speaker\&quot;: string,\n    \&quot;synthesisedCaption\&quot;: string,\n    \&quot;timecode\&quot;: list,\n    \&quot;speed\&quot;: float,\n    \&quot;wavFilename\&quot;: string,\n    \&quot;captionID\&quot;: int,\n    \&quot;imgID\&quot;: int,\n    \&quot;disfluency\&quot;: list\n}\u003c/code\u003e\u003c/pre\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003eOn average, each caption comprises 10.79 tokens, disfluencies included. The WAV files are on average 3.52 seconds long.\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cem\u003eRepository\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eThe repository is organized as follows:\u003c/p\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003eCORPUS-MSCOCO (~75GB once decompressed)\u003c/p\u003e\n\n\t\u003cblockquote\u003e\n\t\u003cul\u003e\n\t\t\u003cli\u003e\n\t\t\u003cp\u003e\u003cstrong\u003etrain2014/\u003c/strong\u003e\u0026nbsp;:\u0026nbsp;folder contains 413,915 captions\u003c/p\u003e\n\n\t\t\u003cul\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003ejson/\u003c/p\u003e\n\t\t\t\u003c/li\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003ewav/\u003c/p\u003e\n\t\t\t\u003c/li\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003etranslations/\u003c/p\u003e\n\n\t\t\t\u003cul\u003e\n\t\t\t\t\u003cli\u003e\n\t\t\t\t\u003cp\u003etrain_en_ja.txt\u003c/p\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\n\t\t\t\t\u003cp\u003etrain_translate.sqlite3\u003c/p\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\u003c/ul\u003e\n\t\t\t\u003c/li\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003etrain_2014.sqlite3\u003c/p\u003e\n\t\t\t\u003c/li\u003e\n\t\t\u003c/ul\u003e\n\t\t\u003c/li\u003e\n\t\t\u003cli\u003e\n\t\t\u003cp\u003e\u003cstrong\u003eval2014/\u003c/strong\u003e\u0026nbsp;:\u0026nbsp;folder contains 202,520 captions\u003c/p\u003e\n\n\t\t\u003cul\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003ejson/\u003c/p\u003e\n\t\t\t\u003c/li\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003ewav/\u003c/p\u003e\n\t\t\t\u003c/li\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003etranslations/\u003c/p\u003e\n\n\t\t\t\u003cul\u003e\n\t\t\t\t\u003cli\u003e\n\t\t\t\t\u003cp\u003etrain_en_ja.txt\u003c/p\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\n\t\t\t\t\u003cp\u003etrain_translate.sqlite3\u003c/p\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\u003c/ul\u003e\n\t\t\t\u003c/li\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003eval_2014.sqlite3\u003c/p\u003e\n\t\t\t\u003c/li\u003e\n\t\t\u003c/ul\u003e\n\t\t\u003c/li\u003e\n\t\t\u003cli\u003e\n\t\t\u003cp\u003e\u003cstrong\u003espeechcoco_API/\u003c/strong\u003e\u003c/p\u003e\n\n\t\t\u003cul\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003espeechcoco/\u003c/p\u003e\n\n\t\t\t\u003cul\u003e\n\t\t\t\t\u003cli\u003e\n\t\t\t\t\u003cp\u003e__init__.py\u003c/p\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\n\t\t\t\t\u003cp\u003espeechcoco.py\u003c/p\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\u003c/ul\u003e\n\t\t\t\u003c/li\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003esetup.py\u003c/p\u003e\n\t\t\t\u003c/li\u003e\n\t\t\u003c/ul\u003e\n\t\t\u003c/li\u003e\n\t\u003c/ul\u003e\n\t\u003c/blockquote\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cem\u003eFilenames\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e.wav\u003c/strong\u003e\u0026nbsp;files contain the spoken version of a caption\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e.json\u003c/strong\u003e\u0026nbsp;files contain all the metadata of a given WAV file\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e.sqlite3\u003c/strong\u003e\u0026nbsp;files are SQLite databases containing all the information contained in the JSON files\u003c/p\u003e\n\n\u003cp\u003eWe adopted the following naming convention for both the WAV and JSON files:\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eimageID_captionID_Speaker_DisfluencyPosition_Speed[.wav/.json]\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eScript\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eWe created a script called\u0026nbsp;\u003cstrong\u003espeechcoco.py\u003c/strong\u003e\u0026nbsp;in order to handle the metadata and allow the user to easily find captions according to specific filters. The script uses the *.db files.\u003c/p\u003e\n\n\u003cp\u003eFeatures:\u003c/p\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003e\u003cstrong\u003eAggregate all the information in the JSON files into a single SQLite database\u003c/strong\u003e\u003c/p\u003e\n\t\u003c/li\u003e\n\t\u003cli\u003e\n\t\u003cp\u003e\u003cstrong\u003eFind captions according to specific filters (name, gender and nationality of the speaker, disfluency position, speed, duration, and words in the caption).\u003c/strong\u003e\u0026nbsp;\u003cem\u003eThe script automatically builds the SQLite query. The user can also provide his own SQLite query.\u003c/em\u003e\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cem\u003eThe following Python code returns all the captions spoken by a male with an American accent for which the speed was slowed\u0026nbsp;down by 10% and that contain \u0026quot;keys\u0026quot; at any position\u003c/em\u003e\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\&quot;language-python\&quot;\u003e# create SpeechCoco object\ndb = SpeechCoco(train_2014.sqlite3, train_translate.sqlite3, verbose=True)\n\n# filter captions (returns Caption Objects)\ncaptions = db.filterCaptions(gender=\&quot;Male\&quot;, nationality=\&quot;US\&quot;, speed=0.9, text=\u0027%keys%\u0027)\nfor caption in captions:\n    print(\u0027\\n{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t\\t{}\u0027.format(caption.imageID,\n                                                  caption.captionID,\n                                                  caption.speaker.name,\n                                                  caption.speaker.nationality,\n                                                  caption.speed,\n                                                  caption.filename,\n                                                  caption.text))\u003c/code\u003e\u003c/pre\u003e\n\n\u003cpre\u003e\u003ccode\u003e...\n298817      26763   Phil    0.9     298817_26763_Phil_None_0-9.wav          A group of turkeys with bushes in the background.\n108505      147972  Phil    0.9     108505_147972_Phil_Middle_0-9.wav               Person using a, um, slider cell phone with blue backlit keys.\n258289      154380  Bruce   0.9     258289_154380_Bruce_None_0-9.wav                Some donkeys and sheep are in their green pens .\n545312      201303  Phil    0.9     545312_201303_Phil_None_0-9.wav         A man walking next to a couple of donkeys.\n...\u003c/code\u003e\u003c/pre\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003e\u003cstrong\u003eFind all the captions belonging to a specific image\u003c/strong\u003e\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cpre\u003e\u003ccode class=\&quot;language-python\&quot;\u003ecaptions = db.getImgCaptions(298817)\nfor caption in captions:\n    print(\u0027\\n{}\u0027.format(caption.text))\u003c/code\u003e\u003c/pre\u003e\n\n\u003cpre\u003e\u003ccode\u003eBirds wondering through grassy ground next to bushes.\nA flock of turkeys are making their way up a hill.\nUm, ah. Two wild turkeys in a field walking around.\nFour wild turkeys and some bushes trees and weeds.\nA group of turkeys with bushes in the background.\u003c/code\u003e\u003c/pre\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003e\u003cstrong\u003eParse the timecodes and have them structured\u003c/strong\u003e\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cstrong\u003einput\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003e...\n[1926.3068, \&quot;SYL\&quot;, \&quot;\&quot;],\n[1926.3068, \&quot;SEPR\&quot;, \&quot; \&quot;],\n[1926.3068, \&quot;WORD\&quot;, \&quot;white\&quot;],\n[1926.3068, \&quot;PHO\&quot;, \&quot;w\&quot;],\n[2050.7955, \&quot;PHO\&quot;, \&quot;ai\&quot;],\n[2144.6591, \&quot;PHO\&quot;, \&quot;t\&quot;],\n[2179.3182, \&quot;SYL\&quot;, \&quot;\&quot;],\n[2179.3182, \&quot;SEPR\&quot;, \&quot; \&quot;]\n...\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\u003cstrong\u003eoutput\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\&quot;language-python\&quot;\u003eprint(caption.timecode.parse())\u003c/code\u003e\u003c/pre\u003e\n\n\u003cpre\u003e\u003ccode\u003e...\n{\n\u0027begin\u0027: 1926.3068,\n\u0027end\u0027: 2179.3182,\n\u0027syllable\u0027: [{\u0027begin\u0027: 1926.3068,\n              \u0027end\u0027: 2179.3182,\n              \u0027phoneme\u0027: [{\u0027begin\u0027: 1926.3068,\n                           \u0027end\u0027: 2050.7955,\n                           \u0027value\u0027: \u0027w\u0027},\n                          {\u0027begin\u0027: 2050.7955,\n                           \u0027end\u0027: 2144.6591,\n                           \u0027value\u0027: \u0027ai\u0027},\n                          {\u0027begin\u0027: 2144.6591,\n                           \u0027end\u0027: 2179.3182,\n                           \u0027value\u0027: \u0027t\u0027}],\n              \u0027value\u0027: \u0027wait\u0027}],\n\u0027value\u0027: \u0027white\u0027\n},\n...\u003c/code\u003e\u003c/pre\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003e\u003cstrong\u003eConvert the timecodes to Praat TextGrid files\u003c/strong\u003e\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cpre\u003e\u003ccode class=\&quot;language-python\&quot;\u003ecaption.timecode.toTextgrid(outputDir, level=3)\u003c/code\u003e\u003c/pre\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003e\u003cstrong\u003eGet the words, syllables and phonemes between\u003c/strong\u003e\u0026nbsp;\u003cem\u003en\u003c/em\u003e\u0026nbsp;\u003cstrong\u003eseconds/milliseconds\u003c/strong\u003e\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cem\u003eThe following Python code returns all the words between 0.2 and 0.6 seconds for which at least 50% of the word\u0026#39;s total length is within the specified interval\u003c/em\u003e\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\&quot;language-python\&quot;\u003epprint(caption.getWords(0.20, 0.60, seconds=True, level=1, olapthr=50))\u003c/code\u003e\u003c/pre\u003e\n\n\u003cpre\u003e\u003ccode\u003e...\n404537      827239  Bruce   US      0.9     404537_827239_Bruce_None_0-9.wav                Eyeglasses, a cellphone, some keys and other pocket items are all laid out on the cloth. .\n[\n    {\n        \u0027begin\u0027: 0.0,\n        \u0027end\u0027: 0.7202778,\n        \u0027overlapPercentage\u0027: 55.53412863758955,\n        \u0027word\u0027: \u0027eyeglasses\u0027\n    }\n]\n ...\u003c/code\u003e\u003c/pre\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003e\u003cstrong\u003eGet the translations of the selected captions\u003c/strong\u003e\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cem\u003eAs for now, only japanese translations are available. We also used\u003c/em\u003e\u0026nbsp;\u003ca href=\&quot;http://www.phontron.com/kytea/\&quot;\u003eKytea\u003c/a\u003e\u0026nbsp;\u003cem\u003eto tokenize and tag the captions translated with Google Translate\u003c/em\u003e\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\&quot;language-python\&quot;\u003ecaptions = db.getImgCaptions(298817)\nfor caption in captions:\n    print(\u0027\\n{}\u0027.format(caption.text))\n\n    # Get translations and POS\n    print(\u0027\\tja_google: {}\u0027.format(db.getTranslation(caption.captionID, \&quot;ja_google\&quot;)))\n    print(\u0027\\t\\tja_google_tokens: {}\u0027.format(db.getTokens(caption.captionID, \&quot;ja_google\&quot;)))\n    print(\u0027\\t\\tja_google_pos: {}\u0027.format(db.getPOS(caption.captionID, \&quot;ja_google\&quot;)))\n    print(\u0027\\tja_excite: {}\u0027.format(db.getTranslation(caption.captionID, \&quot;ja_excite\&quot;)))\u003c/code\u003e\u003c/pre\u003e\n\n\u003cpre\u003e\u003ccode\u003e   Birds wondering through grassy ground next to bushes.\n    ja_google: \u9ce5\u306f\u8302\u307f\u306e\u4e0b\u306b\u8302\u3063\u305f\u5730\u9762\u3092\u62b1\u3048\u3066\u3044\u307e\u3059\u3002\n        ja_google_tokens: \u9ce5 \u306f \u8302\u307f \u306e \u4e0b \u306b \u8302 \u3063 \u305f \u5730\u9762 \u3092 \u62b1\u3048 \u3066 \u3044 \u307e \u3059 \u3002\n        ja_google_pos: \u9ce5/\u540d\u8a5e/\u3068\u308a \u306f/\u52a9\u8a5e/\u306f \u8302\u307f/\u540d\u8a5e/\u3057\u3052\u307f \u306e/\u52a9\u8a5e/\u306e \u4e0b/\u540d\u8a5e/\u3057\u305f \u306b/\u52a9\u8a5e/\u306b \u8302/\u52d5\u8a5e/\u3057\u3052 \u3063/\u8a9e\u5c3e/\u3063 \u305f/\u52a9\u52d5\u8a5e/\u305f \u5730\u9762/\u540d\u8a5e/\u3058\u3081\u3093 \u3092/\u52a9\u8a5e/\u3092 \u62b1\u3048/\u52d5\u8a5e/\u304b\u304b\u3048 \u3066/\u52a9\u8a5e/\u3066 \u3044/\u52d5\u8a5e/\u3044 \u307e/\u52a9\u52d5\u8a5e/\u307e \u3059/\u8a9e\u5c3e/\u3059 \u3002/\u88dc\u52a9\u8a18\u53f7/\u3002\n    ja_excite: \u4f4e\u6728\u3068\u96a3\u63a5\u3057\u305f\u8349\u6df1\u3044\u30b0\u30e9\u30a6\u30f3\u30c9\u3092\u901a\u3063\u3066\u7591\u3046\u9ce5\u3002\n\nA flock of turkeys are making their way up a hill.\n    ja_google: \u4e03\u9762\u9ce5\u306e\u7fa4\u308c\u304c\u4e18\u3092\u4e0a\u3063\u3066\u3044\u307e\u3059\u3002\n        ja_google_tokens: \u4e03 \u9762 \u9ce5 \u306e \u7fa4\u308c \u304c \u4e18 \u3092 \u4e0a \u3063 \u3066 \u3044 \u307e \u3059 \u3002\n        ja_google_pos: \u4e03/\u540d\u8a5e/\u306a\u306a \u9762/\u540d\u8a5e/\u3081\u3093 \u9ce5/\u540d\u8a5e/\u3068\u308a \u306e/\u52a9\u8a5e/\u306e \u7fa4\u308c/\u540d\u8a5e/\u3080\u308c \u304c/\u52a9\u8a5e/\u304c \u4e18/\u540d\u8a5e/\u304a\u304b \u3092/\u52a9\u8a5e/\u3092 \u4e0a/\u52d5\u8a5e/\u306e\u307c \u3063/\u8a9e\u5c3e/\u3063 \u3066/\u52a9\u8a5e/\u3066 \u3044/\u52d5\u8a5e/\u3044 \u307e/\u52a9\u52d5\u8a5e/\u307e \u3059/\u8a9e\u5c3e/\u3059 \u3002/\u88dc\u52a9\u8a18\u53f7/\u3002\n    ja_excite: \u4e03\u9762\u9ce5\u306e\u7fa4\u308c\u306f\u4e18\u306e\u4e0a\u3067\u9032\u3093\u3067\u3044\u308b\u3002\n\nUm, ah. Two wild turkeys in a field walking around.\n    ja_google: \u91ce\u751f\u306e\u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6\u3001\u91ce\u751f\u306e\u4e03\u9762\u9ce5\n        ja_google_tokens: \u91ce\u751f \u306e \u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6 \u3001 \u91ce\u751f \u306e \u4e03 \u9762 \u9ce5\n        ja_google_pos: \u91ce\u751f/\u540d\u8a5e/\u3084\u305b\u3044 \u306e/\u52a9\u8a5e/\u306e \u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6/\u540d\u8a5e/\u3057\u3061\u3081\u3093\u3061\u3087\u3046 \u3001/\u88dc\u52a9\u8a18\u53f7/\u3001 \u91ce\u751f/\u540d\u8a5e/\u3084\u305b\u3044 \u306e/\u52a9\u8a5e/\u306e \u4e03/\u540d\u8a5e/\u306a\u306a \u9762/\u540d\u8a5e/\u3081\u3093 \u9ce5/\u540d\u8a5e/\u3061\u3087\u3046\n    ja_excite: \u307e\u308f\u308a\u3067\u79fb\u52d5\u3057\u3066\u3044\u308b\u30d5\u30a3\u30fc\u30eb\u30c9\u306e2\u7fbd\u306e\u91ce\u751f\u306e\u4e03\u9762\u9ce5\n\nFour wild turkeys and some bushes trees and weeds.\n    ja_google: 4\u672c\u306e\u91ce\u751f\u306e\u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6\u3068\u3044\u304f\u3064\u304b\u306e\u8302\u307f\u306e\u6728\u3068\u96d1\u8349\n        ja_google_tokens: 4 \u672c \u306e \u91ce\u751f \u306e \u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6 \u3068 \u3044\u304f \u3064 \u304b \u306e \u8302\u307f \u306e \u6728 \u3068 \u96d1\u8349\n        ja_google_pos: 4/\u540d\u8a5e/4 \u672c/\u63a5\u5c3e\u8f9e/\u307b\u3093 \u306e/\u52a9\u8a5e/\u306e \u91ce\u751f/\u540d\u8a5e/\u3084\u305b\u3044 \u306e/\u52a9\u8a5e/\u306e \u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6/\u540d\u8a5e/\u3057\u3061\u3081\u3093\u3061\u3087\u3046 \u3068/\u52a9\u8a5e/\u3068 \u3044\u304f/\u540d\u8a5e/\u3044\u304f \u3064/\u63a5\u5c3e\u8f9e/\u3064 \u304b/\u52a9\u8a5e/\u304b \u306e/\u52a9\u8a5e/\u306e \u8302\u307f/\u540d\u8a5e/\u3057\u3052\u307f \u306e/\u52a9\u8a5e/\u306e \u6728/\u540d\u8a5e/\u304d \u3068/\u52a9\u8a5e/\u3068 \u96d1\u8349/\u540d\u8a5e/\u3056\u3063\u305d\u3046\n    ja_excite: 4\u7fbd\u306e\u91ce\u751f\u306e\u4e03\u9762\u9ce5\u304a\u3088\u3073\u3044\u304f\u3064\u304b\u306e\u4f4e\u6728\u6728\u3068\u96d1\u8349\n\nA group of turkeys with bushes in the background.\n    ja_google: \u80cc\u666f\u306b\u8302\u307f\u3092\u6301\u3064\u4e03\u9762\u9ce5\u306e\u7fa4\n        ja_google_tokens: \u80cc\u666f \u306b \u8302\u307f \u3092 \u6301 \u3064 \u4e03 \u9762 \u9ce5 \u306e \u7fa4\n        ja_google_pos: \u80cc\u666f/\u540d\u8a5e/\u306f\u3044\u3051\u3044 \u306b/\u52a9\u8a5e/\u306b \u8302\u307f/\u540d\u8a5e/\u3057\u3052\u307f \u3092/\u52a9\u8a5e/\u3092 \u6301/\u52d5\u8a5e/\u3082 \u3064/\u8a9e\u5c3e/\u3064 \u4e03/\u540d\u8a5e/\u306a\u306a \u9762/\u540d\u8a5e/\u3081\u3093 \u9ce5/\u540d\u8a5e/\u3061\u3087\u3046 \u306e/\u52a9\u8a5e/\u306e \u7fa4/\u540d\u8a5e/\u3080\u308c\n    ja_excite: \u80cc\u666f\u306e\u4f4e\u6728\u3092\u6301\u3064\u4e03\u9762\u9ce5\u306e\u30b0\u30eb\u30fc\u30d7\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e&quot;, &quot;languages&quot;: [{&quot;id&quot;: &quot;eng&quot;, &quot;title&quot;: {&quot;en&quot;: &quot;English&quot;}}], &quot;publication_date&quot;: &quot;2017-06-01&quot;, &quot;publisher&quot;: &quot;Zenodo&quot;, &quot;references&quot;: [{&quot;reference&quot;: &quot;SPEECH-COCO: 600k Visually Grounded Spoken Captions Aligned to MSCOCO Data Set&quot;}], &quot;related_identifiers&quot;: [{&quot;identifier&quot;: &quot;10.18709/perscido.2017.06.ds80&quot;, &quot;relation_type&quot;: {&quot;id&quot;: &quot;isnewversionof&quot;, &quot;title&quot;: {&quot;de&quot;: &quot;Ist eine neue Version von&quot;, &quot;en&quot;: &quot;Is new version of&quot;}}, &quot;resource_type&quot;: {&quot;id&quot;: &quot;dataset&quot;, &quot;title&quot;: {&quot;de&quot;: &quot;Datensatz&quot;, &quot;en&quot;: &quot;Dataset&quot;}}, &quot;scheme&quot;: &quot;doi&quot;}, {&quot;identifier&quot;: &quot;10.21437/GLU.2017-9&quot;, &quot;relation_type&quot;: {&quot;id&quot;: &quot;isdocumentedby&quot;, &quot;title&quot;: {&quot;de&quot;: &quot;Wird dokumentiert von&quot;, &quot;en&quot;: &quot;Is documented by&quot;}}, &quot;resource_type&quot;: {&quot;id&quot;: &quot;publication-conferencepaper&quot;, &quot;title&quot;: {&quot;de&quot;: &quot;Konferenzbeitrag&quot;, &quot;en&quot;: &quot;Conference paper&quot;}}, &quot;scheme&quot;: &quot;doi&quot;}], &quot;resource_type&quot;: {&quot;id&quot;: &quot;dataset&quot;, &quot;title&quot;: {&quot;de&quot;: &quot;Datensatz&quot;, &quot;en&quot;: &quot;Dataset&quot;}}, &quot;rights&quot;: [{&quot;description&quot;: {&quot;en&quot;: &quot;The Creative Commons Attribution license allows re-distribution and re-use of a licensed work on the condition that the creator is appropriately credited.&quot;}, &quot;icon&quot;: &quot;cc-by-icon&quot;, &quot;id&quot;: &quot;cc-by-4.0&quot;, &quot;props&quot;: {&quot;scheme&quot;: &quot;spdx&quot;, &quot;url&quot;: &quot;https://creativecommons.org/licenses/by/4.0/legalcode&quot;}, &quot;title&quot;: {&quot;en&quot;: &quot;Creative Commons Attribution 4.0 International&quot;}}], &quot;subjects&quot;: [{&quot;subject&quot;: &quot;MSCOCO&quot;}, {&quot;subject&quot;: &quot;VGS&quot;}, {&quot;subject&quot;: &quot;Speech&quot;}, {&quot;subject&quot;: &quot;Visually Grounded Speech&quot;}, {&quot;subject&quot;: &quot;audio&quot;}, {&quot;subject&quot;: &quot;captions&quot;}], &quot;title&quot;: &quot;SPEECH-COCO&quot;}, &quot;parent&quot;: {&quot;access&quot;: {&quot;owned_by&quot;: {&quot;user&quot;: &quot;55623&quot;}, &quot;settings&quot;: {&quot;accept_conditions_text&quot;: null, &quot;allow_guest_requests&quot;: false, &quot;allow_user_requests&quot;: false, &quot;secret_link_expiration&quot;: 0}}, &quot;communities&quot;: {}, &quot;id&quot;: &quot;4282266&quot;, &quot;pids&quot;: {&quot;doi&quot;: {&quot;client&quot;: &quot;datacite&quot;, &quot;identifier&quot;: &quot;10.5281/zenodo.4282266&quot;, &quot;provider&quot;: &quot;datacite&quot;}}}, &quot;pids&quot;: {&quot;doi&quot;: {&quot;client&quot;: &quot;datacite&quot;, &quot;identifier&quot;: &quot;10.5281/zenodo.4282267&quot;, &quot;provider&quot;: &quot;datacite&quot;}, &quot;oai&quot;: {&quot;identifier&quot;: &quot;oai:zenodo.org:4282267&quot;, &quot;provider&quot;: &quot;oai&quot;}}, &quot;revision_id&quot;: 2, &quot;stats&quot;: {&quot;all_versions&quot;: {&quot;data_volume&quot;: 33294681758455.0, &quot;downloads&quot;: 1650, &quot;unique_downloads&quot;: 665, &quot;unique_views&quot;: 1849, &quot;views&quot;: 1988}, &quot;this_version&quot;: {&quot;data_volume&quot;: 32965522623500.0, &quot;downloads&quot;: 1636, &quot;unique_downloads&quot;: 653, &quot;unique_views&quot;: 1841, &quot;views&quot;: 1980}}, &quot;status&quot;: &quot;published&quot;, &quot;swh&quot;: {}, &quot;ui&quot;: {&quot;access_status&quot;: {&quot;description_l10n&quot;: &quot;The record and files are publicly accessible.&quot;, &quot;embargo_date_l10n&quot;: null, &quot;icon&quot;: &quot;unlock&quot;, &quot;id&quot;: &quot;open&quot;, &quot;message_class&quot;: &quot;&quot;, &quot;title_l10n&quot;: &quot;Open&quot;}, &quot;conference&quot;: {&quot;acronym&quot;: &quot;GLU2017&quot;, &quot;dates&quot;: &quot;2017&quot;, &quot;place&quot;: &quot;Stockholm, Sweden&quot;, &quot;title&quot;: &quot;International Workshop on Grounding Language Understanding&quot;}, &quot;created_date_l10n_long&quot;: &quot;November 23, 2020&quot;, &quot;creators&quot;: {&quot;affiliations&quot;: [[1, &quot;Universit\u00e9 Grenoble Alpes&quot;, null]], &quot;creators&quot;: [{&quot;affiliations&quot;: [[1, &quot;Universit\u00e9 Grenoble Alpes&quot;]], &quot;person_or_org&quot;: {&quot;family_name&quot;: &quot;William N. Havard&quot;, &quot;identifiers&quot;: [{&quot;identifier&quot;: &quot;0000-0002-1226-4156&quot;, &quot;scheme&quot;: &quot;orcid&quot;}], &quot;name&quot;: &quot;William N. Havard&quot;, &quot;type&quot;: &quot;personal&quot;}}, {&quot;affiliations&quot;: [[1, &quot;Universit\u00e9 Grenoble Alpes&quot;]], &quot;person_or_org&quot;: {&quot;family_name&quot;: &quot;Laurent Besacier&quot;, &quot;identifiers&quot;: [{&quot;identifier&quot;: &quot;0000-0001-7411-9125&quot;, &quot;scheme&quot;: &quot;orcid&quot;}], &quot;name&quot;: &quot;Laurent Besacier&quot;, &quot;type&quot;: &quot;personal&quot;}}]}, &quot;custom_fields&quot;: {&quot;meeting:meeting&quot;: {&quot;acronym&quot;: &quot;GLU2017&quot;, &quot;dates&quot;: &quot;2017&quot;, &quot;place&quot;: &quot;Stockholm, Sweden&quot;, &quot;title&quot;: &quot;International Workshop on Grounding Language Understanding&quot;}}, &quot;description_stripped&quot;: &quot;SpeechCoco\n\n\nIntroduction\n\n\nOur corpus is an extension of the MS COCO image recognition and captioning dataset. MS COCO comprises images paired with a set of five captions. Yet, it does not include any speech. Therefore, we used\u00a0Voxygen\u0027s text-to-speech system\u00a0to synthesise\u00a0the available captions.\n\n\nThe addition of speech as a new modality enables MSCOCO to be used for researches in the field of language acquisition, unsupervised term discovery, keyword spotting, or semantic embedding using speech and vision.\n\n\nOur corpus is licensed under a\u00a0Creative Commons Attribution 4.0 License.\n\n\nData Set\n\n\n\n\t\n\n\t\nThis corpus contains\u00a0616,767\u00a0spoken captions from MSCOCO\u0027s val2014 and train2014 subsets (respectively 414,113 for train2014 and 202,654 for val2014).\n\t\n\t\n\n\t\nWe used 8 different voices. 4 of them have a British accent (Paul, Bronwen, Judith, and Elizabeth) and the 4 others have an American accent (Phil, Bruce, Amanda, Jenny).\n\t\n\t\n\n\t\nIn order to make the captions sound more natural, we used SOX\u00a0tempo\u00a0command, enabling us to change the speed without changing the pitch. 1/3 of the captions are 10% slower than the original pace, 1/3 are 10% faster. The last third of the captions was kept untouched.\n\t\n\t\n\n\t\nWe also modified approximately 30% of the original captions and added\u00a0disfluencies\u00a0such as \&quot;um\&quot;, \&quot;uh\&quot;, \&quot;er\&quot; so that the captions would sound more natural.\n\t\n\t\n\n\t\nEach WAV file is paired with a JSON file containing various information: timecode of each word in the caption, name of the speaker, name of the WAV file, etc. The JSON files have the following data structure:\n\t\n\n\n\n{\n    \&quot;duration\&quot;: float,\n    \&quot;speaker\&quot;: string,\n    \&quot;synthesisedCaption\&quot;: string,\n    \&quot;timecode\&quot;: list,\n    \&quot;speed\&quot;: float,\n    \&quot;wavFilename\&quot;: string,\n    \&quot;captionID\&quot;: int,\n    \&quot;imgID\&quot;: int,\n    \&quot;disfluency\&quot;: list\n}\n\n\n\n\t\n\n\t\nOn average, each caption comprises 10.79 tokens, disfluencies included. The WAV files are on average 3.52 seconds long.\n\t\n\n\n\nRepository\n\n\nThe repository is organized as follows:\n\n\n\n\t\n\n\t\nCORPUS-MSCOCO (~75GB once decompressed)\n\n\t\n\n\t\n\n\t\t\n\n\t\t\ntrain2014/\u00a0:\u00a0folder contains 413,915 captions\n\n\t\t\n\n\t\t\t\n\n\t\t\t\njson/\n\t\t\t\n\t\t\t\n\n\t\t\t\nwav/\n\t\t\t\n\t\t\t\n\n\t\t\t\ntranslations/\n\n\t\t\t\n\n\t\t\t\t\n\n\t\t\t\t\ntrain_en_ja.txt\n\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\ntrain_translate.sqlite3\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\n\t\t\t\ntrain_2014.sqlite3\n\t\t\t\n\t\t\n\t\t\n\t\t\n\n\t\t\nval2014/\u00a0:\u00a0folder contains 202,520 captions\n\n\t\t\n\n\t\t\t\n\n\t\t\t\njson/\n\t\t\t\n\t\t\t\n\n\t\t\t\nwav/\n\t\t\t\n\t\t\t\n\n\t\t\t\ntranslations/\n\n\t\t\t\n\n\t\t\t\t\n\n\t\t\t\t\ntrain_en_ja.txt\n\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\ntrain_translate.sqlite3\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\n\t\t\t\nval_2014.sqlite3\n\t\t\t\n\t\t\n\t\t\n\t\t\n\n\t\t\nspeechcoco_API/\n\n\t\t\n\n\t\t\t\n\n\t\t\t\nspeechcoco/\n\n\t\t\t\n\n\t\t\t\t\n\n\t\t\t\t\n__init__.py\n\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\nspeechcoco.py\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\n\t\t\t\nsetup.py\n\t\t\t\n\t\t\n\t\t\n\t\n\t\n\t\n\n\n\nFilenames\n\n\n.wav\u00a0files contain the spoken version of a caption\n\n\n.json\u00a0files contain all the metadata of a given WAV file\n\n\n.sqlite3\u00a0files are SQLite databases containing all the information contained in the JSON files\n\n\nWe adopted the following naming convention for both the WAV and JSON files:\n\n\nimageID_captionID_Speaker_DisfluencyPosition_Speed[.wav/.json]\n\n\nScript\n\n\nWe created a script called\u00a0speechcoco.py\u00a0in order to handle the metadata and allow the user to easily find captions according to specific filters. The script uses the *.db files.\n\n\nFeatures:\n\n\n\n\t\n\n\t\nAggregate all the information in the JSON files into a single SQLite database\n\t\n\t\n\n\t\nFind captions according to specific filters (name, gender and nationality of the speaker, disfluency position, speed, duration, and words in the caption).\u00a0The script automatically builds the SQLite query. The user can also provide his own SQLite query.\n\t\n\n\n\nThe following Python code returns all the captions spoken by a male with an American accent for which the speed was slowed\u00a0down by 10% and that contain \&quot;keys\&quot; at any position\n\n\n# create SpeechCoco object\ndb = SpeechCoco(train_2014.sqlite3, train_translate.sqlite3, verbose=True)\n\n# filter captions (returns Caption Objects)\ncaptions = db.filterCaptions(gender=\&quot;Male\&quot;, nationality=\&quot;US\&quot;, speed=0.9, text=\u0027%keys%\u0027)\nfor caption in captions:\n    print(\u0027\\n{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t\\t{}\u0027.format(caption.imageID,\n                                                  caption.captionID,\n                                                  caption.speaker.name,\n                                                  caption.speaker.nationality,\n                                                  caption.speed,\n                                                  caption.filename,\n                                                  caption.text))\n\n\n...\n298817      26763   Phil    0.9     298817_26763_Phil_None_0-9.wav          A group of turkeys with bushes in the background.\n108505      147972  Phil    0.9     108505_147972_Phil_Middle_0-9.wav               Person using a, um, slider cell phone with blue backlit keys.\n258289      154380  Bruce   0.9     258289_154380_Bruce_None_0-9.wav                Some donkeys and sheep are in their green pens .\n545312      201303  Phil    0.9     545312_201303_Phil_None_0-9.wav         A man walking next to a couple of donkeys.\n...\n\n\n\n\t\n\n\t\nFind all the captions belonging to a specific image\n\t\n\n\n\ncaptions = db.getImgCaptions(298817)\nfor caption in captions:\n    print(\u0027\\n{}\u0027.format(caption.text))\n\n\nBirds wondering through grassy ground next to bushes.\nA flock of turkeys are making their way up a hill.\nUm, ah. Two wild turkeys in a field walking around.\nFour wild turkeys and some bushes trees and weeds.\nA group of turkeys with bushes in the background.\n\n\n\n\t\n\n\t\nParse the timecodes and have them structured\n\t\n\n\n\ninput:\n\n\n...\n[1926.3068, \&quot;SYL\&quot;, \&quot;\&quot;],\n[1926.3068, \&quot;SEPR\&quot;, \&quot; \&quot;],\n[1926.3068, \&quot;WORD\&quot;, \&quot;white\&quot;],\n[1926.3068, \&quot;PHO\&quot;, \&quot;w\&quot;],\n[2050.7955, \&quot;PHO\&quot;, \&quot;ai\&quot;],\n[2144.6591, \&quot;PHO\&quot;, \&quot;t\&quot;],\n[2179.3182, \&quot;SYL\&quot;, \&quot;\&quot;],\n[2179.3182, \&quot;SEPR\&quot;, \&quot; \&quot;]\n...\n\n\noutput:\n\n\nprint(caption.timecode.parse())\n\n\n...\n{\n\u0027begin\u0027: 1926.3068,\n\u0027end\u0027: 2179.3182,\n\u0027syllable\u0027: [{\u0027begin\u0027: 1926.3068,\n              \u0027end\u0027: 2179.3182,\n              \u0027phoneme\u0027: [{\u0027begin\u0027: 1926.3068,\n                           \u0027end\u0027: 2050.7955,\n                           \u0027value\u0027: \u0027w\u0027},\n                          {\u0027begin\u0027: 2050.7955,\n                           \u0027end\u0027: 2144.6591,\n                           \u0027value\u0027: \u0027ai\u0027},\n                          {\u0027begin\u0027: 2144.6591,\n                           \u0027end\u0027: 2179.3182,\n                           \u0027value\u0027: \u0027t\u0027}],\n              \u0027value\u0027: \u0027wait\u0027}],\n\u0027value\u0027: \u0027white\u0027\n},\n...\n\n\n\n\t\n\n\t\nConvert the timecodes to Praat TextGrid files\n\t\n\n\n\ncaption.timecode.toTextgrid(outputDir, level=3)\n\n\n\n\t\n\n\t\nGet the words, syllables and phonemes between\u00a0n\u00a0seconds/milliseconds\n\t\n\n\n\nThe following Python code returns all the words between 0.2 and 0.6 seconds for which at least 50% of the word\u0027s total length is within the specified interval\n\n\npprint(caption.getWords(0.20, 0.60, seconds=True, level=1, olapthr=50))\n\n\n...\n404537      827239  Bruce   US      0.9     404537_827239_Bruce_None_0-9.wav                Eyeglasses, a cellphone, some keys and other pocket items are all laid out on the cloth. .\n[\n    {\n        \u0027begin\u0027: 0.0,\n        \u0027end\u0027: 0.7202778,\n        \u0027overlapPercentage\u0027: 55.53412863758955,\n        \u0027word\u0027: \u0027eyeglasses\u0027\n    }\n]\n ...\n\n\n\n\t\n\n\t\nGet the translations of the selected captions\n\t\n\n\n\nAs for now, only japanese translations are available. We also used\u00a0Kytea\u00a0to tokenize and tag the captions translated with Google Translate\n\n\ncaptions = db.getImgCaptions(298817)\nfor caption in captions:\n    print(\u0027\\n{}\u0027.format(caption.text))\n\n    # Get translations and POS\n    print(\u0027\\tja_google: {}\u0027.format(db.getTranslation(caption.captionID, \&quot;ja_google\&quot;)))\n    print(\u0027\\t\\tja_google_tokens: {}\u0027.format(db.getTokens(caption.captionID, \&quot;ja_google\&quot;)))\n    print(\u0027\\t\\tja_google_pos: {}\u0027.format(db.getPOS(caption.captionID, \&quot;ja_google\&quot;)))\n    print(\u0027\\tja_excite: {}\u0027.format(db.getTranslation(caption.captionID, \&quot;ja_excite\&quot;)))\n\n\n   Birds wondering through grassy ground next to bushes.\n    ja_google: \u9ce5\u306f\u8302\u307f\u306e\u4e0b\u306b\u8302\u3063\u305f\u5730\u9762\u3092\u62b1\u3048\u3066\u3044\u307e\u3059\u3002\n        ja_google_tokens: \u9ce5 \u306f \u8302\u307f \u306e \u4e0b \u306b \u8302 \u3063 \u305f \u5730\u9762 \u3092 \u62b1\u3048 \u3066 \u3044 \u307e \u3059 \u3002\n        ja_google_pos: \u9ce5/\u540d\u8a5e/\u3068\u308a \u306f/\u52a9\u8a5e/\u306f \u8302\u307f/\u540d\u8a5e/\u3057\u3052\u307f \u306e/\u52a9\u8a5e/\u306e \u4e0b/\u540d\u8a5e/\u3057\u305f \u306b/\u52a9\u8a5e/\u306b \u8302/\u52d5\u8a5e/\u3057\u3052 \u3063/\u8a9e\u5c3e/\u3063 \u305f/\u52a9\u52d5\u8a5e/\u305f \u5730\u9762/\u540d\u8a5e/\u3058\u3081\u3093 \u3092/\u52a9\u8a5e/\u3092 \u62b1\u3048/\u52d5\u8a5e/\u304b\u304b\u3048 \u3066/\u52a9\u8a5e/\u3066 \u3044/\u52d5\u8a5e/\u3044 \u307e/\u52a9\u52d5\u8a5e/\u307e \u3059/\u8a9e\u5c3e/\u3059 \u3002/\u88dc\u52a9\u8a18\u53f7/\u3002\n    ja_excite: \u4f4e\u6728\u3068\u96a3\u63a5\u3057\u305f\u8349\u6df1\u3044\u30b0\u30e9\u30a6\u30f3\u30c9\u3092\u901a\u3063\u3066\u7591\u3046\u9ce5\u3002\n\nA flock of turkeys are making their way up a hill.\n    ja_google: \u4e03\u9762\u9ce5\u306e\u7fa4\u308c\u304c\u4e18\u3092\u4e0a\u3063\u3066\u3044\u307e\u3059\u3002\n        ja_google_tokens: \u4e03 \u9762 \u9ce5 \u306e \u7fa4\u308c \u304c \u4e18 \u3092 \u4e0a \u3063 \u3066 \u3044 \u307e \u3059 \u3002\n        ja_google_pos: \u4e03/\u540d\u8a5e/\u306a\u306a \u9762/\u540d\u8a5e/\u3081\u3093 \u9ce5/\u540d\u8a5e/\u3068\u308a \u306e/\u52a9\u8a5e/\u306e \u7fa4\u308c/\u540d\u8a5e/\u3080\u308c \u304c/\u52a9\u8a5e/\u304c \u4e18/\u540d\u8a5e/\u304a\u304b \u3092/\u52a9\u8a5e/\u3092 \u4e0a/\u52d5\u8a5e/\u306e\u307c \u3063/\u8a9e\u5c3e/\u3063 \u3066/\u52a9\u8a5e/\u3066 \u3044/\u52d5\u8a5e/\u3044 \u307e/\u52a9\u52d5\u8a5e/\u307e \u3059/\u8a9e\u5c3e/\u3059 \u3002/\u88dc\u52a9\u8a18\u53f7/\u3002\n    ja_excite: \u4e03\u9762\u9ce5\u306e\u7fa4\u308c\u306f\u4e18\u306e\u4e0a\u3067\u9032\u3093\u3067\u3044\u308b\u3002\n\nUm, ah. Two wild turkeys in a field walking around.\n    ja_google: \u91ce\u751f\u306e\u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6\u3001\u91ce\u751f\u306e\u4e03\u9762\u9ce5\n        ja_google_tokens: \u91ce\u751f \u306e \u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6 \u3001 \u91ce\u751f \u306e \u4e03 \u9762 \u9ce5\n        ja_google_pos: \u91ce\u751f/\u540d\u8a5e/\u3084\u305b\u3044 \u306e/\u52a9\u8a5e/\u306e \u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6/\u540d\u8a5e/\u3057\u3061\u3081\u3093\u3061\u3087\u3046 \u3001/\u88dc\u52a9\u8a18\u53f7/\u3001 \u91ce\u751f/\u540d\u8a5e/\u3084\u305b\u3044 \u306e/\u52a9\u8a5e/\u306e \u4e03/\u540d\u8a5e/\u306a\u306a \u9762/\u540d\u8a5e/\u3081\u3093 \u9ce5/\u540d\u8a5e/\u3061\u3087\u3046\n    ja_excite: \u307e\u308f\u308a\u3067\u79fb\u52d5\u3057\u3066\u3044\u308b\u30d5\u30a3\u30fc\u30eb\u30c9\u306e2\u7fbd\u306e\u91ce\u751f\u306e\u4e03\u9762\u9ce5\n\nFour wild turkeys and some bushes trees and weeds.\n    ja_google: 4\u672c\u306e\u91ce\u751f\u306e\u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6\u3068\u3044\u304f\u3064\u304b\u306e\u8302\u307f\u306e\u6728\u3068\u96d1\u8349\n        ja_google_tokens: 4 \u672c \u306e \u91ce\u751f \u306e \u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6 \u3068 \u3044\u304f \u3064 \u304b \u306e \u8302\u307f \u306e \u6728 \u3068 \u96d1\u8349\n        ja_google_pos: 4/\u540d\u8a5e/4 \u672c/\u63a5\u5c3e\u8f9e/\u307b\u3093 \u306e/\u52a9\u8a5e/\u306e \u91ce\u751f/\u540d\u8a5e/\u3084\u305b\u3044 \u306e/\u52a9\u8a5e/\u306e \u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6/\u540d\u8a5e/\u3057\u3061\u3081\u3093\u3061\u3087\u3046 \u3068/\u52a9\u8a5e/\u3068 \u3044\u304f/\u540d\u8a5e/\u3044\u304f \u3064/\u63a5\u5c3e\u8f9e/\u3064 \u304b/\u52a9\u8a5e/\u304b \u306e/\u52a9\u8a5e/\u306e \u8302\u307f/\u540d\u8a5e/\u3057\u3052\u307f \u306e/\u52a9\u8a5e/\u306e \u6728/\u540d\u8a5e/\u304d \u3068/\u52a9\u8a5e/\u3068 \u96d1\u8349/\u540d\u8a5e/\u3056\u3063\u305d\u3046\n    ja_excite: 4\u7fbd\u306e\u91ce\u751f\u306e\u4e03\u9762\u9ce5\u304a\u3088\u3073\u3044\u304f\u3064\u304b\u306e\u4f4e\u6728\u6728\u3068\u96d1\u8349\n\nA group of turkeys with bushes in the background.\n    ja_google: \u80cc\u666f\u306b\u8302\u307f\u3092\u6301\u3064\u4e03\u9762\u9ce5\u306e\u7fa4\n        ja_google_tokens: \u80cc\u666f \u306b \u8302\u307f \u3092 \u6301 \u3064 \u4e03 \u9762 \u9ce5 \u306e \u7fa4\n        ja_google_pos: \u80cc\u666f/\u540d\u8a5e/\u306f\u3044\u3051\u3044 \u306b/\u52a9\u8a5e/\u306b \u8302\u307f/\u540d\u8a5e/\u3057\u3052\u307f \u3092/\u52a9\u8a5e/\u3092 \u6301/\u52d5\u8a5e/\u3082 \u3064/\u8a9e\u5c3e/\u3064 \u4e03/\u540d\u8a5e/\u306a\u306a \u9762/\u540d\u8a5e/\u3081\u3093 \u9ce5/\u540d\u8a5e/\u3061\u3087\u3046 \u306e/\u52a9\u8a5e/\u306e \u7fa4/\u540d\u8a5e/\u3080\u308c\n    ja_excite: \u80cc\u666f\u306e\u4f4e\u6728\u3092\u6301\u3064\u4e03\u9762\u9ce5\u306e\u30b0\u30eb\u30fc\u30d7\n\n\n\u00a0&quot;, &quot;is_draft&quot;: false, &quot;languages&quot;: [{&quot;id&quot;: &quot;eng&quot;, &quot;title_l10n&quot;: &quot;English&quot;}], &quot;publication_date_l10n_long&quot;: &quot;June 1, 2017&quot;, &quot;publication_date_l10n_medium&quot;: &quot;Jun 1, 2017&quot;, &quot;related_identifiers&quot;: [{&quot;identifier&quot;: &quot;10.18709/perscido.2017.06.ds80&quot;, &quot;relation_type&quot;: {&quot;id&quot;: &quot;isnewversionof&quot;, &quot;title_l10n&quot;: &quot;Is new version of&quot;}, &quot;resource_type&quot;: {&quot;id&quot;: &quot;dataset&quot;, &quot;title_l10n&quot;: &quot;Dataset&quot;}, &quot;scheme&quot;: &quot;doi&quot;}, {&quot;identifier&quot;: &quot;10.21437/GLU.2017-9&quot;, &quot;relation_type&quot;: {&quot;id&quot;: &quot;isdocumentedby&quot;, &quot;title_l10n&quot;: &quot;Is documented by&quot;}, &quot;resource_type&quot;: {&quot;id&quot;: &quot;publication-conferencepaper&quot;, &quot;title_l10n&quot;: &quot;Conference paper&quot;}, &quot;scheme&quot;: &quot;doi&quot;}], &quot;resource_type&quot;: {&quot;id&quot;: &quot;dataset&quot;, &quot;title_l10n&quot;: &quot;Dataset&quot;}, &quot;rights&quot;: [{&quot;description_l10n&quot;: &quot;The Creative Commons Attribution license allows re-distribution and re-use of a licensed work on the condition that the creator is appropriately credited.&quot;, &quot;icon&quot;: &quot;cc-by-icon&quot;, &quot;id&quot;: &quot;cc-by-4.0&quot;, &quot;props&quot;: {&quot;scheme&quot;: &quot;spdx&quot;, &quot;url&quot;: &quot;https://creativecommons.org/licenses/by/4.0/legalcode&quot;}, &quot;title_l10n&quot;: &quot;Creative Commons Attribution 4.0 International&quot;}], &quot;updated_date_l10n_long&quot;: &quot;November 24, 2020&quot;, &quot;version&quot;: &quot;v1&quot;}, &quot;updated&quot;: &quot;2020-11-24T12:27:21.323278+00:00&quot;, &quot;versions&quot;: {&quot;index&quot;: 1, &quot;is_latest&quot;: true}}" data-preview="false"><div role="list" class="ui divided list"><div role="listitem" class="item version active"><div class="left floated content"><span class="text-break">Version v1</span><a href="https://doi.org/10.5281/zenodo.4282267" class="doi text-muted-darken">10.5281/zenodo.4282267</a></div><div class="right floated content"><small class="text-muted-darken">Jun 1, 2017</small></div></div><div role="listitem" class="item parent-doi pr-0"><div class="left floated content"><p class="text-muted"><strong>Cite all versions?</strong> You can cite all versions by using the DOI <a href="https://doi.org/10.5281/zenodo.4282266">10.5281/zenodo.4282266</a>. This DOI represents all versions, and will always resolve to the latest one. <a href="https://zenodo.org/help/versioning">Read more</a>.</p></div></div></div></div>
    </div>
  </div>
</div><div class="sidebar-container">
  <h2 class="ui small top attached header">External resources</h2>
  <div id="external-resource" aria-label="External resources" class="ui bottom attached segment rdm-sidebar external resource">

    <h3 class="ui small header">Indexed in</h3>
      <ul class="ui relaxed list no-bullet">
              <li class="item flex align-items-center">

                
                  <img class="ui image" src="./SPEECH-COCO_files/openaire.svg" alt="" width="32">
                

                <div class="content">
                  <a class="header" href="https://explore.openaire.eu/search/dataset?pid=10.5281/zenodo.4282267" target="_blank" rel="noreferrer">OpenAIRE
                  </a>

                  </div>
              </li></ul></div>
</div><div id="sidebar-communities-manage" data-user-communities-memberships="{}" data-record-community-endpoint="https://zenodo.org/api/records/4282267/communities" data-record-community-search-endpoint="https://zenodo.org/api/records/4282267/communities-suggestions" data-record-user-community-search-endpoint="" data-pending-communities-search-config="{&quot;aggs&quot;: [{&quot;aggName&quot;: &quot;type&quot;, &quot;field&quot;: &quot;type&quot;, &quot;title&quot;: &quot;Type&quot;}, {&quot;aggName&quot;: &quot;status&quot;, &quot;field&quot;: &quot;status&quot;, &quot;title&quot;: &quot;Status&quot;}], &quot;appId&quot;: &quot;InvenioAppRdm.RecordRequests&quot;, &quot;defaultSortingOnEmptyQueryString&quot;: [{&quot;sortBy&quot;: &quot;newest&quot;}], &quot;initialQueryState&quot;: {&quot;filters&quot;: [], &quot;hiddenParams&quot;: [[&quot;expand&quot;, &quot;1&quot;], [&quot;is_open&quot;, &quot;true&quot;], [&quot;type&quot;, &quot;community-inclusion&quot;], [&quot;type&quot;, &quot;community-submission&quot;]], &quot;layout&quot;: &quot;list&quot;, &quot;page&quot;: 1, &quot;size&quot;: 10, &quot;sortBy&quot;: &quot;bestmatch&quot;}, &quot;layoutOptions&quot;: {&quot;gridView&quot;: false, &quot;listView&quot;: true}, &quot;paginationOptions&quot;: {&quot;defaultValue&quot;: 10, &quot;maxTotalResults&quot;: 10000, &quot;resultsPerPage&quot;: [{&quot;text&quot;: &quot;10&quot;, &quot;value&quot;: 10}, {&quot;text&quot;: &quot;20&quot;, &quot;value&quot;: 20}, {&quot;text&quot;: &quot;50&quot;, &quot;value&quot;: 50}]}, &quot;searchApi&quot;: {&quot;axios&quot;: {&quot;headers&quot;: {&quot;Accept&quot;: &quot;application/json&quot;}, &quot;url&quot;: &quot;https://zenodo.org/api/records/4282267/requests&quot;, &quot;withCredentials&quot;: true}, &quot;invenio&quot;: {&quot;requestSerializer&quot;: &quot;InvenioRecordsResourcesRequestSerializer&quot;}}, &quot;sortOptions&quot;: [{&quot;sortBy&quot;: &quot;bestmatch&quot;, &quot;text&quot;: &quot;Best match&quot;}, {&quot;sortBy&quot;: &quot;newest&quot;, &quot;text&quot;: &quot;Newest&quot;}, {&quot;sortBy&quot;: &quot;oldest&quot;, &quot;text&quot;: &quot;Oldest&quot;}], &quot;sortOrderDisabled&quot;: true}" data-record-community-search-config="{&quot;aggs&quot;: [{&quot;aggName&quot;: &quot;type&quot;, &quot;field&quot;: &quot;type&quot;, &quot;title&quot;: &quot;Type&quot;}, {&quot;aggName&quot;: &quot;funder&quot;, &quot;field&quot;: &quot;metadata.funding.funder&quot;, &quot;title&quot;: &quot;Funders&quot;}, {&quot;aggName&quot;: &quot;organization&quot;, &quot;field&quot;: &quot;metadata.organizations&quot;, &quot;title&quot;: &quot;Organizations&quot;}], &quot;appId&quot;: &quot;InvenioAppRdm.RecordCommunitiesSuggestions&quot;, &quot;defaultSortingOnEmptyQueryString&quot;: [{&quot;sortBy&quot;: &quot;newest&quot;}], &quot;initialQueryState&quot;: {&quot;filters&quot;: [], &quot;hiddenParams&quot;: null, &quot;layout&quot;: &quot;list&quot;, &quot;page&quot;: 1, &quot;size&quot;: 10, &quot;sortBy&quot;: &quot;bestmatch&quot;}, &quot;layoutOptions&quot;: {&quot;gridView&quot;: false, &quot;listView&quot;: true}, &quot;paginationOptions&quot;: {&quot;defaultValue&quot;: 10, &quot;maxTotalResults&quot;: 10000, &quot;resultsPerPage&quot;: [{&quot;text&quot;: &quot;10&quot;, &quot;value&quot;: 10}, {&quot;text&quot;: &quot;20&quot;, &quot;value&quot;: 20}]}, &quot;searchApi&quot;: {&quot;axios&quot;: {&quot;headers&quot;: {&quot;Accept&quot;: &quot;application/vnd.inveniordm.v1+json&quot;}, &quot;url&quot;: &quot;https://zenodo.org/api/records/4282267/communities-suggestions&quot;, &quot;withCredentials&quot;: true}, &quot;invenio&quot;: {&quot;requestSerializer&quot;: &quot;InvenioRecordsResourcesRequestSerializer&quot;}}, &quot;sortOptions&quot;: [{&quot;sortBy&quot;: &quot;bestmatch&quot;, &quot;text&quot;: &quot;Best match&quot;}, {&quot;sortBy&quot;: &quot;newest&quot;, &quot;text&quot;: &quot;Newest&quot;}, {&quot;sortBy&quot;: &quot;oldest&quot;, &quot;text&quot;: &quot;Oldest&quot;}], &quot;sortOrderDisabled&quot;: true}" data-record-user-community-search-config="{&quot;aggs&quot;: [{&quot;aggName&quot;: &quot;type&quot;, &quot;field&quot;: &quot;type&quot;, &quot;title&quot;: &quot;Type&quot;}, {&quot;aggName&quot;: &quot;funder&quot;, &quot;field&quot;: &quot;metadata.funding.funder&quot;, &quot;title&quot;: &quot;Funders&quot;}, {&quot;aggName&quot;: &quot;organization&quot;, &quot;field&quot;: &quot;metadata.organizations&quot;, &quot;title&quot;: &quot;Organizations&quot;}], &quot;appId&quot;: &quot;InvenioAppRdm.RecordUserCommunitiesSuggestions&quot;, &quot;defaultSortingOnEmptyQueryString&quot;: [{&quot;sortBy&quot;: &quot;newest&quot;}], &quot;initialQueryState&quot;: {&quot;filters&quot;: [], &quot;hiddenParams&quot;: [[&quot;membership&quot;, &quot;true&quot;]], &quot;layout&quot;: &quot;list&quot;, &quot;page&quot;: 1, &quot;size&quot;: 10, &quot;sortBy&quot;: &quot;bestmatch&quot;}, &quot;layoutOptions&quot;: {&quot;gridView&quot;: false, &quot;listView&quot;: true}, &quot;paginationOptions&quot;: {&quot;defaultValue&quot;: 10, &quot;maxTotalResults&quot;: 10000, &quot;resultsPerPage&quot;: [{&quot;text&quot;: &quot;10&quot;, &quot;value&quot;: 10}, {&quot;text&quot;: &quot;20&quot;, &quot;value&quot;: 20}]}, &quot;searchApi&quot;: {&quot;axios&quot;: {&quot;headers&quot;: {&quot;Accept&quot;: &quot;application/vnd.inveniordm.v1+json&quot;}, &quot;url&quot;: &quot;https://zenodo.org/api/records/4282267/communities-suggestions&quot;, &quot;withCredentials&quot;: true}, &quot;invenio&quot;: {&quot;requestSerializer&quot;: &quot;InvenioRecordsResourcesRequestSerializer&quot;}}, &quot;sortOptions&quot;: [{&quot;sortBy&quot;: &quot;bestmatch&quot;, &quot;text&quot;: &quot;Best match&quot;}, {&quot;sortBy&quot;: &quot;newest&quot;, &quot;text&quot;: &quot;Newest&quot;}, {&quot;sortBy&quot;: &quot;oldest&quot;, &quot;text&quot;: &quot;Oldest&quot;}], &quot;sortOrderDisabled&quot;: true}" data-permissions="{&quot;can_edit&quot;: false, &quot;can_manage&quot;: false, &quot;can_media_read_files&quot;: true, &quot;can_moderate&quot;: false, &quot;can_new_version&quot;: false, &quot;can_read_files&quot;: true, &quot;can_review&quot;: false, &quot;can_update_draft&quot;: false, &quot;can_view&quot;: false}" class="sidebar-container"></div>


  <div class="sidebar-container">
    <h2 class="ui medium top attached header mt-0">Keywords and subjects</h2>
    <div id="keywords-and-subjects" aria-label="Keywords and subjects" class="ui segment bottom attached rdm-sidebar">
  
  
  <h3 class="hidden">Keywords</h3>
  <ul class="ui horizontal list no-bullets subjects">
    <li class="item">
      <a href="https://zenodo.org/search?q=metadata.subjects.subject:%22MSCOCO%22" class="subject" title="Search results for MSCOCO">
        MSCOCO
      </a>
    </li>
    <li class="item">
      <a href="https://zenodo.org/search?q=metadata.subjects.subject:%22VGS%22" class="subject" title="Search results for VGS">
        VGS
      </a>
    </li>
    <li class="item">
      <a href="https://zenodo.org/search?q=metadata.subjects.subject:%22Speech%22" class="subject" title="Search results for Speech">
        Speech
      </a>
    </li>
    <li class="item">
      <a href="https://zenodo.org/search?q=metadata.subjects.subject:%22Visually+Grounded+Speech%22" class="subject" title="Search results for Visually Grounded Speech">
        Visually Grounded Speech
      </a>
    </li>
    <li class="item">
      <a href="https://zenodo.org/search?q=metadata.subjects.subject:%22audio%22" class="subject" title="Search results for audio">
        audio
      </a>
    </li>
    <li class="item">
      <a href="https://zenodo.org/search?q=metadata.subjects.subject:%22captions%22" class="subject" title="Search results for captions">
        captions
      </a>
    </li>
  </ul>
  

    </div>
  </div>


<div class="sidebar-container">
  <h2 class="ui medium top attached header mt-0">Details</h2>
  <div id="record-details" class="ui segment bottom attached rdm-sidebar">

    <dl class="details-list">
      

 
  <dt class="ui tiny header">DOI
    
  </dt><dd>
    <span class="get-badge" data-toggle="tooltip" data-placement="bottom" style="cursor: pointer;" title="Get the DOI badge!">
      <img id="record-doi-badge" data-target="[data-modal=&#39;10.5281/zenodo.4282267&#39;]" src="./SPEECH-COCO_files/zenodo.4282267.svg" alt="10.5281/zenodo.4282267">
    </span>

    <div id="doi-modal" class="ui modal fade badge-modal" data-modal="10.5281/zenodo.4282267">
      <div class="header">DOI Badge</div>
      <div class="content">
        <h4>
          <small>DOI</small>
        </h4>
        <h4>
          <pre>10.5281/zenodo.4282267</pre>
        </h4>
        
        

    <h3 class="ui small header">
      Markdown
    </h3>
    <div class="ui message code">
      <pre>[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4282267.svg)](https://doi.org/10.5281/zenodo.4282267)</pre>
    </div>

    <h3 class="ui small header">
      reStructuredText
    </h3>
      <div class="ui message code">
        <pre>.. image:: https://zenodo.org/badge/DOI/10.5281/zenodo.4282267.svg
  :target: https://doi.org/10.5281/zenodo.4282267</pre>
      </div>

    <h3 class="ui small header">
      HTML
    </h3>
    <div class="ui message code">
      <pre>&lt;a href="https://doi.org/10.5281/zenodo.4282267"&gt;&lt;img src="https://zenodo.org/badge/DOI/10.5281/zenodo.4282267.svg" alt="DOI"&gt;&lt;/a&gt;</pre>
    </div>

    <h3 class="ui small header">
      Image URL
    </h3>
    <div class="ui message code">
      <pre>https://zenodo.org/badge/DOI/10.5281/zenodo.4282267.svg</pre>
    </div>

    <h3 class="ui small header">
    Target URL
    </h3>
    <div class="ui message code">
      <pre>https://doi.org/10.5281/zenodo.4282267</pre>
    </div>
      </div>
    </div>
  </dd>

      
      
  <dt class="ui tiny header">Resource type</dt>
  <dd>Dataset</dd>
      
      
  <dt class="ui tiny header">Publisher</dt>
  <dd>Zenodo</dd>
      
      
      
      
  <dt class="ui tiny header">Conference</dt>
  <dd>
  </dd><dd>
      International Workshop on Grounding Language Understanding
      (GLU2017), Stockholm, Sweden, 2017
    
  </dd>

      
  <dt class="ui tiny header">Languages</dt>
  <dd>
  
    English
  
</dd>
      
      
    </dl>
  </div>
</div>








  <div class="sidebar-container">
    <h2 class="ui medium top attached header mt-0">Rights</h2>
    <div id="licenses" class="ui segment bottom attached rdm-sidebar">
      
        <h4>License</h4>
        <ul class="details-list m-0 p-0"><li id="license-cc-by-4.0-1" class="has-popup">
              <div id="title-cc-by-4.0-1" class="license clickable" tabindex="0" aria-haspopup="dialog" aria-expanded="false" role="button" aria-label="Creative Commons Attribution 4.0 International">
                
                  
                  <span class="icon-wrap">
                  <img class="icon" src="./SPEECH-COCO_files/cc-by-icon.svg" alt="cc-by-4.0 icon">
                </span>
                

                <span class="title-text">
                Creative Commons Attribution 4.0 International
              </span>
              </div>
              <div id="description-cc-by-4.0-1" class="licenses-description ui flowing popup transition hidden" role="dialog" aria-labelledby="title-cc-by-4.0-1">
                <i role="button" tabindex="0" class="close icon text-muted" aria-label="Close"></i>

                <div id="license-description-1" class="description">
                <span class="text-muted">
                  The Creative Commons Attribution license allows re-distribution and re-use of a licensed work on the condition that the creator is appropriately credited.
                </span>
                  
  
    <a class="license-link" href="https://creativecommons.org/licenses/by/4.0/legalcode" target="_blank" title="Opens in new tab">Read more</a>
  

                </div>
              </div>
            </li>
          

        </ul>
      
      
    </div>

  </div>
<div class="sidebar-container">
  <h2 class="ui medium top attached header mt-0">Citation</h2>
  <div id="citation" class="ui segment bottom attached rdm-sidebar">

<div id="recordCitation" data-record="{&quot;access&quot;: {&quot;embargo&quot;: {&quot;active&quot;: false, &quot;reason&quot;: null}, &quot;files&quot;: &quot;public&quot;, &quot;record&quot;: &quot;public&quot;, &quot;status&quot;: &quot;open&quot;}, &quot;created&quot;: &quot;2020-11-23T13:46:31.442227+00:00&quot;, &quot;custom_fields&quot;: {&quot;meeting:meeting&quot;: {&quot;acronym&quot;: &quot;GLU2017&quot;, &quot;dates&quot;: &quot;2017&quot;, &quot;place&quot;: &quot;Stockholm, Sweden&quot;, &quot;title&quot;: &quot;International Workshop on Grounding Language Understanding&quot;}}, &quot;deletion_status&quot;: {&quot;is_deleted&quot;: false, &quot;status&quot;: &quot;P&quot;}, &quot;expanded&quot;: {&quot;parent&quot;: {&quot;access&quot;: {&quot;owned_by&quot;: {&quot;active&quot;: null, &quot;blocked_at&quot;: null, &quot;confirmed_at&quot;: null, &quot;email&quot;: &quot;&quot;, &quot;id&quot;: &quot;55623&quot;, &quot;is_current_user&quot;: false, &quot;links&quot;: {&quot;avatar&quot;: &quot;https://zenodo.org/api/users/55623/avatar.svg&quot;, &quot;records_html&quot;: &quot;https://zenodo.org/search/records?q=parent.access.owned_by.user:55623&quot;, &quot;self&quot;: &quot;https://zenodo.org/api/users/55623&quot;}, &quot;profile&quot;: {&quot;affiliations&quot;: &quot;&quot;, &quot;full_name&quot;: &quot;&quot;}, &quot;username&quot;: &quot;WilliamNH&quot;, &quot;verified_at&quot;: null}}}}, &quot;files&quot;: {&quot;count&quot;: 3, &quot;enabled&quot;: true, &quot;entries&quot;: {&quot;speechcoco_API.zip&quot;: {&quot;access&quot;: {&quot;hidden&quot;: false}, &quot;checksum&quot;: &quot;md5:8cb717d90ee91183bd737b87690600b1&quot;, &quot;ext&quot;: &quot;zip&quot;, &quot;id&quot;: &quot;c85b78fe-1790-4a61-b03c-1bfd19a9c9ca&quot;, &quot;key&quot;: &quot;speechcoco_API.zip&quot;, &quot;links&quot;: {&quot;content&quot;: &quot;https://zenodo.org/api/records/4282267/files/speechcoco_API.zip/content&quot;, &quot;self&quot;: &quot;https://zenodo.org/api/records/4282267/files/speechcoco_API.zip&quot;}, &quot;metadata&quot;: null, &quot;mimetype&quot;: &quot;application/zip&quot;, &quot;size&quot;: 7171, &quot;storage_class&quot;: &quot;L&quot;}, &quot;train2014.tar.xz&quot;: {&quot;access&quot;: {&quot;hidden&quot;: false}, &quot;checksum&quot;: &quot;md5:418c096d84169d562ea32594c00f218b&quot;, &quot;ext&quot;: &quot;xz&quot;, &quot;id&quot;: &quot;9105f916-ede9-4938-b76a-ecbb4e1b9eea&quot;, &quot;key&quot;: &quot;train2014.tar.xz&quot;, &quot;links&quot;: {&quot;content&quot;: &quot;https://zenodo.org/api/records/4282267/files/train2014.tar.xz/content&quot;, &quot;self&quot;: &quot;https://zenodo.org/api/records/4282267/files/train2014.tar.xz&quot;}, &quot;metadata&quot;: null, &quot;mimetype&quot;: &quot;application/x-xz&quot;, &quot;size&quot;: 31525496168, &quot;storage_class&quot;: &quot;L&quot;}, &quot;val2014.tar.xz&quot;: {&quot;access&quot;: {&quot;hidden&quot;: false}, &quot;checksum&quot;: &quot;md5:16dc7bdbf375e400c47918f0fc25db49&quot;, &quot;ext&quot;: &quot;xz&quot;, &quot;id&quot;: &quot;d5fe8b3f-ed94-4308-9b14-70158f631985&quot;, &quot;key&quot;: &quot;val2014.tar.xz&quot;, &quot;links&quot;: {&quot;content&quot;: &quot;https://zenodo.org/api/records/4282267/files/val2014.tar.xz/content&quot;, &quot;self&quot;: &quot;https://zenodo.org/api/records/4282267/files/val2014.tar.xz&quot;}, &quot;metadata&quot;: null, &quot;mimetype&quot;: &quot;application/x-xz&quot;, &quot;size&quot;: 15391031688, &quot;storage_class&quot;: &quot;L&quot;}}, &quot;order&quot;: [], &quot;total_bytes&quot;: 46916535027}, &quot;id&quot;: &quot;4282267&quot;, &quot;is_draft&quot;: false, &quot;is_published&quot;: true, &quot;links&quot;: {&quot;access&quot;: &quot;https://zenodo.org/api/records/4282267/access&quot;, &quot;access_grants&quot;: &quot;https://zenodo.org/api/records/4282267/access/grants&quot;, &quot;access_links&quot;: &quot;https://zenodo.org/api/records/4282267/access/links&quot;, &quot;access_request&quot;: &quot;https://zenodo.org/api/records/4282267/access/request&quot;, &quot;access_users&quot;: &quot;https://zenodo.org/api/records/4282267/access/users&quot;, &quot;archive&quot;: &quot;https://zenodo.org/api/records/4282267/files-archive&quot;, &quot;archive_media&quot;: &quot;https://zenodo.org/api/records/4282267/media-files-archive&quot;, &quot;communities&quot;: &quot;https://zenodo.org/api/records/4282267/communities&quot;, &quot;communities-suggestions&quot;: &quot;https://zenodo.org/api/records/4282267/communities-suggestions&quot;, &quot;doi&quot;: &quot;https://doi.org/10.5281/zenodo.4282267&quot;, &quot;draft&quot;: &quot;https://zenodo.org/api/records/4282267/draft&quot;, &quot;files&quot;: &quot;https://zenodo.org/api/records/4282267/files&quot;, &quot;latest&quot;: &quot;https://zenodo.org/api/records/4282267/versions/latest&quot;, &quot;latest_html&quot;: &quot;https://zenodo.org/records/4282267/latest&quot;, &quot;media_files&quot;: &quot;https://zenodo.org/api/records/4282267/media-files&quot;, &quot;parent&quot;: &quot;https://zenodo.org/api/records/4282266&quot;, &quot;parent_doi&quot;: &quot;https://doi.org/10.5281/zenodo.4282266&quot;, &quot;parent_doi_html&quot;: &quot;https://zenodo.org/doi/10.5281/zenodo.4282266&quot;, &quot;parent_html&quot;: &quot;https://zenodo.org/records/4282266&quot;, &quot;preview_html&quot;: &quot;https://zenodo.org/records/4282267?preview=1&quot;, &quot;requests&quot;: &quot;https://zenodo.org/api/records/4282267/requests&quot;, &quot;reserve_doi&quot;: &quot;https://zenodo.org/api/records/4282267/draft/pids/doi&quot;, &quot;self&quot;: &quot;https://zenodo.org/api/records/4282267&quot;, &quot;self_doi&quot;: &quot;https://doi.org/10.5281/zenodo.4282267&quot;, &quot;self_doi_html&quot;: &quot;https://zenodo.org/doi/10.5281/zenodo.4282267&quot;, &quot;self_html&quot;: &quot;https://zenodo.org/records/4282267&quot;, &quot;self_iiif_manifest&quot;: &quot;https://zenodo.org/api/iiif/record:4282267/manifest&quot;, &quot;self_iiif_sequence&quot;: &quot;https://zenodo.org/api/iiif/record:4282267/sequence/default&quot;, &quot;versions&quot;: &quot;https://zenodo.org/api/records/4282267/versions&quot;}, &quot;media_files&quot;: {&quot;count&quot;: 0, &quot;enabled&quot;: false, &quot;entries&quot;: {}, &quot;order&quot;: [], &quot;total_bytes&quot;: 0}, &quot;metadata&quot;: {&quot;creators&quot;: [{&quot;affiliations&quot;: [{&quot;name&quot;: &quot;Universit\u00e9 Grenoble Alpes&quot;}], &quot;person_or_org&quot;: {&quot;family_name&quot;: &quot;William N. Havard&quot;, &quot;identifiers&quot;: [{&quot;identifier&quot;: &quot;0000-0002-1226-4156&quot;, &quot;scheme&quot;: &quot;orcid&quot;}], &quot;name&quot;: &quot;William N. Havard&quot;, &quot;type&quot;: &quot;personal&quot;}}, {&quot;affiliations&quot;: [{&quot;name&quot;: &quot;Universit\u00e9 Grenoble Alpes&quot;}], &quot;person_or_org&quot;: {&quot;family_name&quot;: &quot;Laurent Besacier&quot;, &quot;identifiers&quot;: [{&quot;identifier&quot;: &quot;0000-0001-7411-9125&quot;, &quot;scheme&quot;: &quot;orcid&quot;}], &quot;name&quot;: &quot;Laurent Besacier&quot;, &quot;type&quot;: &quot;personal&quot;}}], &quot;description&quot;: &quot;\u003cp\u003e\u003cstrong\u003eSpeechCoco\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eIntroduction\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eOur corpus is an extension of the MS COCO image recognition and captioning dataset. MS COCO comprises images paired with a set of five captions. Yet, it does not include any speech. Therefore, we used\u0026nbsp;\u003ca href=\&quot;https://www.voxygen.fr/\&quot;\u003eVoxygen\u0026#39;s text-to-speech system\u003c/a\u003e\u0026nbsp;to synthesise\u0026nbsp;the available captions.\u003c/p\u003e\n\n\u003cp\u003eThe addition of speech as a new modality enables MSCOCO to be used for researches in the field of language acquisition, unsupervised term discovery, keyword spotting, or semantic embedding using speech and vision.\u003c/p\u003e\n\n\u003cp\u003eOur corpus is licensed under a\u0026nbsp;\u003ca href=\&quot;https://creativecommons.org/licenses/by/4.0/legalcode\&quot;\u003eCreative Commons Attribution 4.0 License\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eData Set\u003c/em\u003e\u003c/p\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003eThis corpus contains\u0026nbsp;\u003cstrong\u003e616,767\u003c/strong\u003e\u0026nbsp;spoken captions from MSCOCO\u0026#39;s val2014 and train2014 subsets (respectively 414,113 for train2014 and 202,654 for val2014).\u003c/p\u003e\n\t\u003c/li\u003e\n\t\u003cli\u003e\n\t\u003cp\u003eWe used 8 different voices. 4 of them have a British accent (Paul, Bronwen, Judith, and Elizabeth) and the 4 others have an American accent (Phil, Bruce, Amanda, Jenny).\u003c/p\u003e\n\t\u003c/li\u003e\n\t\u003cli\u003e\n\t\u003cp\u003eIn order to make the captions sound more natural, we used SOX\u0026nbsp;\u003cem\u003etempo\u003c/em\u003e\u0026nbsp;command, enabling us to change the speed without changing the pitch. 1/3 of the captions are 10% slower than the original pace, 1/3 are 10% faster. The last third of the captions was kept untouched.\u003c/p\u003e\n\t\u003c/li\u003e\n\t\u003cli\u003e\n\t\u003cp\u003eWe also modified approximately 30% of the original captions and added\u0026nbsp;\u003cstrong\u003edisfluencies\u003c/strong\u003e\u0026nbsp;such as \u0026quot;um\u0026quot;, \u0026quot;uh\u0026quot;, \u0026quot;er\u0026quot; so that the captions would sound more natural.\u003c/p\u003e\n\t\u003c/li\u003e\n\t\u003cli\u003e\n\t\u003cp\u003eEach WAV file is paired with a JSON file containing various information: timecode of each word in the caption, name of the speaker, name of the WAV file, etc. The JSON files have the following data structure:\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cpre\u003e\u003ccode class=\&quot;language-json\&quot;\u003e{\n    \&quot;duration\&quot;: float,\n    \&quot;speaker\&quot;: string,\n    \&quot;synthesisedCaption\&quot;: string,\n    \&quot;timecode\&quot;: list,\n    \&quot;speed\&quot;: float,\n    \&quot;wavFilename\&quot;: string,\n    \&quot;captionID\&quot;: int,\n    \&quot;imgID\&quot;: int,\n    \&quot;disfluency\&quot;: list\n}\u003c/code\u003e\u003c/pre\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003eOn average, each caption comprises 10.79 tokens, disfluencies included. The WAV files are on average 3.52 seconds long.\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cem\u003eRepository\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eThe repository is organized as follows:\u003c/p\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003eCORPUS-MSCOCO (~75GB once decompressed)\u003c/p\u003e\n\n\t\u003cblockquote\u003e\n\t\u003cul\u003e\n\t\t\u003cli\u003e\n\t\t\u003cp\u003e\u003cstrong\u003etrain2014/\u003c/strong\u003e\u0026nbsp;:\u0026nbsp;folder contains 413,915 captions\u003c/p\u003e\n\n\t\t\u003cul\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003ejson/\u003c/p\u003e\n\t\t\t\u003c/li\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003ewav/\u003c/p\u003e\n\t\t\t\u003c/li\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003etranslations/\u003c/p\u003e\n\n\t\t\t\u003cul\u003e\n\t\t\t\t\u003cli\u003e\n\t\t\t\t\u003cp\u003etrain_en_ja.txt\u003c/p\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\n\t\t\t\t\u003cp\u003etrain_translate.sqlite3\u003c/p\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\u003c/ul\u003e\n\t\t\t\u003c/li\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003etrain_2014.sqlite3\u003c/p\u003e\n\t\t\t\u003c/li\u003e\n\t\t\u003c/ul\u003e\n\t\t\u003c/li\u003e\n\t\t\u003cli\u003e\n\t\t\u003cp\u003e\u003cstrong\u003eval2014/\u003c/strong\u003e\u0026nbsp;:\u0026nbsp;folder contains 202,520 captions\u003c/p\u003e\n\n\t\t\u003cul\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003ejson/\u003c/p\u003e\n\t\t\t\u003c/li\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003ewav/\u003c/p\u003e\n\t\t\t\u003c/li\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003etranslations/\u003c/p\u003e\n\n\t\t\t\u003cul\u003e\n\t\t\t\t\u003cli\u003e\n\t\t\t\t\u003cp\u003etrain_en_ja.txt\u003c/p\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\n\t\t\t\t\u003cp\u003etrain_translate.sqlite3\u003c/p\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\u003c/ul\u003e\n\t\t\t\u003c/li\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003eval_2014.sqlite3\u003c/p\u003e\n\t\t\t\u003c/li\u003e\n\t\t\u003c/ul\u003e\n\t\t\u003c/li\u003e\n\t\t\u003cli\u003e\n\t\t\u003cp\u003e\u003cstrong\u003espeechcoco_API/\u003c/strong\u003e\u003c/p\u003e\n\n\t\t\u003cul\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003espeechcoco/\u003c/p\u003e\n\n\t\t\t\u003cul\u003e\n\t\t\t\t\u003cli\u003e\n\t\t\t\t\u003cp\u003e__init__.py\u003c/p\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\n\t\t\t\t\u003cp\u003espeechcoco.py\u003c/p\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\u003c/ul\u003e\n\t\t\t\u003c/li\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003esetup.py\u003c/p\u003e\n\t\t\t\u003c/li\u003e\n\t\t\u003c/ul\u003e\n\t\t\u003c/li\u003e\n\t\u003c/ul\u003e\n\t\u003c/blockquote\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cem\u003eFilenames\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e.wav\u003c/strong\u003e\u0026nbsp;files contain the spoken version of a caption\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e.json\u003c/strong\u003e\u0026nbsp;files contain all the metadata of a given WAV file\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e.sqlite3\u003c/strong\u003e\u0026nbsp;files are SQLite databases containing all the information contained in the JSON files\u003c/p\u003e\n\n\u003cp\u003eWe adopted the following naming convention for both the WAV and JSON files:\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eimageID_captionID_Speaker_DisfluencyPosition_Speed[.wav/.json]\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eScript\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eWe created a script called\u0026nbsp;\u003cstrong\u003espeechcoco.py\u003c/strong\u003e\u0026nbsp;in order to handle the metadata and allow the user to easily find captions according to specific filters. The script uses the *.db files.\u003c/p\u003e\n\n\u003cp\u003eFeatures:\u003c/p\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003e\u003cstrong\u003eAggregate all the information in the JSON files into a single SQLite database\u003c/strong\u003e\u003c/p\u003e\n\t\u003c/li\u003e\n\t\u003cli\u003e\n\t\u003cp\u003e\u003cstrong\u003eFind captions according to specific filters (name, gender and nationality of the speaker, disfluency position, speed, duration, and words in the caption).\u003c/strong\u003e\u0026nbsp;\u003cem\u003eThe script automatically builds the SQLite query. The user can also provide his own SQLite query.\u003c/em\u003e\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cem\u003eThe following Python code returns all the captions spoken by a male with an American accent for which the speed was slowed\u0026nbsp;down by 10% and that contain \u0026quot;keys\u0026quot; at any position\u003c/em\u003e\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\&quot;language-python\&quot;\u003e# create SpeechCoco object\ndb = SpeechCoco(train_2014.sqlite3, train_translate.sqlite3, verbose=True)\n\n# filter captions (returns Caption Objects)\ncaptions = db.filterCaptions(gender=\&quot;Male\&quot;, nationality=\&quot;US\&quot;, speed=0.9, text=\u0027%keys%\u0027)\nfor caption in captions:\n    print(\u0027\\n{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t\\t{}\u0027.format(caption.imageID,\n                                                  caption.captionID,\n                                                  caption.speaker.name,\n                                                  caption.speaker.nationality,\n                                                  caption.speed,\n                                                  caption.filename,\n                                                  caption.text))\u003c/code\u003e\u003c/pre\u003e\n\n\u003cpre\u003e\u003ccode\u003e...\n298817      26763   Phil    0.9     298817_26763_Phil_None_0-9.wav          A group of turkeys with bushes in the background.\n108505      147972  Phil    0.9     108505_147972_Phil_Middle_0-9.wav               Person using a, um, slider cell phone with blue backlit keys.\n258289      154380  Bruce   0.9     258289_154380_Bruce_None_0-9.wav                Some donkeys and sheep are in their green pens .\n545312      201303  Phil    0.9     545312_201303_Phil_None_0-9.wav         A man walking next to a couple of donkeys.\n...\u003c/code\u003e\u003c/pre\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003e\u003cstrong\u003eFind all the captions belonging to a specific image\u003c/strong\u003e\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cpre\u003e\u003ccode class=\&quot;language-python\&quot;\u003ecaptions = db.getImgCaptions(298817)\nfor caption in captions:\n    print(\u0027\\n{}\u0027.format(caption.text))\u003c/code\u003e\u003c/pre\u003e\n\n\u003cpre\u003e\u003ccode\u003eBirds wondering through grassy ground next to bushes.\nA flock of turkeys are making their way up a hill.\nUm, ah. Two wild turkeys in a field walking around.\nFour wild turkeys and some bushes trees and weeds.\nA group of turkeys with bushes in the background.\u003c/code\u003e\u003c/pre\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003e\u003cstrong\u003eParse the timecodes and have them structured\u003c/strong\u003e\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cstrong\u003einput\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003e...\n[1926.3068, \&quot;SYL\&quot;, \&quot;\&quot;],\n[1926.3068, \&quot;SEPR\&quot;, \&quot; \&quot;],\n[1926.3068, \&quot;WORD\&quot;, \&quot;white\&quot;],\n[1926.3068, \&quot;PHO\&quot;, \&quot;w\&quot;],\n[2050.7955, \&quot;PHO\&quot;, \&quot;ai\&quot;],\n[2144.6591, \&quot;PHO\&quot;, \&quot;t\&quot;],\n[2179.3182, \&quot;SYL\&quot;, \&quot;\&quot;],\n[2179.3182, \&quot;SEPR\&quot;, \&quot; \&quot;]\n...\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\u003cstrong\u003eoutput\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\&quot;language-python\&quot;\u003eprint(caption.timecode.parse())\u003c/code\u003e\u003c/pre\u003e\n\n\u003cpre\u003e\u003ccode\u003e...\n{\n\u0027begin\u0027: 1926.3068,\n\u0027end\u0027: 2179.3182,\n\u0027syllable\u0027: [{\u0027begin\u0027: 1926.3068,\n              \u0027end\u0027: 2179.3182,\n              \u0027phoneme\u0027: [{\u0027begin\u0027: 1926.3068,\n                           \u0027end\u0027: 2050.7955,\n                           \u0027value\u0027: \u0027w\u0027},\n                          {\u0027begin\u0027: 2050.7955,\n                           \u0027end\u0027: 2144.6591,\n                           \u0027value\u0027: \u0027ai\u0027},\n                          {\u0027begin\u0027: 2144.6591,\n                           \u0027end\u0027: 2179.3182,\n                           \u0027value\u0027: \u0027t\u0027}],\n              \u0027value\u0027: \u0027wait\u0027}],\n\u0027value\u0027: \u0027white\u0027\n},\n...\u003c/code\u003e\u003c/pre\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003e\u003cstrong\u003eConvert the timecodes to Praat TextGrid files\u003c/strong\u003e\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cpre\u003e\u003ccode class=\&quot;language-python\&quot;\u003ecaption.timecode.toTextgrid(outputDir, level=3)\u003c/code\u003e\u003c/pre\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003e\u003cstrong\u003eGet the words, syllables and phonemes between\u003c/strong\u003e\u0026nbsp;\u003cem\u003en\u003c/em\u003e\u0026nbsp;\u003cstrong\u003eseconds/milliseconds\u003c/strong\u003e\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cem\u003eThe following Python code returns all the words between 0.2 and 0.6 seconds for which at least 50% of the word\u0026#39;s total length is within the specified interval\u003c/em\u003e\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\&quot;language-python\&quot;\u003epprint(caption.getWords(0.20, 0.60, seconds=True, level=1, olapthr=50))\u003c/code\u003e\u003c/pre\u003e\n\n\u003cpre\u003e\u003ccode\u003e...\n404537      827239  Bruce   US      0.9     404537_827239_Bruce_None_0-9.wav                Eyeglasses, a cellphone, some keys and other pocket items are all laid out on the cloth. .\n[\n    {\n        \u0027begin\u0027: 0.0,\n        \u0027end\u0027: 0.7202778,\n        \u0027overlapPercentage\u0027: 55.53412863758955,\n        \u0027word\u0027: \u0027eyeglasses\u0027\n    }\n]\n ...\u003c/code\u003e\u003c/pre\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003e\u003cstrong\u003eGet the translations of the selected captions\u003c/strong\u003e\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cem\u003eAs for now, only japanese translations are available. We also used\u003c/em\u003e\u0026nbsp;\u003ca href=\&quot;http://www.phontron.com/kytea/\&quot;\u003eKytea\u003c/a\u003e\u0026nbsp;\u003cem\u003eto tokenize and tag the captions translated with Google Translate\u003c/em\u003e\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\&quot;language-python\&quot;\u003ecaptions = db.getImgCaptions(298817)\nfor caption in captions:\n    print(\u0027\\n{}\u0027.format(caption.text))\n\n    # Get translations and POS\n    print(\u0027\\tja_google: {}\u0027.format(db.getTranslation(caption.captionID, \&quot;ja_google\&quot;)))\n    print(\u0027\\t\\tja_google_tokens: {}\u0027.format(db.getTokens(caption.captionID, \&quot;ja_google\&quot;)))\n    print(\u0027\\t\\tja_google_pos: {}\u0027.format(db.getPOS(caption.captionID, \&quot;ja_google\&quot;)))\n    print(\u0027\\tja_excite: {}\u0027.format(db.getTranslation(caption.captionID, \&quot;ja_excite\&quot;)))\u003c/code\u003e\u003c/pre\u003e\n\n\u003cpre\u003e\u003ccode\u003e   Birds wondering through grassy ground next to bushes.\n    ja_google: \u9ce5\u306f\u8302\u307f\u306e\u4e0b\u306b\u8302\u3063\u305f\u5730\u9762\u3092\u62b1\u3048\u3066\u3044\u307e\u3059\u3002\n        ja_google_tokens: \u9ce5 \u306f \u8302\u307f \u306e \u4e0b \u306b \u8302 \u3063 \u305f \u5730\u9762 \u3092 \u62b1\u3048 \u3066 \u3044 \u307e \u3059 \u3002\n        ja_google_pos: \u9ce5/\u540d\u8a5e/\u3068\u308a \u306f/\u52a9\u8a5e/\u306f \u8302\u307f/\u540d\u8a5e/\u3057\u3052\u307f \u306e/\u52a9\u8a5e/\u306e \u4e0b/\u540d\u8a5e/\u3057\u305f \u306b/\u52a9\u8a5e/\u306b \u8302/\u52d5\u8a5e/\u3057\u3052 \u3063/\u8a9e\u5c3e/\u3063 \u305f/\u52a9\u52d5\u8a5e/\u305f \u5730\u9762/\u540d\u8a5e/\u3058\u3081\u3093 \u3092/\u52a9\u8a5e/\u3092 \u62b1\u3048/\u52d5\u8a5e/\u304b\u304b\u3048 \u3066/\u52a9\u8a5e/\u3066 \u3044/\u52d5\u8a5e/\u3044 \u307e/\u52a9\u52d5\u8a5e/\u307e \u3059/\u8a9e\u5c3e/\u3059 \u3002/\u88dc\u52a9\u8a18\u53f7/\u3002\n    ja_excite: \u4f4e\u6728\u3068\u96a3\u63a5\u3057\u305f\u8349\u6df1\u3044\u30b0\u30e9\u30a6\u30f3\u30c9\u3092\u901a\u3063\u3066\u7591\u3046\u9ce5\u3002\n\nA flock of turkeys are making their way up a hill.\n    ja_google: \u4e03\u9762\u9ce5\u306e\u7fa4\u308c\u304c\u4e18\u3092\u4e0a\u3063\u3066\u3044\u307e\u3059\u3002\n        ja_google_tokens: \u4e03 \u9762 \u9ce5 \u306e \u7fa4\u308c \u304c \u4e18 \u3092 \u4e0a \u3063 \u3066 \u3044 \u307e \u3059 \u3002\n        ja_google_pos: \u4e03/\u540d\u8a5e/\u306a\u306a \u9762/\u540d\u8a5e/\u3081\u3093 \u9ce5/\u540d\u8a5e/\u3068\u308a \u306e/\u52a9\u8a5e/\u306e \u7fa4\u308c/\u540d\u8a5e/\u3080\u308c \u304c/\u52a9\u8a5e/\u304c \u4e18/\u540d\u8a5e/\u304a\u304b \u3092/\u52a9\u8a5e/\u3092 \u4e0a/\u52d5\u8a5e/\u306e\u307c \u3063/\u8a9e\u5c3e/\u3063 \u3066/\u52a9\u8a5e/\u3066 \u3044/\u52d5\u8a5e/\u3044 \u307e/\u52a9\u52d5\u8a5e/\u307e \u3059/\u8a9e\u5c3e/\u3059 \u3002/\u88dc\u52a9\u8a18\u53f7/\u3002\n    ja_excite: \u4e03\u9762\u9ce5\u306e\u7fa4\u308c\u306f\u4e18\u306e\u4e0a\u3067\u9032\u3093\u3067\u3044\u308b\u3002\n\nUm, ah. Two wild turkeys in a field walking around.\n    ja_google: \u91ce\u751f\u306e\u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6\u3001\u91ce\u751f\u306e\u4e03\u9762\u9ce5\n        ja_google_tokens: \u91ce\u751f \u306e \u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6 \u3001 \u91ce\u751f \u306e \u4e03 \u9762 \u9ce5\n        ja_google_pos: \u91ce\u751f/\u540d\u8a5e/\u3084\u305b\u3044 \u306e/\u52a9\u8a5e/\u306e \u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6/\u540d\u8a5e/\u3057\u3061\u3081\u3093\u3061\u3087\u3046 \u3001/\u88dc\u52a9\u8a18\u53f7/\u3001 \u91ce\u751f/\u540d\u8a5e/\u3084\u305b\u3044 \u306e/\u52a9\u8a5e/\u306e \u4e03/\u540d\u8a5e/\u306a\u306a \u9762/\u540d\u8a5e/\u3081\u3093 \u9ce5/\u540d\u8a5e/\u3061\u3087\u3046\n    ja_excite: \u307e\u308f\u308a\u3067\u79fb\u52d5\u3057\u3066\u3044\u308b\u30d5\u30a3\u30fc\u30eb\u30c9\u306e2\u7fbd\u306e\u91ce\u751f\u306e\u4e03\u9762\u9ce5\n\nFour wild turkeys and some bushes trees and weeds.\n    ja_google: 4\u672c\u306e\u91ce\u751f\u306e\u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6\u3068\u3044\u304f\u3064\u304b\u306e\u8302\u307f\u306e\u6728\u3068\u96d1\u8349\n        ja_google_tokens: 4 \u672c \u306e \u91ce\u751f \u306e \u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6 \u3068 \u3044\u304f \u3064 \u304b \u306e \u8302\u307f \u306e \u6728 \u3068 \u96d1\u8349\n        ja_google_pos: 4/\u540d\u8a5e/4 \u672c/\u63a5\u5c3e\u8f9e/\u307b\u3093 \u306e/\u52a9\u8a5e/\u306e \u91ce\u751f/\u540d\u8a5e/\u3084\u305b\u3044 \u306e/\u52a9\u8a5e/\u306e \u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6/\u540d\u8a5e/\u3057\u3061\u3081\u3093\u3061\u3087\u3046 \u3068/\u52a9\u8a5e/\u3068 \u3044\u304f/\u540d\u8a5e/\u3044\u304f \u3064/\u63a5\u5c3e\u8f9e/\u3064 \u304b/\u52a9\u8a5e/\u304b \u306e/\u52a9\u8a5e/\u306e \u8302\u307f/\u540d\u8a5e/\u3057\u3052\u307f \u306e/\u52a9\u8a5e/\u306e \u6728/\u540d\u8a5e/\u304d \u3068/\u52a9\u8a5e/\u3068 \u96d1\u8349/\u540d\u8a5e/\u3056\u3063\u305d\u3046\n    ja_excite: 4\u7fbd\u306e\u91ce\u751f\u306e\u4e03\u9762\u9ce5\u304a\u3088\u3073\u3044\u304f\u3064\u304b\u306e\u4f4e\u6728\u6728\u3068\u96d1\u8349\n\nA group of turkeys with bushes in the background.\n    ja_google: \u80cc\u666f\u306b\u8302\u307f\u3092\u6301\u3064\u4e03\u9762\u9ce5\u306e\u7fa4\n        ja_google_tokens: \u80cc\u666f \u306b \u8302\u307f \u3092 \u6301 \u3064 \u4e03 \u9762 \u9ce5 \u306e \u7fa4\n        ja_google_pos: \u80cc\u666f/\u540d\u8a5e/\u306f\u3044\u3051\u3044 \u306b/\u52a9\u8a5e/\u306b \u8302\u307f/\u540d\u8a5e/\u3057\u3052\u307f \u3092/\u52a9\u8a5e/\u3092 \u6301/\u52d5\u8a5e/\u3082 \u3064/\u8a9e\u5c3e/\u3064 \u4e03/\u540d\u8a5e/\u306a\u306a \u9762/\u540d\u8a5e/\u3081\u3093 \u9ce5/\u540d\u8a5e/\u3061\u3087\u3046 \u306e/\u52a9\u8a5e/\u306e \u7fa4/\u540d\u8a5e/\u3080\u308c\n    ja_excite: \u80cc\u666f\u306e\u4f4e\u6728\u3092\u6301\u3064\u4e03\u9762\u9ce5\u306e\u30b0\u30eb\u30fc\u30d7\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e&quot;, &quot;languages&quot;: [{&quot;id&quot;: &quot;eng&quot;, &quot;title&quot;: {&quot;en&quot;: &quot;English&quot;}}], &quot;publication_date&quot;: &quot;2017-06-01&quot;, &quot;publisher&quot;: &quot;Zenodo&quot;, &quot;references&quot;: [{&quot;reference&quot;: &quot;SPEECH-COCO: 600k Visually Grounded Spoken Captions Aligned to MSCOCO Data Set&quot;}], &quot;related_identifiers&quot;: [{&quot;identifier&quot;: &quot;10.18709/perscido.2017.06.ds80&quot;, &quot;relation_type&quot;: {&quot;id&quot;: &quot;isnewversionof&quot;, &quot;title&quot;: {&quot;de&quot;: &quot;Ist eine neue Version von&quot;, &quot;en&quot;: &quot;Is new version of&quot;}}, &quot;resource_type&quot;: {&quot;id&quot;: &quot;dataset&quot;, &quot;title&quot;: {&quot;de&quot;: &quot;Datensatz&quot;, &quot;en&quot;: &quot;Dataset&quot;}}, &quot;scheme&quot;: &quot;doi&quot;}, {&quot;identifier&quot;: &quot;10.21437/GLU.2017-9&quot;, &quot;relation_type&quot;: {&quot;id&quot;: &quot;isdocumentedby&quot;, &quot;title&quot;: {&quot;de&quot;: &quot;Wird dokumentiert von&quot;, &quot;en&quot;: &quot;Is documented by&quot;}}, &quot;resource_type&quot;: {&quot;id&quot;: &quot;publication-conferencepaper&quot;, &quot;title&quot;: {&quot;de&quot;: &quot;Konferenzbeitrag&quot;, &quot;en&quot;: &quot;Conference paper&quot;}}, &quot;scheme&quot;: &quot;doi&quot;}], &quot;resource_type&quot;: {&quot;id&quot;: &quot;dataset&quot;, &quot;title&quot;: {&quot;de&quot;: &quot;Datensatz&quot;, &quot;en&quot;: &quot;Dataset&quot;}}, &quot;rights&quot;: [{&quot;description&quot;: {&quot;en&quot;: &quot;The Creative Commons Attribution license allows re-distribution and re-use of a licensed work on the condition that the creator is appropriately credited.&quot;}, &quot;icon&quot;: &quot;cc-by-icon&quot;, &quot;id&quot;: &quot;cc-by-4.0&quot;, &quot;props&quot;: {&quot;scheme&quot;: &quot;spdx&quot;, &quot;url&quot;: &quot;https://creativecommons.org/licenses/by/4.0/legalcode&quot;}, &quot;title&quot;: {&quot;en&quot;: &quot;Creative Commons Attribution 4.0 International&quot;}}], &quot;subjects&quot;: [{&quot;subject&quot;: &quot;MSCOCO&quot;}, {&quot;subject&quot;: &quot;VGS&quot;}, {&quot;subject&quot;: &quot;Speech&quot;}, {&quot;subject&quot;: &quot;Visually Grounded Speech&quot;}, {&quot;subject&quot;: &quot;audio&quot;}, {&quot;subject&quot;: &quot;captions&quot;}], &quot;title&quot;: &quot;SPEECH-COCO&quot;}, &quot;parent&quot;: {&quot;access&quot;: {&quot;owned_by&quot;: {&quot;user&quot;: &quot;55623&quot;}, &quot;settings&quot;: {&quot;accept_conditions_text&quot;: null, &quot;allow_guest_requests&quot;: false, &quot;allow_user_requests&quot;: false, &quot;secret_link_expiration&quot;: 0}}, &quot;communities&quot;: {}, &quot;id&quot;: &quot;4282266&quot;, &quot;pids&quot;: {&quot;doi&quot;: {&quot;client&quot;: &quot;datacite&quot;, &quot;identifier&quot;: &quot;10.5281/zenodo.4282266&quot;, &quot;provider&quot;: &quot;datacite&quot;}}}, &quot;pids&quot;: {&quot;doi&quot;: {&quot;client&quot;: &quot;datacite&quot;, &quot;identifier&quot;: &quot;10.5281/zenodo.4282267&quot;, &quot;provider&quot;: &quot;datacite&quot;}, &quot;oai&quot;: {&quot;identifier&quot;: &quot;oai:zenodo.org:4282267&quot;, &quot;provider&quot;: &quot;oai&quot;}}, &quot;revision_id&quot;: 2, &quot;stats&quot;: {&quot;all_versions&quot;: {&quot;data_volume&quot;: 33294681758455.0, &quot;downloads&quot;: 1650, &quot;unique_downloads&quot;: 665, &quot;unique_views&quot;: 1849, &quot;views&quot;: 1988}, &quot;this_version&quot;: {&quot;data_volume&quot;: 32965522623500.0, &quot;downloads&quot;: 1636, &quot;unique_downloads&quot;: 653, &quot;unique_views&quot;: 1841, &quot;views&quot;: 1980}}, &quot;status&quot;: &quot;published&quot;, &quot;swh&quot;: {}, &quot;ui&quot;: {&quot;access_status&quot;: {&quot;description_l10n&quot;: &quot;The record and files are publicly accessible.&quot;, &quot;embargo_date_l10n&quot;: null, &quot;icon&quot;: &quot;unlock&quot;, &quot;id&quot;: &quot;open&quot;, &quot;message_class&quot;: &quot;&quot;, &quot;title_l10n&quot;: &quot;Open&quot;}, &quot;conference&quot;: {&quot;acronym&quot;: &quot;GLU2017&quot;, &quot;dates&quot;: &quot;2017&quot;, &quot;place&quot;: &quot;Stockholm, Sweden&quot;, &quot;title&quot;: &quot;International Workshop on Grounding Language Understanding&quot;}, &quot;created_date_l10n_long&quot;: &quot;November 23, 2020&quot;, &quot;creators&quot;: {&quot;affiliations&quot;: [[1, &quot;Universit\u00e9 Grenoble Alpes&quot;, null]], &quot;creators&quot;: [{&quot;affiliations&quot;: [[1, &quot;Universit\u00e9 Grenoble Alpes&quot;]], &quot;person_or_org&quot;: {&quot;family_name&quot;: &quot;William N. Havard&quot;, &quot;identifiers&quot;: [{&quot;identifier&quot;: &quot;0000-0002-1226-4156&quot;, &quot;scheme&quot;: &quot;orcid&quot;}], &quot;name&quot;: &quot;William N. Havard&quot;, &quot;type&quot;: &quot;personal&quot;}}, {&quot;affiliations&quot;: [[1, &quot;Universit\u00e9 Grenoble Alpes&quot;]], &quot;person_or_org&quot;: {&quot;family_name&quot;: &quot;Laurent Besacier&quot;, &quot;identifiers&quot;: [{&quot;identifier&quot;: &quot;0000-0001-7411-9125&quot;, &quot;scheme&quot;: &quot;orcid&quot;}], &quot;name&quot;: &quot;Laurent Besacier&quot;, &quot;type&quot;: &quot;personal&quot;}}]}, &quot;custom_fields&quot;: {&quot;meeting:meeting&quot;: {&quot;acronym&quot;: &quot;GLU2017&quot;, &quot;dates&quot;: &quot;2017&quot;, &quot;place&quot;: &quot;Stockholm, Sweden&quot;, &quot;title&quot;: &quot;International Workshop on Grounding Language Understanding&quot;}}, &quot;description_stripped&quot;: &quot;SpeechCoco\n\n\nIntroduction\n\n\nOur corpus is an extension of the MS COCO image recognition and captioning dataset. MS COCO comprises images paired with a set of five captions. Yet, it does not include any speech. Therefore, we used\u00a0Voxygen\u0027s text-to-speech system\u00a0to synthesise\u00a0the available captions.\n\n\nThe addition of speech as a new modality enables MSCOCO to be used for researches in the field of language acquisition, unsupervised term discovery, keyword spotting, or semantic embedding using speech and vision.\n\n\nOur corpus is licensed under a\u00a0Creative Commons Attribution 4.0 License.\n\n\nData Set\n\n\n\n\t\n\n\t\nThis corpus contains\u00a0616,767\u00a0spoken captions from MSCOCO\u0027s val2014 and train2014 subsets (respectively 414,113 for train2014 and 202,654 for val2014).\n\t\n\t\n\n\t\nWe used 8 different voices. 4 of them have a British accent (Paul, Bronwen, Judith, and Elizabeth) and the 4 others have an American accent (Phil, Bruce, Amanda, Jenny).\n\t\n\t\n\n\t\nIn order to make the captions sound more natural, we used SOX\u00a0tempo\u00a0command, enabling us to change the speed without changing the pitch. 1/3 of the captions are 10% slower than the original pace, 1/3 are 10% faster. The last third of the captions was kept untouched.\n\t\n\t\n\n\t\nWe also modified approximately 30% of the original captions and added\u00a0disfluencies\u00a0such as \&quot;um\&quot;, \&quot;uh\&quot;, \&quot;er\&quot; so that the captions would sound more natural.\n\t\n\t\n\n\t\nEach WAV file is paired with a JSON file containing various information: timecode of each word in the caption, name of the speaker, name of the WAV file, etc. The JSON files have the following data structure:\n\t\n\n\n\n{\n    \&quot;duration\&quot;: float,\n    \&quot;speaker\&quot;: string,\n    \&quot;synthesisedCaption\&quot;: string,\n    \&quot;timecode\&quot;: list,\n    \&quot;speed\&quot;: float,\n    \&quot;wavFilename\&quot;: string,\n    \&quot;captionID\&quot;: int,\n    \&quot;imgID\&quot;: int,\n    \&quot;disfluency\&quot;: list\n}\n\n\n\n\t\n\n\t\nOn average, each caption comprises 10.79 tokens, disfluencies included. The WAV files are on average 3.52 seconds long.\n\t\n\n\n\nRepository\n\n\nThe repository is organized as follows:\n\n\n\n\t\n\n\t\nCORPUS-MSCOCO (~75GB once decompressed)\n\n\t\n\n\t\n\n\t\t\n\n\t\t\ntrain2014/\u00a0:\u00a0folder contains 413,915 captions\n\n\t\t\n\n\t\t\t\n\n\t\t\t\njson/\n\t\t\t\n\t\t\t\n\n\t\t\t\nwav/\n\t\t\t\n\t\t\t\n\n\t\t\t\ntranslations/\n\n\t\t\t\n\n\t\t\t\t\n\n\t\t\t\t\ntrain_en_ja.txt\n\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\ntrain_translate.sqlite3\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\n\t\t\t\ntrain_2014.sqlite3\n\t\t\t\n\t\t\n\t\t\n\t\t\n\n\t\t\nval2014/\u00a0:\u00a0folder contains 202,520 captions\n\n\t\t\n\n\t\t\t\n\n\t\t\t\njson/\n\t\t\t\n\t\t\t\n\n\t\t\t\nwav/\n\t\t\t\n\t\t\t\n\n\t\t\t\ntranslations/\n\n\t\t\t\n\n\t\t\t\t\n\n\t\t\t\t\ntrain_en_ja.txt\n\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\ntrain_translate.sqlite3\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\n\t\t\t\nval_2014.sqlite3\n\t\t\t\n\t\t\n\t\t\n\t\t\n\n\t\t\nspeechcoco_API/\n\n\t\t\n\n\t\t\t\n\n\t\t\t\nspeechcoco/\n\n\t\t\t\n\n\t\t\t\t\n\n\t\t\t\t\n__init__.py\n\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\nspeechcoco.py\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\n\t\t\t\nsetup.py\n\t\t\t\n\t\t\n\t\t\n\t\n\t\n\t\n\n\n\nFilenames\n\n\n.wav\u00a0files contain the spoken version of a caption\n\n\n.json\u00a0files contain all the metadata of a given WAV file\n\n\n.sqlite3\u00a0files are SQLite databases containing all the information contained in the JSON files\n\n\nWe adopted the following naming convention for both the WAV and JSON files:\n\n\nimageID_captionID_Speaker_DisfluencyPosition_Speed[.wav/.json]\n\n\nScript\n\n\nWe created a script called\u00a0speechcoco.py\u00a0in order to handle the metadata and allow the user to easily find captions according to specific filters. The script uses the *.db files.\n\n\nFeatures:\n\n\n\n\t\n\n\t\nAggregate all the information in the JSON files into a single SQLite database\n\t\n\t\n\n\t\nFind captions according to specific filters (name, gender and nationality of the speaker, disfluency position, speed, duration, and words in the caption).\u00a0The script automatically builds the SQLite query. The user can also provide his own SQLite query.\n\t\n\n\n\nThe following Python code returns all the captions spoken by a male with an American accent for which the speed was slowed\u00a0down by 10% and that contain \&quot;keys\&quot; at any position\n\n\n# create SpeechCoco object\ndb = SpeechCoco(train_2014.sqlite3, train_translate.sqlite3, verbose=True)\n\n# filter captions (returns Caption Objects)\ncaptions = db.filterCaptions(gender=\&quot;Male\&quot;, nationality=\&quot;US\&quot;, speed=0.9, text=\u0027%keys%\u0027)\nfor caption in captions:\n    print(\u0027\\n{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t\\t{}\u0027.format(caption.imageID,\n                                                  caption.captionID,\n                                                  caption.speaker.name,\n                                                  caption.speaker.nationality,\n                                                  caption.speed,\n                                                  caption.filename,\n                                                  caption.text))\n\n\n...\n298817      26763   Phil    0.9     298817_26763_Phil_None_0-9.wav          A group of turkeys with bushes in the background.\n108505      147972  Phil    0.9     108505_147972_Phil_Middle_0-9.wav               Person using a, um, slider cell phone with blue backlit keys.\n258289      154380  Bruce   0.9     258289_154380_Bruce_None_0-9.wav                Some donkeys and sheep are in their green pens .\n545312      201303  Phil    0.9     545312_201303_Phil_None_0-9.wav         A man walking next to a couple of donkeys.\n...\n\n\n\n\t\n\n\t\nFind all the captions belonging to a specific image\n\t\n\n\n\ncaptions = db.getImgCaptions(298817)\nfor caption in captions:\n    print(\u0027\\n{}\u0027.format(caption.text))\n\n\nBirds wondering through grassy ground next to bushes.\nA flock of turkeys are making their way up a hill.\nUm, ah. Two wild turkeys in a field walking around.\nFour wild turkeys and some bushes trees and weeds.\nA group of turkeys with bushes in the background.\n\n\n\n\t\n\n\t\nParse the timecodes and have them structured\n\t\n\n\n\ninput:\n\n\n...\n[1926.3068, \&quot;SYL\&quot;, \&quot;\&quot;],\n[1926.3068, \&quot;SEPR\&quot;, \&quot; \&quot;],\n[1926.3068, \&quot;WORD\&quot;, \&quot;white\&quot;],\n[1926.3068, \&quot;PHO\&quot;, \&quot;w\&quot;],\n[2050.7955, \&quot;PHO\&quot;, \&quot;ai\&quot;],\n[2144.6591, \&quot;PHO\&quot;, \&quot;t\&quot;],\n[2179.3182, \&quot;SYL\&quot;, \&quot;\&quot;],\n[2179.3182, \&quot;SEPR\&quot;, \&quot; \&quot;]\n...\n\n\noutput:\n\n\nprint(caption.timecode.parse())\n\n\n...\n{\n\u0027begin\u0027: 1926.3068,\n\u0027end\u0027: 2179.3182,\n\u0027syllable\u0027: [{\u0027begin\u0027: 1926.3068,\n              \u0027end\u0027: 2179.3182,\n              \u0027phoneme\u0027: [{\u0027begin\u0027: 1926.3068,\n                           \u0027end\u0027: 2050.7955,\n                           \u0027value\u0027: \u0027w\u0027},\n                          {\u0027begin\u0027: 2050.7955,\n                           \u0027end\u0027: 2144.6591,\n                           \u0027value\u0027: \u0027ai\u0027},\n                          {\u0027begin\u0027: 2144.6591,\n                           \u0027end\u0027: 2179.3182,\n                           \u0027value\u0027: \u0027t\u0027}],\n              \u0027value\u0027: \u0027wait\u0027}],\n\u0027value\u0027: \u0027white\u0027\n},\n...\n\n\n\n\t\n\n\t\nConvert the timecodes to Praat TextGrid files\n\t\n\n\n\ncaption.timecode.toTextgrid(outputDir, level=3)\n\n\n\n\t\n\n\t\nGet the words, syllables and phonemes between\u00a0n\u00a0seconds/milliseconds\n\t\n\n\n\nThe following Python code returns all the words between 0.2 and 0.6 seconds for which at least 50% of the word\u0027s total length is within the specified interval\n\n\npprint(caption.getWords(0.20, 0.60, seconds=True, level=1, olapthr=50))\n\n\n...\n404537      827239  Bruce   US      0.9     404537_827239_Bruce_None_0-9.wav                Eyeglasses, a cellphone, some keys and other pocket items are all laid out on the cloth. .\n[\n    {\n        \u0027begin\u0027: 0.0,\n        \u0027end\u0027: 0.7202778,\n        \u0027overlapPercentage\u0027: 55.53412863758955,\n        \u0027word\u0027: \u0027eyeglasses\u0027\n    }\n]\n ...\n\n\n\n\t\n\n\t\nGet the translations of the selected captions\n\t\n\n\n\nAs for now, only japanese translations are available. We also used\u00a0Kytea\u00a0to tokenize and tag the captions translated with Google Translate\n\n\ncaptions = db.getImgCaptions(298817)\nfor caption in captions:\n    print(\u0027\\n{}\u0027.format(caption.text))\n\n    # Get translations and POS\n    print(\u0027\\tja_google: {}\u0027.format(db.getTranslation(caption.captionID, \&quot;ja_google\&quot;)))\n    print(\u0027\\t\\tja_google_tokens: {}\u0027.format(db.getTokens(caption.captionID, \&quot;ja_google\&quot;)))\n    print(\u0027\\t\\tja_google_pos: {}\u0027.format(db.getPOS(caption.captionID, \&quot;ja_google\&quot;)))\n    print(\u0027\\tja_excite: {}\u0027.format(db.getTranslation(caption.captionID, \&quot;ja_excite\&quot;)))\n\n\n   Birds wondering through grassy ground next to bushes.\n    ja_google: \u9ce5\u306f\u8302\u307f\u306e\u4e0b\u306b\u8302\u3063\u305f\u5730\u9762\u3092\u62b1\u3048\u3066\u3044\u307e\u3059\u3002\n        ja_google_tokens: \u9ce5 \u306f \u8302\u307f \u306e \u4e0b \u306b \u8302 \u3063 \u305f \u5730\u9762 \u3092 \u62b1\u3048 \u3066 \u3044 \u307e \u3059 \u3002\n        ja_google_pos: \u9ce5/\u540d\u8a5e/\u3068\u308a \u306f/\u52a9\u8a5e/\u306f \u8302\u307f/\u540d\u8a5e/\u3057\u3052\u307f \u306e/\u52a9\u8a5e/\u306e \u4e0b/\u540d\u8a5e/\u3057\u305f \u306b/\u52a9\u8a5e/\u306b \u8302/\u52d5\u8a5e/\u3057\u3052 \u3063/\u8a9e\u5c3e/\u3063 \u305f/\u52a9\u52d5\u8a5e/\u305f \u5730\u9762/\u540d\u8a5e/\u3058\u3081\u3093 \u3092/\u52a9\u8a5e/\u3092 \u62b1\u3048/\u52d5\u8a5e/\u304b\u304b\u3048 \u3066/\u52a9\u8a5e/\u3066 \u3044/\u52d5\u8a5e/\u3044 \u307e/\u52a9\u52d5\u8a5e/\u307e \u3059/\u8a9e\u5c3e/\u3059 \u3002/\u88dc\u52a9\u8a18\u53f7/\u3002\n    ja_excite: \u4f4e\u6728\u3068\u96a3\u63a5\u3057\u305f\u8349\u6df1\u3044\u30b0\u30e9\u30a6\u30f3\u30c9\u3092\u901a\u3063\u3066\u7591\u3046\u9ce5\u3002\n\nA flock of turkeys are making their way up a hill.\n    ja_google: \u4e03\u9762\u9ce5\u306e\u7fa4\u308c\u304c\u4e18\u3092\u4e0a\u3063\u3066\u3044\u307e\u3059\u3002\n        ja_google_tokens: \u4e03 \u9762 \u9ce5 \u306e \u7fa4\u308c \u304c \u4e18 \u3092 \u4e0a \u3063 \u3066 \u3044 \u307e \u3059 \u3002\n        ja_google_pos: \u4e03/\u540d\u8a5e/\u306a\u306a \u9762/\u540d\u8a5e/\u3081\u3093 \u9ce5/\u540d\u8a5e/\u3068\u308a \u306e/\u52a9\u8a5e/\u306e \u7fa4\u308c/\u540d\u8a5e/\u3080\u308c \u304c/\u52a9\u8a5e/\u304c \u4e18/\u540d\u8a5e/\u304a\u304b \u3092/\u52a9\u8a5e/\u3092 \u4e0a/\u52d5\u8a5e/\u306e\u307c \u3063/\u8a9e\u5c3e/\u3063 \u3066/\u52a9\u8a5e/\u3066 \u3044/\u52d5\u8a5e/\u3044 \u307e/\u52a9\u52d5\u8a5e/\u307e \u3059/\u8a9e\u5c3e/\u3059 \u3002/\u88dc\u52a9\u8a18\u53f7/\u3002\n    ja_excite: \u4e03\u9762\u9ce5\u306e\u7fa4\u308c\u306f\u4e18\u306e\u4e0a\u3067\u9032\u3093\u3067\u3044\u308b\u3002\n\nUm, ah. Two wild turkeys in a field walking around.\n    ja_google: \u91ce\u751f\u306e\u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6\u3001\u91ce\u751f\u306e\u4e03\u9762\u9ce5\n        ja_google_tokens: \u91ce\u751f \u306e \u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6 \u3001 \u91ce\u751f \u306e \u4e03 \u9762 \u9ce5\n        ja_google_pos: \u91ce\u751f/\u540d\u8a5e/\u3084\u305b\u3044 \u306e/\u52a9\u8a5e/\u306e \u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6/\u540d\u8a5e/\u3057\u3061\u3081\u3093\u3061\u3087\u3046 \u3001/\u88dc\u52a9\u8a18\u53f7/\u3001 \u91ce\u751f/\u540d\u8a5e/\u3084\u305b\u3044 \u306e/\u52a9\u8a5e/\u306e \u4e03/\u540d\u8a5e/\u306a\u306a \u9762/\u540d\u8a5e/\u3081\u3093 \u9ce5/\u540d\u8a5e/\u3061\u3087\u3046\n    ja_excite: \u307e\u308f\u308a\u3067\u79fb\u52d5\u3057\u3066\u3044\u308b\u30d5\u30a3\u30fc\u30eb\u30c9\u306e2\u7fbd\u306e\u91ce\u751f\u306e\u4e03\u9762\u9ce5\n\nFour wild turkeys and some bushes trees and weeds.\n    ja_google: 4\u672c\u306e\u91ce\u751f\u306e\u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6\u3068\u3044\u304f\u3064\u304b\u306e\u8302\u307f\u306e\u6728\u3068\u96d1\u8349\n        ja_google_tokens: 4 \u672c \u306e \u91ce\u751f \u306e \u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6 \u3068 \u3044\u304f \u3064 \u304b \u306e \u8302\u307f \u306e \u6728 \u3068 \u96d1\u8349\n        ja_google_pos: 4/\u540d\u8a5e/4 \u672c/\u63a5\u5c3e\u8f9e/\u307b\u3093 \u306e/\u52a9\u8a5e/\u306e \u91ce\u751f/\u540d\u8a5e/\u3084\u305b\u3044 \u306e/\u52a9\u8a5e/\u306e \u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6/\u540d\u8a5e/\u3057\u3061\u3081\u3093\u3061\u3087\u3046 \u3068/\u52a9\u8a5e/\u3068 \u3044\u304f/\u540d\u8a5e/\u3044\u304f \u3064/\u63a5\u5c3e\u8f9e/\u3064 \u304b/\u52a9\u8a5e/\u304b \u306e/\u52a9\u8a5e/\u306e \u8302\u307f/\u540d\u8a5e/\u3057\u3052\u307f \u306e/\u52a9\u8a5e/\u306e \u6728/\u540d\u8a5e/\u304d \u3068/\u52a9\u8a5e/\u3068 \u96d1\u8349/\u540d\u8a5e/\u3056\u3063\u305d\u3046\n    ja_excite: 4\u7fbd\u306e\u91ce\u751f\u306e\u4e03\u9762\u9ce5\u304a\u3088\u3073\u3044\u304f\u3064\u304b\u306e\u4f4e\u6728\u6728\u3068\u96d1\u8349\n\nA group of turkeys with bushes in the background.\n    ja_google: \u80cc\u666f\u306b\u8302\u307f\u3092\u6301\u3064\u4e03\u9762\u9ce5\u306e\u7fa4\n        ja_google_tokens: \u80cc\u666f \u306b \u8302\u307f \u3092 \u6301 \u3064 \u4e03 \u9762 \u9ce5 \u306e \u7fa4\n        ja_google_pos: \u80cc\u666f/\u540d\u8a5e/\u306f\u3044\u3051\u3044 \u306b/\u52a9\u8a5e/\u306b \u8302\u307f/\u540d\u8a5e/\u3057\u3052\u307f \u3092/\u52a9\u8a5e/\u3092 \u6301/\u52d5\u8a5e/\u3082 \u3064/\u8a9e\u5c3e/\u3064 \u4e03/\u540d\u8a5e/\u306a\u306a \u9762/\u540d\u8a5e/\u3081\u3093 \u9ce5/\u540d\u8a5e/\u3061\u3087\u3046 \u306e/\u52a9\u8a5e/\u306e \u7fa4/\u540d\u8a5e/\u3080\u308c\n    ja_excite: \u80cc\u666f\u306e\u4f4e\u6728\u3092\u6301\u3064\u4e03\u9762\u9ce5\u306e\u30b0\u30eb\u30fc\u30d7\n\n\n\u00a0&quot;, &quot;is_draft&quot;: false, &quot;languages&quot;: [{&quot;id&quot;: &quot;eng&quot;, &quot;title_l10n&quot;: &quot;English&quot;}], &quot;publication_date_l10n_long&quot;: &quot;June 1, 2017&quot;, &quot;publication_date_l10n_medium&quot;: &quot;Jun 1, 2017&quot;, &quot;related_identifiers&quot;: [{&quot;identifier&quot;: &quot;10.18709/perscido.2017.06.ds80&quot;, &quot;relation_type&quot;: {&quot;id&quot;: &quot;isnewversionof&quot;, &quot;title_l10n&quot;: &quot;Is new version of&quot;}, &quot;resource_type&quot;: {&quot;id&quot;: &quot;dataset&quot;, &quot;title_l10n&quot;: &quot;Dataset&quot;}, &quot;scheme&quot;: &quot;doi&quot;}, {&quot;identifier&quot;: &quot;10.21437/GLU.2017-9&quot;, &quot;relation_type&quot;: {&quot;id&quot;: &quot;isdocumentedby&quot;, &quot;title_l10n&quot;: &quot;Is documented by&quot;}, &quot;resource_type&quot;: {&quot;id&quot;: &quot;publication-conferencepaper&quot;, &quot;title_l10n&quot;: &quot;Conference paper&quot;}, &quot;scheme&quot;: &quot;doi&quot;}], &quot;resource_type&quot;: {&quot;id&quot;: &quot;dataset&quot;, &quot;title_l10n&quot;: &quot;Dataset&quot;}, &quot;rights&quot;: [{&quot;description_l10n&quot;: &quot;The Creative Commons Attribution license allows re-distribution and re-use of a licensed work on the condition that the creator is appropriately credited.&quot;, &quot;icon&quot;: &quot;cc-by-icon&quot;, &quot;id&quot;: &quot;cc-by-4.0&quot;, &quot;props&quot;: {&quot;scheme&quot;: &quot;spdx&quot;, &quot;url&quot;: &quot;https://creativecommons.org/licenses/by/4.0/legalcode&quot;}, &quot;title_l10n&quot;: &quot;Creative Commons Attribution 4.0 International&quot;}], &quot;updated_date_l10n_long&quot;: &quot;November 24, 2020&quot;, &quot;version&quot;: &quot;v1&quot;}, &quot;updated&quot;: &quot;2020-11-24T12:27:21.323278+00:00&quot;, &quot;versions&quot;: {&quot;index&quot;: 1, &quot;is_latest&quot;: true}}" data-styles="[[&quot;apa&quot;, &quot;APA&quot;], [&quot;harvard-cite-them-right&quot;, &quot;Harvard&quot;], [&quot;modern-language-association&quot;, &quot;MLA&quot;], [&quot;vancouver&quot;, &quot;Vancouver&quot;], [&quot;chicago-fullnote-bibliography&quot;, &quot;Chicago&quot;], [&quot;ieee&quot;, &quot;IEEE&quot;]]" data-defaultstyle="&quot;apa&quot;" data-include-deleted="false"><div><div id="citation-text" class="wrap-overflowing-text rel-mb-2"><div>William N. Havard, &amp; Laurent Besacier. (2017). SPEECH-COCO [Data set]. International Workshop on Grounding Language Understanding (GLU2017), Stockholm, Sweden. Zenodo. <a href="https://doi.org/10.5281/zenodo.4282267" target="_blank">https://doi.org/10.5281/zenodo.4282267</a></div></div><div class="auto-column-grid no-wrap"><div class="flex align-items-center"><label id="citation-style-label" class="mr-10">Style</label><div aria-labelledby="citation-style-label" role="listbox" aria-expanded="false" class="ui selection dropdown citation-dropdown" tabindex="0"><div aria-atomic="true" aria-live="polite" role="alert" class="divider text">APA</div><i aria-hidden="true" class="dropdown icon"></i><div class="menu transition"><div role="option" aria-checked="true" aria-selected="true" class="active selected item" style="pointer-events: all;"><span class="text">APA</span></div><div role="option" aria-checked="false" aria-selected="false" class="item" style="pointer-events: all;"><span class="text">Harvard</span></div><div role="option" aria-checked="false" aria-selected="false" class="item" style="pointer-events: all;"><span class="text">MLA</span></div><div role="option" aria-checked="false" aria-selected="false" class="item" style="pointer-events: all;"><span class="text">Vancouver</span></div><div role="option" aria-checked="false" aria-selected="false" class="item" style="pointer-events: all;"><span class="text">Chicago</span></div><div role="option" aria-checked="false" aria-selected="false" class="item" style="pointer-events: all;"><span class="text">IEEE</span></div></div></div></div><button aria-label="Copy to clipboard" class="ui basic icon button copy"><i aria-hidden="true" class="copy icon"></i></button></div></div></div>
  </div>
</div>
  <div class="sidebar-container">
    <h2 class="ui medium top attached header mt-0">Export</h2>
    <div id="export-record" class="ui segment bottom attached exports rdm-sidebar">

      
      

        

        

        

        

        

        

        

        

        

        

        

        <div id="recordExportDownload" data-formats="[{&quot;export_url&quot;: &quot;/records/4282267/export/json&quot;, &quot;name&quot;: &quot;JSON&quot;}, {&quot;export_url&quot;: &quot;/records/4282267/export/json-ld&quot;, &quot;name&quot;: &quot;JSON-LD&quot;}, {&quot;export_url&quot;: &quot;/records/4282267/export/csl&quot;, &quot;name&quot;: &quot;CSL&quot;}, {&quot;export_url&quot;: &quot;/records/4282267/export/datacite-json&quot;, &quot;name&quot;: &quot;DataCite JSON&quot;}, {&quot;export_url&quot;: &quot;/records/4282267/export/datacite-xml&quot;, &quot;name&quot;: &quot;DataCite XML&quot;}, {&quot;export_url&quot;: &quot;/records/4282267/export/dublincore&quot;, &quot;name&quot;: &quot;Dublin Core XML&quot;}, {&quot;export_url&quot;: &quot;/records/4282267/export/marcxml&quot;, &quot;name&quot;: &quot;MARCXML&quot;}, {&quot;export_url&quot;: &quot;/records/4282267/export/bibtex&quot;, &quot;name&quot;: &quot;BibTeX&quot;}, {&quot;export_url&quot;: &quot;/records/4282267/export/geojson&quot;, &quot;name&quot;: &quot;GeoJSON&quot;}, {&quot;export_url&quot;: &quot;/records/4282267/export/dcat-ap&quot;, &quot;name&quot;: &quot;DCAT&quot;}, {&quot;export_url&quot;: &quot;/records/4282267/export/codemeta&quot;, &quot;name&quot;: &quot;Codemeta&quot;}, {&quot;export_url&quot;: &quot;/records/4282267/export/cff&quot;, &quot;name&quot;: &quot;Citation File Format&quot;}]"><div class="ui grid"><div class="eleven wide column"><div aria-label="Export selection" role="listbox" aria-expanded="false" class="ui fluid selection dropdown" tabindex="0"><div aria-atomic="true" aria-live="polite" role="alert" class="divider text">JSON</div><i aria-hidden="true" class="dropdown icon"></i><div class="menu transition"><div role="option" aria-checked="true" aria-selected="true" class="active selected item" style="pointer-events: all;"><span class="text">JSON</span></div><div role="option" aria-checked="false" aria-selected="false" class="item" style="pointer-events: all;"><span class="text">JSON-LD</span></div><div role="option" aria-checked="false" aria-selected="false" class="item" style="pointer-events: all;"><span class="text">CSL</span></div><div role="option" aria-checked="false" aria-selected="false" class="item" style="pointer-events: all;"><span class="text">DataCite JSON</span></div><div role="option" aria-checked="false" aria-selected="false" class="item" style="pointer-events: all;"><span class="text">DataCite XML</span></div><div role="option" aria-checked="false" aria-selected="false" class="item" style="pointer-events: all;"><span class="text">Dublin Core XML</span></div><div role="option" aria-checked="false" aria-selected="false" class="item" style="pointer-events: all;"><span class="text">MARCXML</span></div><div role="option" aria-checked="false" aria-selected="false" class="item" style="pointer-events: all;"><span class="text">BibTeX</span></div><div role="option" aria-checked="false" aria-selected="false" class="item" style="pointer-events: all;"><span class="text">GeoJSON</span></div><div role="option" aria-checked="false" aria-selected="false" class="item" style="pointer-events: all;"><span class="text">DCAT</span></div><div role="option" aria-checked="false" aria-selected="false" class="item" style="pointer-events: all;"><span class="text">Codemeta</span></div><div role="option" aria-checked="false" aria-selected="false" class="item" style="pointer-events: all;"><span class="text">Citation File Format</span></div></div></div></div><div class="five wide column pl-0"><a href="https://zenodo.org/records/4282267/export/json" title="Download file" class="ui fluid button" role="button">Export</a></div></div></div>
    </div>
  </div>
<section id="upload-info" role="note" aria-label="Upload information" class="sidebar-container ui segment rdm-sidebar text-muted">
  <h2 class="ui small header text-muted p-0 mb-5"><small>Technical metadata</small></h2>
  <dl class="m-0">
    <dt class="inline"><small>Created</small></dt>
    <dd class="inline">
      <small>November 23, 2020</small>
    </dd>
    <div>
      <dt class="rel-mt-1 inline"><small>Modified</small></dt>
      <dd class="inline">
        <small>November 24, 2020</small>
      </dd>
    </div>
  </dl>
</section>
          
        </aside>
      </div>
    </div>

    <div class="ui container">
      <div class="ui relaxed grid">
        <div class="two column row">
          <div class="sixteen wide tablet eleven wide computer column">
            <div class="ui grid">
                <div class="centered row rel-mt-1">
                  <button id="jump-btn" class="jump-to-top ui button labeled icon" aria-label="Jump to top of page">
                    <i class="arrow alternate circle up outline icon"></i>
                    Jump up
                  </button>
                </div>
              </div></div>
        </div>
      </div>
    </div>
  </div>
    </div>
  </main>
    <footer id="rdm-footer-element">
        

  <div class="footer-top">
    <div class="ui container app-rdm-footer">
<div class="ui equal width stackable grid zenodo-footer">
  <div class="column">
    <h2 class="ui inverted tiny header">About</h2>
    <ul class="ui inverted link list">
      <li class="item">
        <a href="https://about.zenodo.org/">About</a>
      </li>
      <li class="item">
        <a href="https://about.zenodo.org/policies">Policies</a>
      </li>
      <li class="item">
        <a href="https://about.zenodo.org/infrastructure">Infrastructure</a>
      </li>
      <li class="item">
        <a href="https://about.zenodo.org/principles">Principles</a>
      </li>
      <li class="item">
        <a href="https://about.zenodo.org/projects/">Projects</a>
      </li>
      <li class="item">
        <a href="https://about.zenodo.org/roadmap/">Roadmap</a>
      </li>
      <li class="item">
        <a href="https://about.zenodo.org/contact">Contact</a>
      </li>
    </ul>
  </div>
  <div class="column">
    <h2 class="ui inverted tiny header">Blog</h2>
    <ul class="ui inverted link list">
      <li class="item">
        <a href="https://blog.zenodo.org/">Blog</a>
      </li>
    </ul>
  </div>
  <div class="column">
    <h2 class="ui inverted tiny header">Help</h2>
    <ul class="ui inverted link list">
      <li class="item">
        <a href="https://help.zenodo.org/">FAQ</a>
      </li>
      <li class="item">
        <a href="https://help.zenodo.org/docs/">Docs</a>
      </li>
      <li class="item">
        <a href="https://help.zenodo.org/guides/">Guides</a>
      </li>
      <li class="item">
        <a href="https://zenodo.org/support">Support</a>
      </li>
    </ul>
  </div>
  <div class="column">
    <h2 class="ui inverted tiny header">Developers</h2>
    <ul class="ui inverted link list">
      <li class="item">
        <a href="https://developers.zenodo.org/">REST API</a>
      </li>
      <li class="item">
        <a href="https://developers.zenodo.org/#oai-pmh">OAI-PMH</a>
      </li>
    </ul>
  </div>
  <div class="column">
    <h2 class="ui inverted tiny header">Contribute</h2>
    <ul class="ui inverted link list">
      <li class="item">
        <a href="https://github.com/zenodo/zenodo-rdm">
          <i class="icon external" aria-hidden="true"></i>
          GitHub
        </a>
      </li>
      <li class="item">
        <a href="https://zenodo.org/donate">
          <i class="icon external" aria-hidden="true"></i>
          Donate
        </a>
      </li>
    </ul>
  </div>
      <div class="six wide column right aligned">
  <h2 class="ui inverted tiny header">Funded by</h2>
  <ul class="ui horizontal link list">
    <li class="item">
      <a href="https://home.cern/" aria-label="CERN">
        <img src="./SPEECH-COCO_files/cern.png" width="60" height="60" alt="">
      </a>
    </li>
    <li class="item">
      <a href="https://www.openaire.eu/" aria-label="OpenAIRE">
        <img src="./SPEECH-COCO_files/openaire.png" width="60" height="60" alt="">
      </a>
    </li>
    <li class="item">
      <a href="https://commission.europa.eu/index_en" aria-label="European Commission">
        <img src="./SPEECH-COCO_files/eu.png" width="88" height="60" alt="">
      </a>
    </li>
  </ul>
      </div>
      </div>
    </div>

  </div>
        <div class="footer-bottom">
  <div class="ui inverted container">
    <div class="ui grid">
      <div class="eight wide column left middle aligned">
        <p class="m-0">
          Powered by
          <a href="http://information-technology.web.cern.ch/about/computer-centre">CERN Data Centre</a> &amp; <a href="https://inveniordm.docs.cern.ch/">InvenioRDM</a>
        </p>
      </div>
      <div class="eight wide column right aligned">
        <ul class="ui inverted horizontal link list">
          <li class="item">
            <a href="https://stats.uptimerobot.com/vlYOVuWgM/">Status</a>
          </li>
          <li class="item">
            <a href="https://about.zenodo.org/privacy-policy">Privacy policy</a>
          </li>
          <li class="item">
            <a href="https://about.zenodo.org/cookie-policy">Cookie policy</a>
          </li>
          <li class="item">
            <a href="https://about.zenodo.org/terms">Terms of Use</a>
          </li>
          <li class="item">
            <a href="https://zenodo.org/support">Support</a>
          </li>
        </ul>
      </div>
    </div>
  </div>
</div>
    </footer>
  
  
    

        
          <script async="" src="./SPEECH-COCO_files/matomo.js.download"></script><script type="text/javascript">
            window.MathJax = {
              tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                processEscapes: true // Allows escaping $ signs if needed
              }
            };
          </script>
          <script type="text/javascript" src="./SPEECH-COCO_files/tex-mml-chtml.js.download"></script>
        

        
<script src="./SPEECH-COCO_files/manifest.c47d6f704a5489b6b439.js.download"></script>
<script src="./SPEECH-COCO_files/73.2cda961d9050137708ae.js.download"></script>
<script src="./SPEECH-COCO_files/3526.b4af76165310b9b820fc.js.download"></script>
<script src="./SPEECH-COCO_files/theme.26c984896f9dd909eb83.js.download"></script>
<script src="./SPEECH-COCO_files/3378.6cc088aac664ffa7d282.js.download"></script>
<script src="./SPEECH-COCO_files/1057.f91e0ee4ae21c416cc6a.js.download"></script>
<script src="./SPEECH-COCO_files/7655.1b0c9441edb2e3fb64ab.js.download"></script>
<script src="./SPEECH-COCO_files/5092.5ce3d18ec2e01c29778a.js.download"></script>
<script src="./SPEECH-COCO_files/8871.051794682ad0c4499b4d.js.download"></script>
<script src="./SPEECH-COCO_files/3040.2ff503ccd367ab93d9cf.js.download"></script>
<script src="./SPEECH-COCO_files/2373.9847bb5e25219c1fe6a1.js.download"></script>
<script src="./SPEECH-COCO_files/9827.74a386c7fd0c742cdb6b.js.download"></script>
<script src="./SPEECH-COCO_files/5638.5956d8b64591fdd54939.js.download"></script>
<script src="./SPEECH-COCO_files/base-theme-rdm.17dba47f57bddf10b033.js.download"></script>
<script src="./SPEECH-COCO_files/i18n_app.040a6518e2e4f649b2ca.js.download"></script>
<script src="./SPEECH-COCO_files/5941.c0ed1e47bc25243118d3.js.download"></script>
<script src="./SPEECH-COCO_files/9736.c54ebdf68a56e765a800.js.download"></script>
<script src="./SPEECH-COCO_files/8837.7d0d955be3c5f8cbeaf5.js.download"></script>
<script src="./SPEECH-COCO_files/1677.c380f60986f87a787507.js.download"></script>
<script src="./SPEECH-COCO_files/5494.1d58da497bf3984a5426.js.download"></script>
<script src="./SPEECH-COCO_files/5368.fe54b0dad92ced35db4d.js.download"></script>
<script src="./SPEECH-COCO_files/8585.0718c7a9406adcbc83e7.js.download"></script>
<script src="./SPEECH-COCO_files/7007.c0c4f3d19cab66fb8cd6.js.download"></script>
<script src="./SPEECH-COCO_files/8092.ddee682e7012bf7c444a.js.download"></script>
<script src="./SPEECH-COCO_files/overridable-registry.728897bcac705430fd15.js.download"></script>
  
    
      <script type="application/ld+json">{"@context": "http://schema.org", "@id": "https://doi.org/10.5281/zenodo.4282267", "@type": "https://schema.org/Dataset", "author": [{"@id": "https://orcid.org/0000-0002-1226-4156", "@type": "Person", "affiliation": [{"@type": "Organization", "name": "Universit\u00e9 Grenoble Alpes"}], "familyName": "William N. Havard", "name": "William N. Havard"}, {"@id": "https://orcid.org/0000-0001-7411-9125", "@type": "Person", "affiliation": [{"@type": "Organization", "name": "Universit\u00e9 Grenoble Alpes"}], "familyName": "Laurent Besacier", "name": "Laurent Besacier"}], "contentSize": "43.69 GB", "creator": [{"@id": "https://orcid.org/0000-0002-1226-4156", "@type": "Person", "affiliation": [{"@type": "Organization", "name": "Universit\u00e9 Grenoble Alpes"}], "familyName": "William N. Havard", "name": "William N. Havard"}, {"@id": "https://orcid.org/0000-0001-7411-9125", "@type": "Person", "affiliation": [{"@type": "Organization", "name": "Universit\u00e9 Grenoble Alpes"}], "familyName": "Laurent Besacier", "name": "Laurent Besacier"}], "dateCreated": "2020-11-23T13:46:31.442227+00:00", "dateModified": "2020-11-24T12:27:21.323278+00:00", "datePublished": "2017-06-01", "description": "\u003cp\u003e\u003cstrong\u003eSpeechCoco\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eIntroduction\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eOur corpus is an extension of the MS COCO image recognition and captioning dataset. MS COCO comprises images paired with a set of five captions. Yet, it does not include any speech. Therefore, we used\u0026nbsp;\u003ca href=\"https://www.voxygen.fr/\"\u003eVoxygen\u0026#39;s text-to-speech system\u003c/a\u003e\u0026nbsp;to synthesise\u0026nbsp;the available captions.\u003c/p\u003e\n\n\u003cp\u003eThe addition of speech as a new modality enables MSCOCO to be used for researches in the field of language acquisition, unsupervised term discovery, keyword spotting, or semantic embedding using speech and vision.\u003c/p\u003e\n\n\u003cp\u003eOur corpus is licensed under a\u0026nbsp;\u003ca href=\"https://creativecommons.org/licenses/by/4.0/legalcode\"\u003eCreative Commons Attribution 4.0 License\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eData Set\u003c/em\u003e\u003c/p\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003eThis corpus contains\u0026nbsp;\u003cstrong\u003e616,767\u003c/strong\u003e\u0026nbsp;spoken captions from MSCOCO\u0026#39;s val2014 and train2014 subsets (respectively 414,113 for train2014 and 202,654 for val2014).\u003c/p\u003e\n\t\u003c/li\u003e\n\t\u003cli\u003e\n\t\u003cp\u003eWe used 8 different voices. 4 of them have a British accent (Paul, Bronwen, Judith, and Elizabeth) and the 4 others have an American accent (Phil, Bruce, Amanda, Jenny).\u003c/p\u003e\n\t\u003c/li\u003e\n\t\u003cli\u003e\n\t\u003cp\u003eIn order to make the captions sound more natural, we used SOX\u0026nbsp;\u003cem\u003etempo\u003c/em\u003e\u0026nbsp;command, enabling us to change the speed without changing the pitch. 1/3 of the captions are 10% slower than the original pace, 1/3 are 10% faster. The last third of the captions was kept untouched.\u003c/p\u003e\n\t\u003c/li\u003e\n\t\u003cli\u003e\n\t\u003cp\u003eWe also modified approximately 30% of the original captions and added\u0026nbsp;\u003cstrong\u003edisfluencies\u003c/strong\u003e\u0026nbsp;such as \u0026quot;um\u0026quot;, \u0026quot;uh\u0026quot;, \u0026quot;er\u0026quot; so that the captions would sound more natural.\u003c/p\u003e\n\t\u003c/li\u003e\n\t\u003cli\u003e\n\t\u003cp\u003eEach WAV file is paired with a JSON file containing various information: timecode of each word in the caption, name of the speaker, name of the WAV file, etc. The JSON files have the following data structure:\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cpre\u003e\u003ccode class=\"language-json\"\u003e{\n    \"duration\": float,\n    \"speaker\": string,\n    \"synthesisedCaption\": string,\n    \"timecode\": list,\n    \"speed\": float,\n    \"wavFilename\": string,\n    \"captionID\": int,\n    \"imgID\": int,\n    \"disfluency\": list\n}\u003c/code\u003e\u003c/pre\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003eOn average, each caption comprises 10.79 tokens, disfluencies included. The WAV files are on average 3.52 seconds long.\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cem\u003eRepository\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eThe repository is organized as follows:\u003c/p\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003eCORPUS-MSCOCO (~75GB once decompressed)\u003c/p\u003e\n\n\t\u003cblockquote\u003e\n\t\u003cul\u003e\n\t\t\u003cli\u003e\n\t\t\u003cp\u003e\u003cstrong\u003etrain2014/\u003c/strong\u003e\u0026nbsp;:\u0026nbsp;folder contains 413,915 captions\u003c/p\u003e\n\n\t\t\u003cul\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003ejson/\u003c/p\u003e\n\t\t\t\u003c/li\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003ewav/\u003c/p\u003e\n\t\t\t\u003c/li\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003etranslations/\u003c/p\u003e\n\n\t\t\t\u003cul\u003e\n\t\t\t\t\u003cli\u003e\n\t\t\t\t\u003cp\u003etrain_en_ja.txt\u003c/p\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\n\t\t\t\t\u003cp\u003etrain_translate.sqlite3\u003c/p\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\u003c/ul\u003e\n\t\t\t\u003c/li\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003etrain_2014.sqlite3\u003c/p\u003e\n\t\t\t\u003c/li\u003e\n\t\t\u003c/ul\u003e\n\t\t\u003c/li\u003e\n\t\t\u003cli\u003e\n\t\t\u003cp\u003e\u003cstrong\u003eval2014/\u003c/strong\u003e\u0026nbsp;:\u0026nbsp;folder contains 202,520 captions\u003c/p\u003e\n\n\t\t\u003cul\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003ejson/\u003c/p\u003e\n\t\t\t\u003c/li\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003ewav/\u003c/p\u003e\n\t\t\t\u003c/li\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003etranslations/\u003c/p\u003e\n\n\t\t\t\u003cul\u003e\n\t\t\t\t\u003cli\u003e\n\t\t\t\t\u003cp\u003etrain_en_ja.txt\u003c/p\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\n\t\t\t\t\u003cp\u003etrain_translate.sqlite3\u003c/p\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\u003c/ul\u003e\n\t\t\t\u003c/li\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003eval_2014.sqlite3\u003c/p\u003e\n\t\t\t\u003c/li\u003e\n\t\t\u003c/ul\u003e\n\t\t\u003c/li\u003e\n\t\t\u003cli\u003e\n\t\t\u003cp\u003e\u003cstrong\u003espeechcoco_API/\u003c/strong\u003e\u003c/p\u003e\n\n\t\t\u003cul\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003espeechcoco/\u003c/p\u003e\n\n\t\t\t\u003cul\u003e\n\t\t\t\t\u003cli\u003e\n\t\t\t\t\u003cp\u003e__init__.py\u003c/p\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\n\t\t\t\t\u003cp\u003espeechcoco.py\u003c/p\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\u003c/ul\u003e\n\t\t\t\u003c/li\u003e\n\t\t\t\u003cli\u003e\n\t\t\t\u003cp\u003esetup.py\u003c/p\u003e\n\t\t\t\u003c/li\u003e\n\t\t\u003c/ul\u003e\n\t\t\u003c/li\u003e\n\t\u003c/ul\u003e\n\t\u003c/blockquote\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cem\u003eFilenames\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e.wav\u003c/strong\u003e\u0026nbsp;files contain the spoken version of a caption\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e.json\u003c/strong\u003e\u0026nbsp;files contain all the metadata of a given WAV file\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e.sqlite3\u003c/strong\u003e\u0026nbsp;files are SQLite databases containing all the information contained in the JSON files\u003c/p\u003e\n\n\u003cp\u003eWe adopted the following naming convention for both the WAV and JSON files:\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eimageID_captionID_Speaker_DisfluencyPosition_Speed[.wav/.json]\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eScript\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eWe created a script called\u0026nbsp;\u003cstrong\u003espeechcoco.py\u003c/strong\u003e\u0026nbsp;in order to handle the metadata and allow the user to easily find captions according to specific filters. The script uses the *.db files.\u003c/p\u003e\n\n\u003cp\u003eFeatures:\u003c/p\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003e\u003cstrong\u003eAggregate all the information in the JSON files into a single SQLite database\u003c/strong\u003e\u003c/p\u003e\n\t\u003c/li\u003e\n\t\u003cli\u003e\n\t\u003cp\u003e\u003cstrong\u003eFind captions according to specific filters (name, gender and nationality of the speaker, disfluency position, speed, duration, and words in the caption).\u003c/strong\u003e\u0026nbsp;\u003cem\u003eThe script automatically builds the SQLite query. The user can also provide his own SQLite query.\u003c/em\u003e\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cem\u003eThe following Python code returns all the captions spoken by a male with an American accent for which the speed was slowed\u0026nbsp;down by 10% and that contain \u0026quot;keys\u0026quot; at any position\u003c/em\u003e\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# create SpeechCoco object\ndb = SpeechCoco(train_2014.sqlite3, train_translate.sqlite3, verbose=True)\n\n# filter captions (returns Caption Objects)\ncaptions = db.filterCaptions(gender=\"Male\", nationality=\"US\", speed=0.9, text=\u0027%keys%\u0027)\nfor caption in captions:\n    print(\u0027\\n{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t\\t{}\u0027.format(caption.imageID,\n                                                  caption.captionID,\n                                                  caption.speaker.name,\n                                                  caption.speaker.nationality,\n                                                  caption.speed,\n                                                  caption.filename,\n                                                  caption.text))\u003c/code\u003e\u003c/pre\u003e\n\n\u003cpre\u003e\u003ccode\u003e...\n298817      26763   Phil    0.9     298817_26763_Phil_None_0-9.wav          A group of turkeys with bushes in the background.\n108505      147972  Phil    0.9     108505_147972_Phil_Middle_0-9.wav               Person using a, um, slider cell phone with blue backlit keys.\n258289      154380  Bruce   0.9     258289_154380_Bruce_None_0-9.wav                Some donkeys and sheep are in their green pens .\n545312      201303  Phil    0.9     545312_201303_Phil_None_0-9.wav         A man walking next to a couple of donkeys.\n...\u003c/code\u003e\u003c/pre\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003e\u003cstrong\u003eFind all the captions belonging to a specific image\u003c/strong\u003e\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003ecaptions = db.getImgCaptions(298817)\nfor caption in captions:\n    print(\u0027\\n{}\u0027.format(caption.text))\u003c/code\u003e\u003c/pre\u003e\n\n\u003cpre\u003e\u003ccode\u003eBirds wondering through grassy ground next to bushes.\nA flock of turkeys are making their way up a hill.\nUm, ah. Two wild turkeys in a field walking around.\nFour wild turkeys and some bushes trees and weeds.\nA group of turkeys with bushes in the background.\u003c/code\u003e\u003c/pre\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003e\u003cstrong\u003eParse the timecodes and have them structured\u003c/strong\u003e\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cstrong\u003einput\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003e...\n[1926.3068, \"SYL\", \"\"],\n[1926.3068, \"SEPR\", \" \"],\n[1926.3068, \"WORD\", \"white\"],\n[1926.3068, \"PHO\", \"w\"],\n[2050.7955, \"PHO\", \"ai\"],\n[2144.6591, \"PHO\", \"t\"],\n[2179.3182, \"SYL\", \"\"],\n[2179.3182, \"SEPR\", \" \"]\n...\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\u003cstrong\u003eoutput\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eprint(caption.timecode.parse())\u003c/code\u003e\u003c/pre\u003e\n\n\u003cpre\u003e\u003ccode\u003e...\n{\n\u0027begin\u0027: 1926.3068,\n\u0027end\u0027: 2179.3182,\n\u0027syllable\u0027: [{\u0027begin\u0027: 1926.3068,\n              \u0027end\u0027: 2179.3182,\n              \u0027phoneme\u0027: [{\u0027begin\u0027: 1926.3068,\n                           \u0027end\u0027: 2050.7955,\n                           \u0027value\u0027: \u0027w\u0027},\n                          {\u0027begin\u0027: 2050.7955,\n                           \u0027end\u0027: 2144.6591,\n                           \u0027value\u0027: \u0027ai\u0027},\n                          {\u0027begin\u0027: 2144.6591,\n                           \u0027end\u0027: 2179.3182,\n                           \u0027value\u0027: \u0027t\u0027}],\n              \u0027value\u0027: \u0027wait\u0027}],\n\u0027value\u0027: \u0027white\u0027\n},\n...\u003c/code\u003e\u003c/pre\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003e\u003cstrong\u003eConvert the timecodes to Praat TextGrid files\u003c/strong\u003e\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003ecaption.timecode.toTextgrid(outputDir, level=3)\u003c/code\u003e\u003c/pre\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003e\u003cstrong\u003eGet the words, syllables and phonemes between\u003c/strong\u003e\u0026nbsp;\u003cem\u003en\u003c/em\u003e\u0026nbsp;\u003cstrong\u003eseconds/milliseconds\u003c/strong\u003e\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cem\u003eThe following Python code returns all the words between 0.2 and 0.6 seconds for which at least 50% of the word\u0026#39;s total length is within the specified interval\u003c/em\u003e\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003epprint(caption.getWords(0.20, 0.60, seconds=True, level=1, olapthr=50))\u003c/code\u003e\u003c/pre\u003e\n\n\u003cpre\u003e\u003ccode\u003e...\n404537      827239  Bruce   US      0.9     404537_827239_Bruce_None_0-9.wav                Eyeglasses, a cellphone, some keys and other pocket items are all laid out on the cloth. .\n[\n    {\n        \u0027begin\u0027: 0.0,\n        \u0027end\u0027: 0.7202778,\n        \u0027overlapPercentage\u0027: 55.53412863758955,\n        \u0027word\u0027: \u0027eyeglasses\u0027\n    }\n]\n ...\u003c/code\u003e\u003c/pre\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\n\t\u003cp\u003e\u003cstrong\u003eGet the translations of the selected captions\u003c/strong\u003e\u003c/p\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cem\u003eAs for now, only japanese translations are available. We also used\u003c/em\u003e\u0026nbsp;\u003ca href=\"http://www.phontron.com/kytea/\"\u003eKytea\u003c/a\u003e\u0026nbsp;\u003cem\u003eto tokenize and tag the captions translated with Google Translate\u003c/em\u003e\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003ecaptions = db.getImgCaptions(298817)\nfor caption in captions:\n    print(\u0027\\n{}\u0027.format(caption.text))\n\n    # Get translations and POS\n    print(\u0027\\tja_google: {}\u0027.format(db.getTranslation(caption.captionID, \"ja_google\")))\n    print(\u0027\\t\\tja_google_tokens: {}\u0027.format(db.getTokens(caption.captionID, \"ja_google\")))\n    print(\u0027\\t\\tja_google_pos: {}\u0027.format(db.getPOS(caption.captionID, \"ja_google\")))\n    print(\u0027\\tja_excite: {}\u0027.format(db.getTranslation(caption.captionID, \"ja_excite\")))\u003c/code\u003e\u003c/pre\u003e\n\n\u003cpre\u003e\u003ccode\u003e   Birds wondering through grassy ground next to bushes.\n    ja_google: \u9ce5\u306f\u8302\u307f\u306e\u4e0b\u306b\u8302\u3063\u305f\u5730\u9762\u3092\u62b1\u3048\u3066\u3044\u307e\u3059\u3002\n        ja_google_tokens: \u9ce5 \u306f \u8302\u307f \u306e \u4e0b \u306b \u8302 \u3063 \u305f \u5730\u9762 \u3092 \u62b1\u3048 \u3066 \u3044 \u307e \u3059 \u3002\n        ja_google_pos: \u9ce5/\u540d\u8a5e/\u3068\u308a \u306f/\u52a9\u8a5e/\u306f \u8302\u307f/\u540d\u8a5e/\u3057\u3052\u307f \u306e/\u52a9\u8a5e/\u306e \u4e0b/\u540d\u8a5e/\u3057\u305f \u306b/\u52a9\u8a5e/\u306b \u8302/\u52d5\u8a5e/\u3057\u3052 \u3063/\u8a9e\u5c3e/\u3063 \u305f/\u52a9\u52d5\u8a5e/\u305f \u5730\u9762/\u540d\u8a5e/\u3058\u3081\u3093 \u3092/\u52a9\u8a5e/\u3092 \u62b1\u3048/\u52d5\u8a5e/\u304b\u304b\u3048 \u3066/\u52a9\u8a5e/\u3066 \u3044/\u52d5\u8a5e/\u3044 \u307e/\u52a9\u52d5\u8a5e/\u307e \u3059/\u8a9e\u5c3e/\u3059 \u3002/\u88dc\u52a9\u8a18\u53f7/\u3002\n    ja_excite: \u4f4e\u6728\u3068\u96a3\u63a5\u3057\u305f\u8349\u6df1\u3044\u30b0\u30e9\u30a6\u30f3\u30c9\u3092\u901a\u3063\u3066\u7591\u3046\u9ce5\u3002\n\nA flock of turkeys are making their way up a hill.\n    ja_google: \u4e03\u9762\u9ce5\u306e\u7fa4\u308c\u304c\u4e18\u3092\u4e0a\u3063\u3066\u3044\u307e\u3059\u3002\n        ja_google_tokens: \u4e03 \u9762 \u9ce5 \u306e \u7fa4\u308c \u304c \u4e18 \u3092 \u4e0a \u3063 \u3066 \u3044 \u307e \u3059 \u3002\n        ja_google_pos: \u4e03/\u540d\u8a5e/\u306a\u306a \u9762/\u540d\u8a5e/\u3081\u3093 \u9ce5/\u540d\u8a5e/\u3068\u308a \u306e/\u52a9\u8a5e/\u306e \u7fa4\u308c/\u540d\u8a5e/\u3080\u308c \u304c/\u52a9\u8a5e/\u304c \u4e18/\u540d\u8a5e/\u304a\u304b \u3092/\u52a9\u8a5e/\u3092 \u4e0a/\u52d5\u8a5e/\u306e\u307c \u3063/\u8a9e\u5c3e/\u3063 \u3066/\u52a9\u8a5e/\u3066 \u3044/\u52d5\u8a5e/\u3044 \u307e/\u52a9\u52d5\u8a5e/\u307e \u3059/\u8a9e\u5c3e/\u3059 \u3002/\u88dc\u52a9\u8a18\u53f7/\u3002\n    ja_excite: \u4e03\u9762\u9ce5\u306e\u7fa4\u308c\u306f\u4e18\u306e\u4e0a\u3067\u9032\u3093\u3067\u3044\u308b\u3002\n\nUm, ah. Two wild turkeys in a field walking around.\n    ja_google: \u91ce\u751f\u306e\u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6\u3001\u91ce\u751f\u306e\u4e03\u9762\u9ce5\n        ja_google_tokens: \u91ce\u751f \u306e \u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6 \u3001 \u91ce\u751f \u306e \u4e03 \u9762 \u9ce5\n        ja_google_pos: \u91ce\u751f/\u540d\u8a5e/\u3084\u305b\u3044 \u306e/\u52a9\u8a5e/\u306e \u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6/\u540d\u8a5e/\u3057\u3061\u3081\u3093\u3061\u3087\u3046 \u3001/\u88dc\u52a9\u8a18\u53f7/\u3001 \u91ce\u751f/\u540d\u8a5e/\u3084\u305b\u3044 \u306e/\u52a9\u8a5e/\u306e \u4e03/\u540d\u8a5e/\u306a\u306a \u9762/\u540d\u8a5e/\u3081\u3093 \u9ce5/\u540d\u8a5e/\u3061\u3087\u3046\n    ja_excite: \u307e\u308f\u308a\u3067\u79fb\u52d5\u3057\u3066\u3044\u308b\u30d5\u30a3\u30fc\u30eb\u30c9\u306e2\u7fbd\u306e\u91ce\u751f\u306e\u4e03\u9762\u9ce5\n\nFour wild turkeys and some bushes trees and weeds.\n    ja_google: 4\u672c\u306e\u91ce\u751f\u306e\u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6\u3068\u3044\u304f\u3064\u304b\u306e\u8302\u307f\u306e\u6728\u3068\u96d1\u8349\n        ja_google_tokens: 4 \u672c \u306e \u91ce\u751f \u306e \u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6 \u3068 \u3044\u304f \u3064 \u304b \u306e \u8302\u307f \u306e \u6728 \u3068 \u96d1\u8349\n        ja_google_pos: 4/\u540d\u8a5e/4 \u672c/\u63a5\u5c3e\u8f9e/\u307b\u3093 \u306e/\u52a9\u8a5e/\u306e \u91ce\u751f/\u540d\u8a5e/\u3084\u305b\u3044 \u306e/\u52a9\u8a5e/\u306e \u30b7\u30c1\u30e1\u30f3\u30c1\u30e7\u30a6/\u540d\u8a5e/\u3057\u3061\u3081\u3093\u3061\u3087\u3046 \u3068/\u52a9\u8a5e/\u3068 \u3044\u304f/\u540d\u8a5e/\u3044\u304f \u3064/\u63a5\u5c3e\u8f9e/\u3064 \u304b/\u52a9\u8a5e/\u304b \u306e/\u52a9\u8a5e/\u306e \u8302\u307f/\u540d\u8a5e/\u3057\u3052\u307f \u306e/\u52a9\u8a5e/\u306e \u6728/\u540d\u8a5e/\u304d \u3068/\u52a9\u8a5e/\u3068 \u96d1\u8349/\u540d\u8a5e/\u3056\u3063\u305d\u3046\n    ja_excite: 4\u7fbd\u306e\u91ce\u751f\u306e\u4e03\u9762\u9ce5\u304a\u3088\u3073\u3044\u304f\u3064\u304b\u306e\u4f4e\u6728\u6728\u3068\u96d1\u8349\n\nA group of turkeys with bushes in the background.\n    ja_google: \u80cc\u666f\u306b\u8302\u307f\u3092\u6301\u3064\u4e03\u9762\u9ce5\u306e\u7fa4\n        ja_google_tokens: \u80cc\u666f \u306b \u8302\u307f \u3092 \u6301 \u3064 \u4e03 \u9762 \u9ce5 \u306e \u7fa4\n        ja_google_pos: \u80cc\u666f/\u540d\u8a5e/\u306f\u3044\u3051\u3044 \u306b/\u52a9\u8a5e/\u306b \u8302\u307f/\u540d\u8a5e/\u3057\u3052\u307f \u3092/\u52a9\u8a5e/\u3092 \u6301/\u52d5\u8a5e/\u3082 \u3064/\u8a9e\u5c3e/\u3064 \u4e03/\u540d\u8a5e/\u306a\u306a \u9762/\u540d\u8a5e/\u3081\u3093 \u9ce5/\u540d\u8a5e/\u3061\u3087\u3046 \u306e/\u52a9\u8a5e/\u306e \u7fa4/\u540d\u8a5e/\u3080\u308c\n    ja_excite: \u80cc\u666f\u306e\u4f4e\u6728\u3092\u6301\u3064\u4e03\u9762\u9ce5\u306e\u30b0\u30eb\u30fc\u30d7\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e", "distribution": [{"@type": "DataDownload", "contentUrl": "https://zenodo.org/api/records/4282267/files/train2014.tar.xz/content", "encodingFormat": "application/x-xz"}, {"@type": "DataDownload", "contentUrl": "https://zenodo.org/api/records/4282267/files/speechcoco_API.zip/content", "encodingFormat": "application/zip"}, {"@type": "DataDownload", "contentUrl": "https://zenodo.org/api/records/4282267/files/val2014.tar.xz/content", "encodingFormat": "application/x-xz"}], "identifier": "https://doi.org/10.5281/zenodo.4282267", "inLanguage": {"@type": "Language", "alternateName": "eng", "name": "English"}, "keywords": "MSCOCO, VGS, Speech, Visually Grounded Speech, audio, captions", "license": "https://creativecommons.org/licenses/by/4.0/legalcode", "name": "SPEECH-COCO", "publisher": {"@type": "Organization", "name": "Zenodo"}, "size": "43.69 GB", "url": "https://zenodo.org/records/4282267"}</script>

  <script src="./SPEECH-COCO_files/invenio-app-rdm-landing-page-theme.9cd3eb8a17f67478560a.js.download"></script>
  <script src="./SPEECH-COCO_files/9161.5755dd7c36b83e94db04.js.download"></script>
<script src="./SPEECH-COCO_files/1357.9ea8472fd5d9f864314a.js.download"></script>
<script src="./SPEECH-COCO_files/1644.075da9193d9514fb59d4.js.download"></script>
<script src="./SPEECH-COCO_files/8962.809e5b10c300f06544bb.js.download"></script>
<script src="./SPEECH-COCO_files/9300.b1748799c0d73c3dae83.js.download"></script>
<script src="./SPEECH-COCO_files/5680.dd8c89f97dd13e02260b.js.download"></script>
<script src="./SPEECH-COCO_files/invenio-app-rdm-landing-page.612547d242e637113919.js.download"></script>
  <script src="./SPEECH-COCO_files/previewer_theme.0c35eaf223c61e6c44e9.js.download"></script>
  <script src="./SPEECH-COCO_files/zenodo-rdm-citations.321ede81206c36d62e57.js.download"></script>
  

        <div class="ui container info message cookie-banner transition hidden">
  <i class="close icon"></i>
  <div>
    <i aria-hidden="true" class="info icon"></i>
    <p class="inline">This site uses cookies. Find out more on <a href="https://about.zenodo.org/cookie-policy">how we use cookies</a></p>
  </div>
  <div class="buttons">
    <button class="ui button small primary" id="cookies-all">Accept all cookies</button>
    <button class="ui button small" id="cookies-essential">Accept only essential cookies</button>
  </div>
</div>

<script>
  var _paq = window._paq = window._paq || [];
  _paq.push(['requireCookieConsent']);

  (function() {
    var u="https://webanalytics.web.cern.ch/";
    _paq.push(['setTrackerUrl', u+'matomo.php']);
    _paq.push(['setSiteId', '366']);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
  })();

  const cookieConsent = document.cookie
    .split("; ")
    .find((row) => row.startsWith("cookie_consent="))
    ?.split("=")[1];

  if (cookieConsent) {
    if (cookieConsent === "all") {
      matomo();
    }
  } else {
    document.querySelector(".cookie-banner").classList.remove("hidden")
    _paq.push(['forgetConsentGiven']);
  }

  $('.cookie-banner .close')
    .on('click', function () {
      $(this)
        .closest('.message')
        .transition('fade');
      setCookie("cookie_consent","essential");
    });

  $('#cookies-essential')
    .on('click', function () {
      $(this)
        .closest('.message')
        .transition('fade');
      setCookie("cookie_consent","essential");
    });

  $('#cookies-all')
    .on('click', function () {
      $(this)
        .closest('.message')
        .transition('fade');
      setCookie("cookie_consent","all");
      _paq.push(['rememberCookieConsentGiven']);
      matomo();
    });

  function matomo() {
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
  }

  function setCookie(cname, cvalue) {
    var d = new Date();
    d.setTime(d.getTime() + (365 * 24 * 60 * 60 * 1000)); // one year
    var expires = "expires=" + d.toUTCString();
    var cookie = cname + "=" + cvalue + ";" + expires + ";"
    cookie += "Domain=zenodo.org;Path=/;SameSite=None; Secure";
    document.cookie = cookie;
  }
</script>
  
</body></html>